{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Copyright 1996- ECMWF.\n",
    "#\n",
    "# This software is licensed under the terms of the Apache Licence Version 2.0\n",
    "# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "# In applying this licence, ECMWF does not waive the privileges and immunities\n",
    "# granted to it by virtue of its status as an intergovernmental organisation\n",
    "# nor does it submit to any jurisdiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from itertools import product, groupby\n",
    "import random\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import metview as mv # for retrieving grib data from MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_out_folder = '/Data/'\n",
    "ERA5_loc = main_out_folder+'ERA5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_days_used = list((np.arange(1, 46)*24))\n",
    "\n",
    "area_atm_var = [55, -15, 20, 45] # domain used for the large-scale atmospheric variability (coordinates as in N/W/S/E)\n",
    "grid_atm_var = [1, 1] # resolution for the large-scale atmospheric variability\n",
    "\n",
    "area_precipt = [48, -10, 27, 41] #  domain of interest for the precipitation (Mediterranean domain)\n",
    "grid_precipt = [.25, .25] # grid resolution in degrees for the precipitation data\n",
    "\n",
    "Reforecasts_vars_used = {'SLP': [0, 'sfc', '151.128'],\n",
    "             'Z500': [500, 'pl', '129.128'],\n",
    "            }\n",
    "\n",
    "ERA5_vars_used = [[0, 'sfc', '151.128', 'D1_Mean_SLP'],   # SLP data\n",
    "               [500, 'pl', '129.128', 'D1_Mean_Z500'], # Z500 data\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all files for storing the large-scale atmospheric variability reforecasts data\n",
    "Files_locs = [main_out_folder+i[0]+'/'+i[1]+'/' for i in list(product(Reforecasts_vars_used.keys(), ['cf', 'pf']))]\n",
    "[Path(files_loc).mkdir(parents=True, exist_ok=True) for files_loc in Files_locs]\n",
    "\n",
    "# Create all files for storing the precipitation reforecasts data\n",
    "Files_locs = [main_out_folder+i[0]+'/'+i[1]+'/' for i in list(product(['Precipitation'], ['cf', 'pf']))]\n",
    "[Path(files_loc).mkdir(parents=True, exist_ok=True) for files_loc in Files_locs]\n",
    "\n",
    "# Create the subfolder for storing the ERA5 data\n",
    "Path(ERA5_loc).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "del(Files_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download reforecasts data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dates that have the same Cycle so that the forecast data are consistent. Details about changes in cycles are available at https://www.ecmwf.int/en/forecasts/documentation-and-support/changes-ecmwf-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dates for Cycle 46r1 11 June 2019 - 30 June 2020\n",
    "start_date = '20190611'\n",
    "end_date = '20200630'\n",
    "\n",
    "initialization_dates = pd.date_range(start_date, end_date)\n",
    "\n",
    "# keep Mondays (0) and Thursdays (3)\n",
    "kept_dates = (initialization_dates.weekday == 0) | (initialization_dates.weekday == 3)\n",
    "initialization_dates = initialization_dates[kept_dates]\n",
    "initialization_dates = initialization_dates.strftime('%Y%m%d')\n",
    "\n",
    "del(start_date, end_date, kept_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data of SLP and Z500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(input_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Download data internally from MARS and not from S2S database, because the latter has data stored in coarse\n",
    "    resolution, thus there is downscaling perfomed, which increases the errors\n",
    "    \"\"\"\n",
    "    \n",
    "    param, type_used, init_date = input_data\n",
    "    \n",
    "    levelist_used = Reforecasts_vars_used[param][0]\n",
    "    levtype_used = Reforecasts_vars_used[param][1]\n",
    "    param_used = Reforecasts_vars_used[param][2]\n",
    "    \n",
    "    files_loc = main_out_folder+param+'/'+type_used+'/'\n",
    "    file_name = files_loc+param+'_'+type_used+'_'+init_date+'.grb'\n",
    "\n",
    "    if not Path(file_name).exists(): # if data are not already available, then download and save them\n",
    "        \n",
    "        init_year = int(init_date[:4])\n",
    "        hdates_used = ['{}{}'.format(x, init_date[-4:]) for x in range(init_year-20, init_year)]\n",
    "\n",
    "        if type_used == 'cf':\n",
    "            fc_all = mv.retrieve(Class = 'od', \n",
    "                                 date = init_date, \n",
    "                                 expver = 1, \n",
    "                                 hdate = hdates_used,\n",
    "                                 levelist = [levelist_used],\n",
    "                                 levtype = levtype_used,\n",
    "                                 param = param_used,\n",
    "                                 step = lead_days_used, \n",
    "                                 time = ['00:00:00'],\n",
    "                                 stream = 'enfh',\n",
    "                                 type = 'cf',\n",
    "                                 area = area_atm_var, \n",
    "                                 grid = grid_atm_var)\n",
    "\n",
    "        elif type_used == 'pf':\n",
    "            fc_all = mv.retrieve(Class = 'od', \n",
    "                                 date = init_date, \n",
    "                                 expver = 1, \n",
    "                                 hdate = hdates_used,\n",
    "                                 levelist = [levelist_used],\n",
    "                                 levtype = levtype_used,\n",
    "                                 param = param_used,\n",
    "                                 step = lead_days_used, \n",
    "                                 time = ['00:00:00'],\n",
    "                                 stream = 'enfh',\n",
    "                                 type = 'pf',\n",
    "                                 number = list(np.arange(1, 11)),\n",
    "                                 area = area_atm_var, \n",
    "                                 grid = grid_atm_var)        \n",
    "        \n",
    "        mv.write(file_name, fc_all) # save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_combs = list(product(Reforecasts_vars_used.keys(), ['cf', 'pf'], initialization_dates))\n",
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "Downloads = list(tqdm.tqdm(pool.imap(download_data, All_combs), total=len(All_combs), position=0, leave=True)) \n",
    "pool.close()\n",
    "del(pool, All_combs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download precipitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_precip(input_data):\n",
    "    \n",
    "    type_used, init_date = input_data\n",
    "    \n",
    "    files_loc = main_out_folder+'Precipitation/'+type_used+'/'\n",
    "    file_name = files_loc+'Precipitation_'+type_used+'_'+init_date+'.grb'\n",
    "\n",
    "    if not Path(file_name).exists(): # if data are not already available, then download and save them\n",
    "        \n",
    "        init_year = int(init_date[:4])\n",
    "        hdates_used = ['{}{}'.format(x, init_date[-4:]) for x in range(init_year-20, init_year)]\n",
    "\n",
    "        if type_used == 'cf':\n",
    "            fc_all = mv.retrieve(Class = 'od', \n",
    "                                 date = init_date, \n",
    "                                 expver = 1, \n",
    "                                 hdate = hdates_used,\n",
    "                                 levelist = 0,\n",
    "                                 levtype = 'sfc',\n",
    "                                 param = '228.128',\n",
    "                                 step = lead_days_used, \n",
    "                                 time = ['00:00:00'],\n",
    "                                 stream = 'enfh',\n",
    "                                 type = 'cf',\n",
    "                                 area = area_precipt, \n",
    "                                 grid = grid_precipt,\n",
    "                                 )\n",
    "\n",
    "        elif type_used == 'pf':\n",
    "            fc_all = mv.retrieve(Class = 'od', \n",
    "                                 date = init_date, \n",
    "                                 expver = 1, \n",
    "                                 hdate = hdates_used,\n",
    "                                 levelist = 0,\n",
    "                                 levtype = 'sfc',\n",
    "                                 param = '228.128',\n",
    "                                 step = lead_days_used, \n",
    "                                 time = ['00:00:00'],\n",
    "                                 stream = 'enfh',\n",
    "                                 type = 'pf',\n",
    "                                 number = list(np.arange(1, 11)),\n",
    "                                 area = area_precipt, \n",
    "                                 grid = grid_precipt,\n",
    "                                 )        \n",
    "        \n",
    "        fc_all = fc_all*1000 # convert to mm\n",
    "        \n",
    "        mv.write(file_name, fc_all) # save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_combs = list(product(['cf', 'pf'], initialization_dates))\n",
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "Downloads = list(tqdm.tqdm(pool.imap(download_precip, All_combs), total=len(All_combs), position=0, leave=True)) \n",
    "pool.close()\n",
    "del(pool, All_combs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download ERA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download SLP & Z500 data consistent with reforecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_generated_all = pd.date_range(start = '19790101', end = '20210101').strftime('%Y%m%d').to_list() # dates\n",
    " \n",
    "# dates are chunked per year-month for efficient download, since MARS uses this subsetting for storing the data\n",
    "dates_atm_vars = [list(v) for l, v in groupby(dates_generated_all[:], lambda x: x[:6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ERA5_0UTC(input_data):\n",
    "    \n",
    "    levelist, levtype, atm_var, dates_subset = input_data # inputs to be a list of 4 in specific order!\n",
    "    \n",
    "    '''\n",
    "    Function for downloading data of atmospheric variables from MARS and calculating daily mean values\n",
    "    \n",
    "    :param levelist: level of interest, e.g. 0 for surface parameters, 500 for 500 hPa\n",
    "    :param levtype: leveltype of interest, e.g. pressure levels ('pl'), surface ('sfc')\n",
    "    :param atm_var: paramater of interest (e.g. the SLP is flagged as 151.128 at MARS)\n",
    "    :param dates_subset: the subset of dates to be downloaded\n",
    "    '''\n",
    "    \n",
    "    # function for retrieving the data from MARS\n",
    "    fc_all = mv.retrieve(Class = 'ea', # class of data, e.g. ERA5 ('ea')\n",
    "                         stream = 'oper', # stream of interest, e.g. Ensemble ('enfo'), Deterministic ('oper') \n",
    "                         expver = 1, # experiment's version, e.g. Operational (1), Research (xxxx[A-Z/0-9])\n",
    "                         type = 'an', # type of data, e.g. Analysis ('an')\n",
    "                         param = atm_var, \n",
    "                         levtype = levtype,\n",
    "                         levelist = levelist,\n",
    "                         date = dates_subset,\n",
    "                         time = 0, # keep only 0 UTC for having consistency with reforecasts data\n",
    "                         area = area_atm_var,\n",
    "                         grid = grid_atm_var, \n",
    "                         )\n",
    "    \n",
    "    return fc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times = len(dates_atm_vars)\n",
    "    \n",
    "AtmVar = {}\n",
    "for var in ERA5_vars_used:\n",
    "    \n",
    "    Daily = mv.Fieldset()\n",
    "    for i_dates in tqdm.tqdm(dates_atm_vars):\n",
    "        levelist, levtype, atm_var, file_name = var \n",
    "        i_daily = download_ERA5_0UTC([levelist, levtype, atm_var, i_dates])\n",
    "        Daily.append(i_daily)\n",
    "        \n",
    "    mv.write(ERA5_loc + file_name + '_00UTC.grb', Daily) # save data\n",
    "    AtmVar[var[-1].split('_')[-1]] = Daily\n",
    "    \n",
    "del(Times, var, i_dates, levelist, levtype, atm_var, file_name, i_daily, Daily)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download ERA data based on all hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ERA5_fullhourly(input_data):\n",
    "    \n",
    "    levelist, levtype, atm_var, dates_subset = input_data # inputs to be a list of 4 in specific order!\n",
    "    \n",
    "    '''\n",
    "    Function for downloading data of atmospheric variables from MARS and calculating daily mean values\n",
    "    \n",
    "    :param levelist: level of interest, e.g. 0 for surface parameters, 500 for 500 hPa\n",
    "    :param levtype: leveltype of interest, e.g. pressure levels ('pl'), surface ('sfc')\n",
    "    :param atm_var: paramater of interest (e.g. the SLP is flagged as 151.128 at MARS)\n",
    "    :param dates_subset: the subset of dates to be downloaded\n",
    "    '''\n",
    "    \n",
    "    # function for retrieving the data from MARS\n",
    "    fc_all = mv.retrieve(Class = 'ea', # class of data, e.g. ERA5 ('ea')\n",
    "                         stream = 'oper', # stream of interest, e.g. Ensemble ('enfo'), Deterministic ('oper') \n",
    "                         expver = 1, # experiment's version, e.g. Operational (1), Research (xxxx[A-Z/0-9])\n",
    "                         type = 'an', # type of data, e.g. Analysis ('an')\n",
    "                         param = atm_var, \n",
    "                         levtype = levtype,\n",
    "                         levelist = levelist,\n",
    "                         date = dates_subset,\n",
    "                         time = list(range(0,24)), # all hourly timesteps\n",
    "                         area = area_atm_var,\n",
    "                         grid = grid_atm_var, \n",
    "                         )\n",
    "\n",
    "    Daily_sub = mv.Fieldset() # mv for values for dates_subset\n",
    "    fields = mv.grib_get(fc_all, ['date']) # get the 'date' field from the fc_all object\n",
    "\n",
    "    for day_i in dates_subset: # loop through the whole list of unique dates_subset\n",
    "\n",
    "        used_indices = list(np.where(np.array(fields) == day_i)[0]) # indices that belong to the day of interest\n",
    "        used_indices = np.array(used_indices, dtype='float64') # convert to float64 for using it at mv object\n",
    "        daily_subset = fc_all[used_indices] # subset and keep only the fields of the day of interest\n",
    "        Daily_sub.append(mv.mean(daily_subset)) # calculate the daily mean and append it\n",
    "    \n",
    "    return Daily_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Times = len(dates_atm_vars)\n",
    "    \n",
    "AtmVar = {}\n",
    "for var in ERA5_vars_used:\n",
    "    \n",
    "    levelist, levtype, atm_var, file_name = var \n",
    "    Inputs = list(zip([levelist]*Times, [levtype]*Times, [atm_var]*Times, dates_atm_vars))\n",
    "    \n",
    "    pool_atmvar = multiprocessing.Pool() # object for multiprocessing\n",
    "    Daily = list(tqdm.tqdm(pool_atmvar.imap(download_ERA5_fullhourly, Inputs), \n",
    "                           total=Times, position=0, leave=True)) # list of mv.Fieldsets\n",
    "    pool_atmvar.close()\n",
    "    del(pool_atmvar)\n",
    "    \n",
    "    for i in range(1, Times): # concatenate all Fieldsets to the first one\n",
    "        Daily[0].append(Daily[i])\n",
    "\n",
    "    Daily = Daily[0] # keep the full set of the atmospheric variable data\n",
    "    \n",
    "    mv.write(ERA5_loc + file_name + '.grb', Daily) # save data\n",
    "\n",
    "    AtmVar[var[-1].split('_')[-1]] = Daily\n",
    "    \n",
    "del(Times, var, levelist, levtype, atm_var, file_name, Inputs, Daily, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download precipitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_precipit = [dates_atm_vars[i][-1:] + dates_atm_vars[i+1] for i in range(len(dates_atm_vars)-1)] # from 2nd chunk\n",
    "dates_precipit.insert(0, dates_atm_vars[0]) # append the 1st chunk so all the dates are now complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_precip(dates_subset):\n",
    "    \n",
    "    ' Function for downloading precipitation data from MARS and calculating daily total values '\n",
    "    \n",
    "    fc_all = mv.retrieve(Class = 'ea', # class of data, e.g. ERA5 ('ea')\n",
    "                         stream = 'oper', # stream of interest, e.g. Ensemble ('enfo'), Deterministic ('oper') \n",
    "                         expver = 1, # experiment's version, e.g. Operational (1), Research (xxxx[A-Z/0-9])\n",
    "                         type = 'fc', # type of data, e.g. Forecast ('fc'), Analysis ('an')\n",
    "                         param = 'tp', # used paramater: Total Precipitation ('tp' = '228.128')\n",
    "                         levtype = 'sfc',\n",
    "                         levelist = 0,\n",
    "                         date = dates_subset, # use the subset of dates\n",
    "                         time = [6, 18], # time steps of interest (forecast fields only at 06:00 & 18:00)\n",
    "                         step = list(range(7,19)), # precipitation is calculated from short-range forecasted data\n",
    "                         grid = grid_precipt,\n",
    "                         area = area_precipt,\n",
    "                         )\n",
    "\n",
    "    Daily_sub = mv.Fieldset() # create the mv object for storing the daily values for the dates_subset\n",
    "\n",
    "    for i_day in range(len(dates_subset) - 1): # loop through the whole list of unique dates_subset\n",
    "\n",
    "        # downloaded data are in the sequence: day_i 06:00 steps 7-18 (12 steps), 18:00 steps 7-18 (12 steps)\n",
    "        start_indice = 12 + 24*i_day # data for daily accumulation start at 18:00 step 7 of previous day\n",
    "        end_indice = 12 + 24*(i_day+1) # data end at 06:00 step 12 of current day (24 hourly steps in total)\n",
    "\n",
    "        sub = mv.sum(fc_all[start_indice : end_indice]) # total daily precipitation\n",
    "        sub = mv.grib_set(sub, ['date', int(dates_subset[i_day + 1])]) # replace the date field with the correct date\n",
    "\n",
    "        Daily_sub.append(sub) # append to Daily_sub metview object\n",
    "    \n",
    "    return Daily_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_precip = multiprocessing.Pool() # object for multiprocessing for creating a list of mv.Fieldsets\n",
    "Precip = list(tqdm.tqdm(pool_precip.imap(download_precip, dates_precipit), \n",
    "                        total=len(dates_precipit), position=0, leave=True))\n",
    "pool_precip.close()\n",
    "\n",
    "for i in range(1, len(Precip)): # concatenate all Fieldsets to the first one\n",
    "    Precip[0].append(Precip[i])\n",
    "\n",
    "Precip = Precip[0] # keep the full set of the precipitation data\n",
    "Precip = Precip*1000 # convert to mm\n",
    "Precip = mv.read(data=Precip, area=area_precipt) # crop to the actual area of interest\n",
    "\n",
    "mv.write(ERA5_loc + 'D1_Total_Precipitation.grb', Precip) # Save the daily total precipitation file\n",
    "\n",
    "del(pool_precip, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
