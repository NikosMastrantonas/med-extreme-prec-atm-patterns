{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Copyright 1996- ECMWF.\n",
    "#\n",
    "# This software is licensed under the terms of the Apache Licence Version 2.0\n",
    "# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "# In applying this licence, ECMWF does not waive the privileges and immunities\n",
    "# granted to it by virtue of its status as an intergovernmental organisation\n",
    "# nor does it submit to any jurisdiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "import multiprocessing\n",
    "import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_loc = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vars_used = ['SLP', 'Z500']\n",
    "Area_used = [50, -11, 26, 41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming convention and prefered order of the clusters\n",
    "New_names = ['Atlantic Low', 'Biscay Low', 'Iberian Low', 'Sicilian Low', 'Balkan Low', 'Black Sea Low',\n",
    "             'Mediterranean High', 'Minor Low', 'Minor High'] # naming in the final order of interest\n",
    "New_order = [4, 8, 1, 6, 7, 2, 0, 3, 5] # list with the final order of each cluster\n",
    "\n",
    "# Dictionary for reordering the clusters\n",
    "New_order = {i:j for i, j in enumerate(New_order)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clusters = dir_loc+'Data/ERA5/Clusters_Med_SLP~Z500.csv'\n",
    "Clusters = pd.read_csv(Clusters, index_col=0)\n",
    "Clusters = Clusters['Label']\n",
    "Clusters.index = pd.to_datetime(Clusters.index, format='%d/%m/%Y')\n",
    "Clusters = Clusters.map(New_order) # rename the clusters based on the prefered order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalies(variable):\n",
    "    \n",
    "    # read actual daily values\n",
    "    file_path = dir_loc+'/Data/ERA5/D1_Mean_'+variable+'.grb'\n",
    "    Daily = xr.open_dataarray(file_path, engine='cfgrib') # read data\n",
    "    Daily = Daily.reset_coords(drop=True).astype('float32') # float 32 and drop not used variables\n",
    "    Daily = Daily.sel(latitude=slice(Area_used[0], Area_used[2]), # adjust domain for having only\n",
    "                      longitude=slice(Area_used[1], Area_used[3])) # .. area of actual data used\n",
    "    Daily_Patt = Daily.sel(time=Clusters.index) # select only dates that were used for deriving the patterns\n",
    "    \n",
    "    actual_days = Daily.time.values # get actual timesteps\n",
    "    dates_grouped = pd.to_datetime(actual_days).strftime('%m%d') # get Month-Day of each timestep\n",
    "    Daily = Daily.assign_coords({'time': dates_grouped}) # change the time to Month-Day\n",
    "    \n",
    "    dates_grouped_patterns = pd.to_datetime(Daily_Patt.time.values).strftime('%m%d') # get Month-Day of each timestep\n",
    "    \n",
    "    # 5-day smoothed climatology. Rolling can be applied directly because the daily data refer to consequtive days. If\n",
    "    # days are not consecutive, firstly the xr.resample should be applied, so that missing days are generated with NaN\n",
    "    Smoothed = Daily_Patt.rolling(time=5, center=True, min_periods=1).mean() # 5-day smoothing\n",
    "    Smoothed = Smoothed.assign_coords({'time': dates_grouped_patterns}) # change the time to Month-Day\n",
    "    \n",
    "    Climatology = Smoothed.groupby('time').mean() # climatology of the smoothed data\n",
    "    \n",
    "    Anomalies = Daily.groupby('time') - Climatology\n",
    "    Anomalies = Anomalies.assign_coords({'time': actual_days}) # change back to the original timestep information\n",
    "    Anomalies.name = variable\n",
    "    Daily = Daily.assign_coords({'time': actual_days}) \n",
    "    Daily.name = variable\n",
    "    \n",
    "    return Anomalies, Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "PatternsAll = list(tqdm.tqdm(pool.imap(anomalies, Vars_used), total=len(Vars_used), position=0, leave=True))\n",
    "pool.close()\n",
    "\n",
    "Patterns = xr.merge([i[0] for i in PatternsAll])\n",
    "DailyFull = xr.merge([i[1] for i in PatternsAll])\n",
    "\n",
    "del(pool, PatternsAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate patterns composites based on given xarray data, and tags for each daily field\n",
    "def patterns_composites(data, tags):\n",
    "    \n",
    "    Comp = data.assign_coords({'time': tags}).groupby('time').mean() # composites\n",
    "    Comp = Comp.rename({'time': 'cluster'})\n",
    "    \n",
    "    return Comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial composites only based on K-means clustering (as in Mastrantonas et al, 2021)\n",
    "Composites_Kmeans = patterns_composites(Patterns.sel(time=Clusters.index), Clusters.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new composites based on minimum Euclidian distance (RMSE) of each daily field to all 9 cluster composites\n",
    "def composites_RMSE(data, composites_input, Difs_normalization='actual'):\n",
    "    \n",
    "    Difs = data - composites_input # error per cell from each composite\n",
    "    Difs = Difs**2 # square of error\n",
    "    Weights = np.cos(np.deg2rad(Difs.latitude)) # weights due to areal differences of each grid\n",
    "    Difs = Difs.weighted(Weights).mean(['latitude', 'longitude']) # weighted mean of the differences for all cells\n",
    "    Difs = np.sqrt(Difs) # square root of error (as in RMSE metric)\n",
    "    \n",
    "    Difs_mean = Difs.mean(['time', 'cluster']) # get the average Euclidian distance\n",
    "    \n",
    "    if type(Difs_normalization) == str: # normalise difs from the used variables so values can be comparable\n",
    "        Difs = Difs/Difs_mean \n",
    "    else:\n",
    "        Difs = Difs/Difs_normalization\n",
    "        \n",
    "    Difs = Difs.to_array()\n",
    "    Difs = Difs.rename({'variable': 'atm_variable'})\n",
    "    Difs = Difs.mean('atm_variable') # mean of differences for all variables\n",
    "\n",
    "    NewLabel = Difs.argmin('cluster').values # allocate each field to the cluster of lowest final Euclidian distance\n",
    "    NewComposite = patterns_composites(data, NewLabel) # calculate new composites\n",
    "    \n",
    "    return (NewLabel, NewComposite, Difs_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_max = 100\n",
    "mismatching_max = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_iter = 1\n",
    "print('Starting iterations for convergence of composites based on RMSE of daily patterns.')\n",
    "IterationResults = composites_RMSE(Patterns.sel(time=Clusters.index), Composites_Kmeans)\n",
    "Mismatch = (IterationResults[0] != Clusters.values).sum()/len(Clusters.values)*100\n",
    "print(f'Initial mismatch between standalone K-means and K-means followed by RMSE is {np.round(Mismatch, 2)}%.')\n",
    "while Mismatch>mismatching_max and i_iter<=iterations_max:\n",
    "    old_tags = IterationResults[0]\n",
    "    IterationResults = composites_RMSE(Patterns.sel(time=Clusters.index), IterationResults[1])\n",
    "    Mismatch = (IterationResults[0] != old_tags).sum()/len(old_tags)*100\n",
    "    i_iter = i_iter+1\n",
    "\n",
    "if Mismatch==0:\n",
    "    print(f'Analysis converged in {i_iter} iterations. There is a full agreement and clusters are stabilized.')\n",
    "else:\n",
    "    print(f'Analysis not converged after {i_iter} iterations. Final mismatch is {np.round(Mismatch, 2)}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(dir_loc+'/ProcessedData/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save stabilized clusters and the Mean Cluster Climatological differences for using at next steps\n",
    "IterationResults[1].to_netcdf(dir_loc+'/ProcessedData/PatternComposites_ERA5.nc')\n",
    "IterationResults[2].to_netcdf(dir_loc+'/ProcessedData/DifMeanERA5_climatological.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final cluster allocations for all dates, including the ones not used for deriving the clusters\n",
    "Final_Allocations = composites_RMSE(Patterns, IterationResults[1], IterationResults[2])[0] # get labels of all data\n",
    "Labels = pd.Series(Final_Allocations, index=Patterns.time.values, name='Label')\n",
    "Labels.to_csv(dir_loc+'/ProcessedData/PatternAllocations_ERA5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive allocations to patterns only based on ERA5 0 UTC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalies_0UTC(variable):\n",
    "    \n",
    "    # read actual daily values\n",
    "    file_path = dir_loc+'/Data/ERA5/D1_Mean_'+variable+'_00UTC.grb'\n",
    "    Daily = xr.open_dataarray(file_path, engine='cfgrib') # read data\n",
    "    Daily = Daily.reset_coords(drop=True).astype('float32') # float 32 and drop not used variables\n",
    "    Daily = Daily.sel(latitude=slice(Area_used[0], Area_used[2]), # adjust domain for having only\n",
    "                      longitude=slice(Area_used[1], Area_used[3])) # .. area of actual data used\n",
    "    \n",
    "    Daily = Daily.rolling(time=2).mean().dropna('time') # get daily field by averaging 0UTC of same and next day\n",
    "    Daily = Daily.assign_coords({'time': Daily.time.values-np.timedelta64(1, 'D')}) # data refer to the previous day  \n",
    "    \n",
    "    Daily_Patt = Daily.sel(time=Clusters.index) # select only dates that were used for deriving the patterns\n",
    "    \n",
    "    actual_days = Daily.time.values # get actual timesteps\n",
    "    dates_grouped = pd.to_datetime(actual_days).strftime('%m%d') # get Month-Day of each timestep\n",
    "    Daily = Daily.assign_coords({'time': dates_grouped}) # change the time to Month-Day\n",
    "    \n",
    "    dates_grouped_patterns = pd.to_datetime(Daily_Patt.time.values).strftime('%m%d') # get Month-Day of each timestep\n",
    "    \n",
    "    # 5-day smoothed climatology. Rolling can be applied directly because the daily data refer to consequtive days. If\n",
    "    # days are not consecutive, firstly the xr.resample should be applied, so that missing days are generated with NaN\n",
    "    Smoothed = Daily_Patt.rolling(time=5, center=True, min_periods=1).mean() # 5-day smoothing\n",
    "    Smoothed = Smoothed.assign_coords({'time': dates_grouped_patterns}) # change the time to Month-Day\n",
    "    \n",
    "    Climatology = Smoothed.groupby('time').mean() # climatology of the smoothed data\n",
    "    \n",
    "    Anomalies = Daily.groupby('time') - Climatology\n",
    "    Anomalies = Anomalies.assign_coords({'time': actual_days}) # change back to the original timestep information\n",
    "    Anomalies.name = variable\n",
    "    Daily = Daily.assign_coords({'time': actual_days}) \n",
    "    Daily.name = variable\n",
    "    \n",
    "    return Anomalies, Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "Patterns_0UTCAll = list(tqdm.tqdm(pool.imap(anomalies_0UTC, Vars_used), total=len(Vars_used), position=0, leave=True))\n",
    "pool.close()\n",
    "\n",
    "Patterns_0UTC = xr.merge([i[0] for i in Patterns_0UTCAll])\n",
    "Daily_0UTC = xr.merge([i[1] for i in Patterns_0UTCAll])\n",
    "\n",
    "del(pool, Patterns_0UTCAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights = np.cos(np.deg2rad(Daily_0UTC.latitude)) # weights due to areal differences of each grid\n",
    "Weights_2d = Weights.expand_dims({'longitude': Daily_0UTC.longitude.values}) # weights on both lat-lon\n",
    "\n",
    "Corr_Pat = xs.pearson_r(Daily_0UTC.to_array(), DailyFull.to_array(), # spatial correlation of ERA5 data based ...\n",
    "                        dim=['latitude', 'longitude'], weights=Weights_2d) # ... on full hourly and only 0 UTC data\n",
    "Corr_Pat = Corr_Pat.to_dataframe('Corr').pivot_table(index='time', columns='variable', values='Corr')\n",
    "Corr_Pat['Mean'] = Corr_Pat.mean(axis=1)\n",
    "Corr_Pat = Corr_Pat.describe()\n",
    "Corr_Pat.to_csv(dir_loc+'/ProcessedData/Correlation_Full_0UTC.csv')\n",
    "Corr_Pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate days to cluster, using the ERA5 0UTC data only for the reference period (1979-2019)\n",
    "Alcs0UTC_OUTC = composites_RMSE(Patterns_0UTC.sel(time=Clusters.index), IterationResults[1])[0]\n",
    "Labels_0UTC = pd.Series(Alcs0UTC_OUTC, index=Clusters.index, name='Label')\n",
    "Labels_0UTC.to_csv(dir_loc+'/ProcessedData/PatternAllocations_ERA5_0UTC.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alcs = pd.DataFrame({'Actual': Clusters.values, '0_UTC': Alcs0UTC_OUTC}, index=Clusters.index)\n",
    "print('Percentage of mismatch:')\n",
    "Alcs.apply(lambda x: x!=Alcs['Actual']).sum()/len(Alcs)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sorted_Dates = np.array(pd.date_range('20040101', '20041231').strftime('%m%d')) # a leap year for getting all dates\n",
    "StartSummerHalf = np.where(Sorted_Dates=='0416')[0]\n",
    "EndSummerHalf = np.where(Sorted_Dates=='1015')[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_flagging(valid_dates, temp_subset):\n",
    "    \n",
    "    valid_dates = pd.to_datetime(valid_dates)\n",
    "    \n",
    "    if temp_subset == 'All':\n",
    "        temporal_flag = ['All']*len(valid_dates)\n",
    "    elif temp_subset == 'HalfYear':\n",
    "        temporal_flag_aux = pd.Series([i[-4:] for i in valid_dates.strftime('%Y%m%d')])\n",
    "        temporal_flag_aux = temporal_flag_aux.map({i: i_c for i_c, i in enumerate(Sorted_Dates)})\n",
    "        temporal_flag_aux = temporal_flag_aux.values\n",
    "        temporal_flag = np.repeat(['WinterHalf'], len(temporal_flag_aux))\n",
    "        temporal_flag[(temporal_flag_aux>=StartSummerHalf) & (temporal_flag_aux<=EndSummerHalf)] = 'SummerHalf'\n",
    "    elif temp_subset == 'Season':\n",
    "        temporal_flag = (valid_dates.month%12 + 3)//3\n",
    "        temporal_flag = temporal_flag.map({1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Autumn'})\n",
    "    elif temp_subset == 'Month':\n",
    "        temporal_flag = valid_dates.month.astype(str)\n",
    "    elif temp_subset == 'DayMonth':\n",
    "        temporal_flag = pd.Series([i[-4:] for i in valid_dates.strftime('%Y%m%d')])\n",
    "        temporal_flag = temporal_flag.values\n",
    "        \n",
    "    return temporal_flag  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alcs['HalfYear'] = temp_flagging(Alcs.index, 'HalfYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_calc(temp_subset):\n",
    "    PercentDifs = Alcs.query('HalfYear == @temp_subset')[['Actual', '0_UTC']]\n",
    "    PercentDifs = PercentDifs.apply(lambda x: x.value_counts()).sort_index()\n",
    "    PercentDifs = PercentDifs.apply(lambda x: x/PercentDifs['Actual'], axis=0)*100\n",
    "    PercentDifs = PercentDifs['0_UTC']\n",
    "    return PercentDifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Biases = pd.DataFrame({'WH': bias_calc(['WinterHalf']), 'SH': bias_calc(['SummerHalf']), \n",
    "                       'Full': bias_calc(['WinterHalf', 'SummerHalf'])})\n",
    "Biases.index = New_names\n",
    "Biases.to_csv(dir_loc+'/ProcessedData/Biases_Full_0UTC.csv')\n",
    "Biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
