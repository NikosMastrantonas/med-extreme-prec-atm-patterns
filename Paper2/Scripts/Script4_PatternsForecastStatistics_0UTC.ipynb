{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Copyright 1996- ECMWF.\n",
    "#\n",
    "# This software is licensed under the terms of the Apache Licence Version 2.0\n",
    "# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "# In applying this licence, ECMWF does not waive the privileges and immunities\n",
    "# granted to it by virtue of its status as an intergovernmental organisation\n",
    "# nor does it submit to any jurisdiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOS\n",
    "This script takes too much time. It is recommended to run it in cluster, and preferably in a different session for each lead time, so multiple sessions can run in parallel.\n",
    "\n",
    "### Note\n",
    "This is essentially same as the \"*Script4_PatternsForecastStatistics*\" script. The only difference is the input data at cell 5, and the outputs naming at cell 28. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstr = 1000 # Number of bootstraps for assessing the statistical significance of the results\n",
    "\n",
    "# get the index values of the 5th, 95th and median number, when data are ordered (for the bootstraping)\n",
    "l_m = int(bootstr*5/100)\n",
    "l_M = int(bootstr*95/100)-1\n",
    "Md = int(bootstr/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/ProcessedData/'\n",
    "output_dir = '/ProcessedData/ForecastsPatterns/'\n",
    "\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True) # generate subfolder for storing the results\n",
    "offset_days = 45 # days to offset before/after date of interest for defining \"DayMonth\" moving-window clim/pers\n",
    "offset_months = 1 # months to offset before/after date of interest for defining \"Monthly\" moving_window clim/pers\n",
    "FlexWindows = [0, 1, 3, 5] # flex windows used for assessing the brier score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActualClusters = pd.read_csv(input_dir+'/PatternAllocations_ERA5_0UTC.csv', index_col=0)\n",
    "ActualClusters.index = pd.to_datetime(ActualClusters.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllocatedClusters = pd.read_csv(input_dir+'ForecastsClusterAllocations.csv').iloc[:, 1:]\n",
    "AllocatedClusters[['time', 'valid_time']] = AllocatedClusters[['time', 'valid_time']].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Members = AllocatedClusters.number.unique()\n",
    "ActualMem_len = (All_Members>=0).sum()\n",
    "LeadDays = AllocatedClusters.step.unique()\n",
    "del(All_Members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_States = list(set(ActualClusters.Label)) # all unique clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sorted_Dates = np.array(pd.date_range('20040101', '20041231').strftime('%m%d')) # a leap year for getting all dates\n",
    "StartSummerHalf = np.where(Sorted_Dates=='0416')[0]\n",
    "EndSummerHalf = np.where(Sorted_Dates=='1015')[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_flagging(valid_dates, temp_subset):\n",
    "    \n",
    "    valid_dates = pd.to_datetime(valid_dates)\n",
    "    \n",
    "    if temp_subset == 'All':\n",
    "        temporal_flag = ['All']*len(valid_dates)\n",
    "    elif temp_subset == 'HalfYear':\n",
    "        temporal_flag_aux = pd.Series([i[-4:] for i in valid_dates.strftime('%Y%m%d')])\n",
    "        temporal_flag_aux = temporal_flag_aux.map({i: i_c for i_c, i in enumerate(Sorted_Dates)})\n",
    "        temporal_flag_aux = temporal_flag_aux.values\n",
    "        temporal_flag = np.repeat(['WinterHalf'], len(temporal_flag_aux))\n",
    "        temporal_flag[(temporal_flag_aux>=StartSummerHalf) & (temporal_flag_aux<=EndSummerHalf)] = 'SummerHalf'\n",
    "    elif temp_subset == 'Season':\n",
    "        temporal_flag = (valid_dates.month%12 + 3)//3\n",
    "        temporal_flag = temporal_flag.map({1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Autumn'})\n",
    "    elif temp_subset == 'Month':\n",
    "        temporal_flag = valid_dates.month.astype(str)\n",
    "    elif temp_subset == 'DayMonth':\n",
    "        temporal_flag = pd.Series([i[-4:] for i in valid_dates.strftime('%Y%m%d')])\n",
    "        temporal_flag = temporal_flag.values\n",
    "        \n",
    "    return temporal_flag  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_tag in ['HalfYear', 'Season', 'Month', 'DayMonth']:\n",
    "    ActualClusters[i_tag] = temp_flagging(ActualClusters.index, i_tag)\n",
    "    \n",
    "del(i_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequencies(data_flags=ActualClusters.Label.values, temporal_flags=ActualClusters.index,\n",
    "                temporal_window=0, subset_used='All'):\n",
    "    \n",
    "    Subsets_used = ['All', 'HalfYear', 'Season', 'Month', 'DayMonth']\n",
    "    if subset_used not in Subsets_used: return print('Wrong input for \"subset_used\", should be from:', Subsets_used)\n",
    "    \n",
    "    clim_data = pd.Series(data_flags, index=temporal_flags) # get data for transitions' calculations    \n",
    "    full_dates_range = pd.date_range(temporal_flags.min(), temporal_flags.max()) # range with all dates\n",
    "    clim_data = clim_data.reindex(full_dates_range) # fill possible missing dates\n",
    "    \n",
    "    # target date for each temporal offset, so that latter temporal subsetting can be implemented\n",
    "    Dates_targets = [clim_data.index + np.timedelta64(i_shift, 'D') for i_shift in range(temporal_window+1)]\n",
    "    Dates_targets = [pd.Series(i_target) for i_target in Dates_targets] # convert from index to series\n",
    "    Dates_targets = pd.concat(Dates_targets, axis=1) # concat to 1 single dataframe\n",
    "    Dates_targets.index = range(len(Dates_targets))\n",
    "\n",
    "    clim_data = [clim_data.shift(-i_shift) for i_shift in range(temporal_window+1)] # shift all necessary temp offsets\n",
    "    clim_data = pd.concat(clim_data, axis=1) # concat to 1 single dataframe\n",
    "    clim_data.index = range(len(clim_data))\n",
    "    \n",
    "    Dates_targets.columns = clim_data.columns # same name of columns for masking later on\n",
    "    Dates_targets = Dates_targets.apply(lambda x: temp_flagging(x.values, subset_used), axis=0)\n",
    "\n",
    "    if subset_used == 'All':\n",
    "        index_used = ['All']\n",
    "    elif subset_used == 'HalfYear':\n",
    "        index_used = ['WinterHalf', 'SummerHalf']\n",
    "    elif subset_used == 'Season':\n",
    "        index_used = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "    elif subset_used == 'Month':\n",
    "        index_used = list(np.arange(1, 13).astype(str))\n",
    "    elif subset_used == 'DayMonth':\n",
    "        index_used = Sorted_Dates\n",
    "    \n",
    "    Freqs = pd.DataFrame(0, index=index_used, columns=Unique_States)\n",
    "    for date_check in index_used:\n",
    "        dates_check_all = date_check\n",
    "        if subset_used == 'Month':\n",
    "            dates_check_all = int(dates_check_all)\n",
    "            dates_check_all = np.arange(dates_check_all-offset_months, dates_check_all+offset_months+1) # used months\n",
    "            # correct possible values outside [1,12] by circulating the months (1,2,3,...,12,1,2,...11,,12,1,2)\n",
    "            for loc, mn_i in enumerate(dates_check_all): \n",
    "                if mn_i>12:\n",
    "                    dates_check_all[loc] = mn_i - 12\n",
    "                elif mn_i<1:\n",
    "                    dates_check_all[loc] = 12 + mn_i\n",
    "            dates_check_all = dates_check_all.astype(str)\n",
    "        elif subset_used == 'DayMonth':\n",
    "            central_loc = np.where(Sorted_Dates==dates_check_all)[0]\n",
    "            dates_check_all = np.linspace(central_loc-offset_days, central_loc+offset_days, 2*offset_days+1)\n",
    "            for loc, i_date_loc in enumerate(dates_check_all): \n",
    "                if i_date_loc >= len(Sorted_Dates):\n",
    "                    dates_check_all[loc] = dates_check_all[loc] - len(Sorted_Dates)\n",
    "            dates_check_all = np.take(Sorted_Dates, dates_check_all.astype(int))\n",
    "\n",
    "        # keep the final subset of interest, by replacing all data not belonging to the temporal subset by NaN\n",
    "        dates_check_all = list(np.array(dates_check_all).flatten())\n",
    "        clim_data_kept = clim_data.where(Dates_targets.isin(dates_check_all))\n",
    "        clim_data_kept.dropna(inplace=True) # drop rows with at least 1 NaN (not a full set thus a bit biased)\n",
    "        \n",
    "        for i_state in range(len(Unique_States)):\n",
    "            Counts_state = (clim_data_kept==i_state).sum(axis=1)>0\n",
    "            Freqs.loc[date_check, i_state] = Counts_state.sum()/len(clim_data_kept)\n",
    "            \n",
    "    return Freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqs_all(temporal_window):\n",
    "    Fr = [frequencies(temporal_window=temporal_window), \n",
    "          frequencies(temporal_window=temporal_window, subset_used='HalfYear'), \n",
    "          frequencies(temporal_window=temporal_window, subset_used='DayMonth')]\n",
    "    \n",
    "    Fr = pd.concat(Fr)\n",
    "    Fr = xr.DataArray(Fr, dims={'temp_subset': Fr.index, 'cluster': Fr.columns})\n",
    "    return Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "FreqsAll = list(tqdm.tqdm(pool.imap(freqs_all, FlexWindows), total=len(FlexWindows), position=0, leave=True))\n",
    "pool.close()\n",
    "FreqsAll = xr.concat(FreqsAll, dim=pd.Index(FlexWindows, name='temp_window'))\n",
    "del(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transitions(data_flags=ActualClusters.Label.values, temporal_flags=ActualClusters.index,\n",
    "                      lead=1, temporal_window=0, subset_used='All'):\n",
    "    \n",
    "    Subsets_used = ['All', 'HalfYear', 'Season', 'Month', 'DayMonth']\n",
    "    if subset_used not in Subsets_used: return print('Wrong input for \"subset_used\", should be from:', Subsets_used)\n",
    "   \n",
    "    # get all shift_dates of interest, and keep only the ones that are in the \"LeadDays\" for comperability \n",
    "    shift_days = list(range(lead-temporal_window, lead+temporal_window+1)) # get all lead_days of interest\n",
    "    shift_days = [i_shift for i_shift in shift_days if i_shift in LeadDays] # keep lead_days available in LeadDays\n",
    "\n",
    "    transitions_data = pd.Series(data_flags, index=temporal_flags) # get data for transitions' calculations    \n",
    "    full_dates_range = pd.date_range(temporal_flags.min(), temporal_flags.max()) # range with all dates\n",
    "    transitions_data = transitions_data.reindex(full_dates_range) # fill possible missing dates\n",
    "    transitions_data = transitions_data.fillna(-1) # any missing data fill with -1, since clusters have values >= 0\n",
    "    States_actual = transitions_data.values # actual state (cluster) at each observation (row)\n",
    "    \n",
    "    # target date for each lead time, so that latter temporal subsetting can be implemented\n",
    "    Dates_targets = [transitions_data.index + np.timedelta64(i_shift, 'D') for i_shift in shift_days]\n",
    "    Dates_targets = [pd.Series(i_target) for i_target in Dates_targets] # convert from index to series\n",
    "    Dates_targets = pd.concat(Dates_targets, axis=1) # concat to 1 single dataframe\n",
    "    Dates_targets.index = States_actual # rename the index from actual date to actual state (cluster)\n",
    "\n",
    "    transitions_data = [transitions_data.shift(-i) for i in shift_days] # shift all necessary lead times\n",
    "    transitions_data = pd.concat(transitions_data, axis=1) # concat to 1 single dataframe\n",
    "    transitions_data.index = States_actual # rename the index from actual date to actual state (cluster)\n",
    "    transitions_data.replace({-1:np.nan}, inplace=True) # replace back all -1 to NaN since those dates don't exist   \n",
    "    \n",
    "    Dates_targets.columns = transitions_data.columns # same name of columns for masking later on\n",
    "    Dates_targets = Dates_targets.apply(lambda x: temp_flagging(x.values, subset_used), axis=0)\n",
    "\n",
    "    if subset_used == 'All':\n",
    "        index_used = ['All']\n",
    "    elif subset_used == 'HalfYear':\n",
    "        index_used = ['WinterHalf', 'SummerHalf']\n",
    "    elif subset_used == 'Season':\n",
    "        index_used = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "    elif subset_used == 'Month':\n",
    "        index_used = list(np.arange(1, 13).astype(str))\n",
    "    elif subset_used == 'DayMonth':\n",
    "        index_used = Sorted_Dates\n",
    "    \n",
    "    TransitionsMatrix = np.zeros([len(index_used), len(Unique_States), len(Unique_States)])\n",
    "    for i_date_check, date_check in enumerate(index_used):\n",
    "        dates_check_all = date_check\n",
    "        if subset_used == 'Month':\n",
    "            dates_check_all = int(dates_check_all)\n",
    "            dates_check_all = np.arange(dates_check_all-offset_months, dates_check_all+offset_months+1) # used months\n",
    "            # correct possible values outside [1,12] by circulating the months (1,2,3,...,12,1,2,...11,,12,1,2)\n",
    "            for loc, mn_i in enumerate(dates_check_all): \n",
    "                if mn_i>12:\n",
    "                    dates_check_all[loc] = mn_i - 12\n",
    "                elif mn_i<1:\n",
    "                    dates_check_all[loc] = 12 + mn_i\n",
    "            dates_check_all = dates_check_all.astype(str)\n",
    "        elif subset_used == 'DayMonth':\n",
    "            central_loc = np.where(Sorted_Dates==dates_check_all)[0]\n",
    "            dates_check_all = np.linspace(central_loc-offset_days, central_loc+offset_days, 2*offset_days+1)\n",
    "            for loc, i_date_loc in enumerate(dates_check_all): \n",
    "                if i_date_loc >= len(Sorted_Dates):\n",
    "                    dates_check_all[loc] = dates_check_all[loc] - len(Sorted_Dates)\n",
    "            dates_check_all = np.take(Sorted_Dates, dates_check_all.astype(int))\n",
    "\n",
    "        # keep the final subset of interest, by replacing all data not belonging to the temporal subset by NaN\n",
    "        dates_check_all = list(np.array(dates_check_all).flatten())\n",
    "        transitions_data_kept = transitions_data.where(Dates_targets.isin(dates_check_all))\n",
    "        transitions_data_kept.dropna(inplace=True) # drop rows with at least 1 NaN (not a full set thus a bit biased)\n",
    "        \n",
    "        M = pd.DataFrame(0, columns=Unique_States, index=Unique_States) # DF to store transitions\n",
    "        for from_state in (set(States_actual)- set([-1])): # loop through all available clusters (excluding -1 flag)\n",
    "            subset_from = transitions_data_kept.loc[from_state] # keep the rows with actual cluster == from_state\n",
    "            for to_state in (set(States_actual)- set([-1])):\n",
    "                Trans_FromTo = (subset_from==to_state).sum(axis=1)>0 # check dates when analysed transition occurs\n",
    "                M.loc[from_state, to_state] = Trans_FromTo.sum()/len(subset_from) # percentage of transitions\n",
    "\n",
    "        TransitionsMatrix[i_date_check,:,:] = M\n",
    "    \n",
    "    xr_dims = {'temp_subset': index_used, 'from_cluster': Unique_States, 'to_cluster': Unique_States}\n",
    "    TransitionsMatrix = xr.DataArray(TransitionsMatrix, coords=list(xr_dims.values()) ,dims=list(xr_dims.keys()))\n",
    "    \n",
    "    return TransitionsMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_temporal_subsets(input_data):\n",
    "    \n",
    "    temporal_window, lead_days = input_data\n",
    "    Tr = [transitions(temporal_window=temporal_window, lead=lead_days), \n",
    "          transitions(temporal_window=temporal_window, subset_used='HalfYear', lead=lead_days)]\n",
    "    \n",
    "    Tr = xr.concat(Tr, dim='temp_subset')\n",
    "    return Tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_all(lead_days):\n",
    "    Tr = []\n",
    "    for i_flex in FlexWindows:\n",
    "        Tr.append( trans_temporal_subsets([i_flex, lead_days]) )\n",
    "        \n",
    "    Tr = xr.concat(Tr, dim=pd.Index(FlexWindows, name='temp_window'))\n",
    "    return Tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "TransAll = list(tqdm.tqdm(pool.imap(trans_all, LeadDays), total=len(LeadDays), position=0, leave=True))\n",
    "pool.close()\n",
    "TransAll = xr.concat(TransAll, dim=pd.Index(LeadDays, name='lead_days'))\n",
    "del(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_data(lead_days=1, season_used='All', flex_window=0):\n",
    "    \n",
    "    Seasons_used = ['All', 'WinterHalf', 'SummerHalf']\n",
    "    if season_used not in Seasons_used: return print('Wrong input for \"season_used\", should be from:', Seasons_used)\n",
    "   \n",
    "    temporal_flags_used = 'All' if season_used == 'All' else 'HalfYear' # get aux for generating temporal flags\n",
    "\n",
    "    lead_days_all = list(range(lead_days-flex_window, lead_days+flex_window+1)) # get all lead_days of interest\n",
    "    lead_days_all = [i_lead for i_lead in lead_days_all if i_lead in LeadDays] # keep lead_days available in the data\n",
    "\n",
    "    Frcst_ALL = {i_days: AllocatedClusters.query('step == @i_days') for i_days in lead_days_all} # get all forecasts\n",
    "\n",
    "    # keep only forecast data that belong to the studied temporal subset. Use only if all lead-days are in the subset\n",
    "    used_rows = [temp_flagging(Frcst_ALL[i_days].valid_time.values, temporal_flags_used) for i_days in lead_days_all] \n",
    "    used_rows = pd.concat([pd.Series(i) for i in used_rows], axis=1) # dataframe with temporal flag for each instance\n",
    "    used_rows = ((used_rows == season_used).sum(axis=1)==len(lead_days_all)).values # only if all belong to the subset\n",
    "    Frcst_ALL = {i_days: Frcst_ALL[i_days][used_rows] for i_days in lead_days_all} # get final subset of frcst data\n",
    "    \n",
    "    # subset the ERA5 data for getting the timeseries with the actual regime at each of the lead_days_all\n",
    "    Frcst_Dates = {i_days: sorted(Frcst_ALL[i_days].valid_time.unique()) for i_days in lead_days_all} # dates needed\n",
    "    ERA5_all = {i_days: ActualClusters.loc[dates] for i_days, dates in Frcst_Dates.items()} # ERA5 subset \n",
    "    \n",
    "    Actuals = np.concatenate([i_df[['Label']].values for i_df in list(ERA5_all.values())], axis=1)\n",
    "\n",
    "    # dates at \"lead_days\" before date of interest, for being able to analyse forecasting based on persistence\n",
    "    Persistence_Days = Frcst_Dates[lead_days] - np.timedelta64(lead_days, 'D') \n",
    "    Persistence = ActualClusters.loc[Persistence_Days, ['Label']] # actual regime from ERA5 used for persistence\n",
    "    \n",
    "    Frcst_Data = {} # generate dictionary with the different methods for allocating the patterns (use numpy 3d array)\n",
    "    for i_type in list(AllocatedClusters.columns[4:]): # loop through the pattern allocation methods (after column 4)\n",
    "        i_frcst = {i_days: frcst_subset.pivot_table(index='valid_time', columns='number', values=i_type)\n",
    "                   for i_days, frcst_subset in Frcst_ALL.items()} \n",
    "        # predictions as 3d array (time, ens. mem., days of flexible window)\n",
    "        i_frcst = np.concatenate([i_df.values[..., np.newaxis] for i_df in list(i_frcst.values())], axis=2) \n",
    "        Frcst_Data[i_type] = i_frcst\n",
    "    \n",
    "    return (Frcst_Data, Actuals, Persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Freq_forecasts(input_data):\n",
    "    \n",
    "    Frcst_Data, Actuals, lead_days, season_used, flex_window = input_data\n",
    "    \n",
    "    # because of model clim., there is no reason to seperate control member, but it is considered together with ens.\n",
    "    coords_Member = ['ERA5', 'Reforecasts', 'Mean'] \n",
    "    \n",
    "    Freq_Occur = np.zeros([len(Unique_States), len(coords_Member), len(Frcst_Data)]) \n",
    "    \n",
    "    for k, member_k in enumerate(Frcst_Data): # loop through the clustering options\n",
    "\n",
    "        predict = Frcst_Data[member_k]\n",
    "\n",
    "        for j_cl in Unique_States:\n",
    "            y_true = (Actuals == j_cl).sum(axis=1)>0 # True if cluster exists in at least 1 day on the flex window\n",
    "            Freq_Occur[j_cl, 0, k] = np.sum(y_true)/len(y_true)*100 # Freq of Occurrence on the ERA5 subset used\n",
    "\n",
    "            frcst_mem = (predict[:, 1:, :]==j_cl).sum(axis=2) # number of days cluster observed for members (not mean)\n",
    "            frcst_mem = frcst_mem>0\n",
    "            Freq_Occur[j_cl, 1, k] = np.sum(frcst_mem)/frcst_mem.size*100 # Freq of Occur of all forecast members\n",
    "            \n",
    "            mean_mem = (predict[:, 0, :]==j_cl).sum(axis=1) # number of days cluster is observed for mean member\n",
    "            mean_mem = mean_mem>0\n",
    "            Freq_Occur[j_cl, 2, k] = np.sum(mean_mem)/mean_mem.size*100 # Freq of Occurrence of mean member\n",
    "            \n",
    "    Freq_Occur = xr.DataArray(Freq_Occur, \n",
    "                              coords=[list(Unique_States), coords_Member, list(Frcst_Data.keys())], \n",
    "                              dims=['Cluster', 'Member', 'Method']) # convert np array to xr dataarray\n",
    "    \n",
    "    Freq_Occur = Freq_Occur.assign_coords({'Lead_days':lead_days, 'Flex_win': flex_window, 'Season': season_used})\n",
    "    \n",
    "    Freq_Occur_Difs = (Freq_Occur-Freq_Occur.sel(Member='ERA5'))/Freq_Occur.sel(Member='ERA5')*100 # get freq biases\n",
    "    Freq_Occur = xr.concat([Freq_Occur, Freq_Occur_Difs], dim=pd.Index(['Freq', 'FreqBias'], name='Var'))\n",
    "    \n",
    "    return Freq_Occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BS_decomposition(actual, prob):\n",
    "    \n",
    "    Probs = pd.DataFrame({'Prob': prob, 'Actual': actual})\n",
    "\n",
    "    ProbsStats = pd.DataFrame({'Mi':np.nan}, index=np.arange(ActualMem_len+1)/ActualMem_len)\n",
    "    Mi = Probs.groupby(['Prob']).apply(lambda x: (x['Actual']==1).sum())\n",
    "    Ni = Probs.groupby('Prob')['Actual'].count()\n",
    "    ProbsStats.loc[Mi.index, 'Mi'] = Mi\n",
    "    ProbsStats.loc[Ni.index, 'Ni'] = Ni\n",
    "    ProbsStats.fillna(0, inplace=True)\n",
    "    M_all = ProbsStats.Mi.sum()\n",
    "    N_all = ProbsStats.Ni.sum()\n",
    "    ProbsStats['RelFreq'] = ProbsStats.Mi/ProbsStats.Ni\n",
    "    ProbsStats['Rel'] = (ProbsStats.index - ProbsStats['RelFreq'])**2\n",
    "    ProbsStats['Res'] = (M_all/N_all - ProbsStats['RelFreq'])**2 * (ProbsStats.Ni/ProbsStats.Ni.sum())\n",
    "    ProbsStats['Rel_Weighted'] = ProbsStats.Rel * (ProbsStats.Ni/N_all)\n",
    "    ProbsStats['Res_Weighted'] = ProbsStats.Res * (ProbsStats.Ni/ProbsStats.Ni.sum())\n",
    "    Reliability = ProbsStats['Rel_Weighted'].sum()\n",
    "    Resolution = ProbsStats['Res_Weighted'].sum()\n",
    "    Uncertainty = M_all/N_all*(1-M_all/N_all)\n",
    "    \n",
    "    AgrStats = pd.Series([Reliability, Resolution, Uncertainty], index=['Reliability', 'Resolution', 'Uncertainty'])\n",
    "    \n",
    "    return (ProbsStats.iloc[:, :], AgrStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(input_data):\n",
    "    \n",
    "    Frcst_Data, Actuals, Persistence, lead_days, season_used, flex_window = input_data\n",
    "    methods_used = list(Frcst_Data.keys())\n",
    "    \n",
    "    # create numpy arrays for storing the Brier Score Decomposition statistics\n",
    "    probs_all = np.arange(ActualMem_len+1)/ActualMem_len\n",
    "    BS_Dec_Se = np.zeros([len(probs_all), 7, len(Unique_States), len(methods_used)])\n",
    "    BS_Dec_Ag = np.zeros([3, len(Unique_States), len(methods_used)])\n",
    "    i_k_decomp = -1\n",
    "    \n",
    "    # create arrays and auxiliary elements for calculating Brier Score of ens. (proper & fair) and of reference\n",
    "    methods_used = methods_used+['Frequencies', 'Persistence']\n",
    "    methods_used_all = methods_used+[i+'_Fair'for i in methods_used[:-2]] # add Fair BS data for the ens. forecasts\n",
    "    \n",
    "    forecasted_dates = Persistence.index + np.timedelta64(lead_days, 'D') # central dates of forecast\n",
    "    forecasted_dates_freqs_flag = temp_flagging(forecasted_dates, 'DayMonth') # temporal flags for climatology\n",
    "    forecasted_dates_persi_flag = temp_flagging(forecasted_dates, 'HalfYear') # temporal flags for persistence\n",
    "\n",
    "    BS_Clusters = np.zeros([len(Unique_States), len(methods_used_all)]) \n",
    "    \n",
    "    for i_k, k_method in enumerate(methods_used): # loop through the allocation methods\n",
    "        i_k_decomp += 1 # add 1 for having the correct index on the Decomposition arrays\n",
    "        if k_method in list(Frcst_Data.keys()):\n",
    "            predict = Frcst_Data[k_method][:, 1:, :] # remove ens mean (1st column, as it has -1 flag)\n",
    "\n",
    "        for j_cl in Unique_States: # loop through each cluster for calculating the brier score\n",
    "            \n",
    "            # get the boolean indicating whether the cluster is observed or not in at least 1 day on the flex win.\n",
    "            y_true = (Actuals == j_cl).sum(axis=1)>0\n",
    "            \n",
    "            # get the probabilities of observing the cluster of interest\n",
    "            if k_method == 'Frequencies':\n",
    "                Freqs_used = FreqsAll.sel(temp_window=flex_window, cluster=j_cl)\n",
    "                probs = Freqs_used.sel(temp_subset=forecasted_dates_freqs_flag).values\n",
    "\n",
    "            elif k_method == 'Persistence':\n",
    "                Trans_used = TransAll.sel(lead_days=lead_days, temp_window=flex_window, to_cluster=j_cl)\n",
    "                probs = [Trans_used.sel(from_cluster=Persistence.values.flatten()[i],\n",
    "                                        temp_subset=forecasted_dates_persi_flag[i]).values \n",
    "                         for i in range(len(Actuals))]\n",
    "                probs = np.array(probs).flatten()\n",
    "\n",
    "            else:\n",
    "                probs = (predict==j_cl).sum(axis=2) # number of days the cluster is observed for each ens. member\n",
    "                mem_counts = (probs>0)*1 # Boolean; 1 if cluster is observed at least once for each esn. mem.\n",
    "                mem_counts = mem_counts.sum(axis=1) # total number of ens. members indicating the cluster\n",
    "                probs = mem_counts/probs.shape[1] # % of ens. members. indicating the cluster\n",
    "\n",
    "                # get the brier score decomposition statistics for the ens. forecasts\n",
    "                DecompStats = BS_decomposition(y_true, probs)\n",
    "                \n",
    "                BS_Dec_Se[:, :, j_cl, i_k_decomp] = DecompStats[0].values\n",
    "                BS_Dec_Ag[:, j_cl, i_k_decomp] = DecompStats[1].values\n",
    "\n",
    "            # calculate proper brier score\n",
    "            brier = metrics.brier_score_loss(y_true=y_true, y_prob=probs) # brier score\n",
    "            BS_Clusters[j_cl, i_k] = brier\n",
    "            \n",
    "            # calculate fair brier score only for ens. forecasts\n",
    "            if k_method in list(Frcst_Data.keys()):\n",
    "                m_members = predict.shape[1]\n",
    "                adjustment = mem_counts*(m_members-mem_counts)/m_members**2/(m_members-1)\n",
    "                adjustment_mean = np.mean(adjustment)\n",
    "                brier_fair = brier - adjustment_mean\n",
    "                BS_Clusters[j_cl, i_k+len(methods_used)] = brier_fair\n",
    "            \n",
    "    # convert np array to xr dataarray\n",
    "    BS_Clusters = xr.DataArray(BS_Clusters, coords=[Unique_States, methods_used_all], dims=['Cluster', 'Method']) \n",
    "\n",
    "    # number of actual instances that each cluster is observed for calculating weighted Combo Brier Score\n",
    "    Instances_all = []\n",
    "    for j_cl in Unique_States: # loop through each cluster for calculating the brier score\n",
    "        y_true = (Actuals == j_cl).sum(axis=1)>0 # True if cluster exists in at least 1 day on the flex win.\n",
    "        Instances_all.append(y_true.sum()) # total observed instances\n",
    "    Instances_all = np.array(Instances_all)\n",
    "\n",
    "    # calculate average brier score: either consider equal weights for each cluster (macro), or weighted (weighted)\n",
    "    BS_Combo = np.zeros([2, len(methods_used_all)]) # Type, Method\n",
    "\n",
    "    BS_Combo[0, :] = np.average(BS_Clusters, axis=0)\n",
    "    BS_Combo[1, :] = np.average(BS_Clusters, axis=0, weights=Instances_all)\n",
    "\n",
    "    BS_Combo = xr.DataArray(BS_Combo, coords=[['Macro', 'Weighted'], methods_used_all], dims=['Type', 'Method'])\n",
    "    \n",
    "    \n",
    "    BS_Dec_Se = xr.DataArray(BS_Dec_Se, dims=['Probs', 'Stat', 'Cluster', 'Method'], \n",
    "                             coords=[probs_all, ['Mi', 'Ni', 'RelFreq', 'Rel', 'Res', 'Rel_Weight', 'Res_Weight'], \n",
    "                                     Unique_States, list(Frcst_Data.keys())])\n",
    "    BS_Dec_Ag = xr.DataArray(BS_Dec_Ag, dims=['Stat', 'Cluster', 'Method'], \n",
    "                             coords=[['Rel.', 'Res.', 'Unc.'], Unique_States, list(Frcst_Data.keys())])   \n",
    "    \n",
    "    # assign additional coordinates with the specified arguments\n",
    "    BS_Clusters = BS_Clusters.assign_coords({'Lead_days':lead_days, 'Flex_win': flex_window, 'Season': season_used})\n",
    "    BS_Combo = BS_Combo.assign_coords({'Lead_days':lead_days, 'Flex_win': flex_window, 'Season': season_used})\n",
    "    BS_Dec_Ag = BS_Dec_Ag.assign_coords({'Lead_days':lead_days, 'Flex_win': flex_window, 'Season': season_used})\n",
    "    BS_Dec_Se = BS_Dec_Se.assign_coords({'Lead_days':lead_days, 'Flex_win': flex_window, 'Season': season_used})\n",
    "    \n",
    "    return {'Combo': BS_Combo, 'Clusters': BS_Clusters, 'Dec_Aggr': BS_Dec_Ag, 'Dec_Seg': BS_Dec_Se}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BS_bootstrap_summary_statistics(dict_key):\n",
    "    \n",
    "    BS_data = xr.concat([i[dict_key] for i in BS_BS_Statistics], dim='bootstrap')\n",
    "    BS_data_ordered = BS_data.argsort(axis=0)\n",
    "    BS_data_ordered_values = np.take_along_axis(BS_data.values, BS_data_ordered.values, axis=0)     \n",
    "    \n",
    "    # get the quantiles from the actual bootstrapping statistics\n",
    "    Lower_BS = BS_data[0]*0 + BS_data_ordered_values[l_m]\n",
    "    Upper_BS = BS_data[0]*0 + BS_data_ordered_values[l_M]\n",
    "\n",
    "    # append the actual data to the full bootstraps so now the median value can be extracted\n",
    "    BS_data = xr.concat([BS_data, Actual_BS_Statistics[dict_key]], dim='bootstrap')\n",
    "    BS_data_ordered = BS_data.argsort(axis=0)\n",
    "    BS_data_ordered_values = np.take_along_axis(BS_data.values, BS_data_ordered.values, axis=0)\n",
    "    \n",
    "    Median_BS = BS_data[0]*0 + BS_data_ordered_values[Md]\n",
    "\n",
    "    # combine all data to the final xarrays\n",
    "    dim_name = pd.Index(['P5', 'P50', 'Actual', 'P95'], name='bootstrap')\n",
    "    Final_BS = xr.concat([Lower_BS, Median_BS, Actual_BS_Statistics[dict_key], Upper_BS], dim=dim_name)\n",
    "    Final_BS.name = 'BS'\n",
    "    \n",
    "    return Final_BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BSS_bootstrap_summary_statistics(dict_key):\n",
    "    \n",
    "    BS_data = xr.concat([i[dict_key] for i in BS_BS_Statistics], dim='bootstrap')\n",
    "    BSS_data = 1 - BS_data/BS_data.sel(Method=['Frequencies', 'Persistence']).min('Method')\n",
    "    BSS_data_ordered = BSS_data.argsort(axis=0)\n",
    "    BSS_data_ordered_values = np.take_along_axis(BSS_data.values, BSS_data_ordered.values, axis=0)     \n",
    "    \n",
    "    # get the quantiles from the actual bootstrapping statistics\n",
    "    Lower_BSS = BSS_data[0]*0 + BSS_data_ordered_values[l_m]\n",
    "    Upper_BSS = BSS_data[0]*0 + BSS_data_ordered_values[l_M]\n",
    "\n",
    "    # append the actual data to the full bootstraps so now the median value can be extracted\n",
    "    BS_data = xr.concat([BS_data, Actual_BS_Statistics[dict_key]], dim='bootstrap')\n",
    "    BSS_data = 1 - BS_data/BS_data.sel(Method=['Frequencies', 'Persistence']).min('Method')\n",
    "    BSS_data_ordered = BSS_data.argsort(axis=0)\n",
    "    BSS_data_ordered_values = np.take_along_axis(BSS_data.values, BSS_data_ordered.values, axis=0)     \n",
    "    \n",
    "    Median_BSS = BSS_data[0]*0 + BSS_data_ordered_values[Md]\n",
    "    \n",
    "    Actual_BSS = Actual_BS_Statistics[dict_key]\n",
    "    Actual_BSS = 1 - Actual_BSS/Actual_BSS.sel(Method=['Frequencies', 'Persistence']).min('Method')\n",
    "    \n",
    "    # combine all data to the final xarrays\n",
    "    dim_name = pd.Index(['P5', 'P50', 'Actual', 'P95'], name='bootstrap')\n",
    "    Final_BSS = xr.concat([Lower_BSS, Median_BSS, Actual_BSS, Upper_BSS], dim=dim_name)\n",
    "    Final_BSS.name = 'BSS'\n",
    "    \n",
    "    Wins = (BSS_data>0).sum('bootstrap')\n",
    "    Wins = Wins/(bootstr+1)\n",
    "    Wins.name = 'Sign'\n",
    "    \n",
    "    return (Final_BSS, Wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_bootstrapped(lead_days=1, season_used='All', flex_window=0):\n",
    "    \n",
    "    Frcst_Data, Actuals, Persistence = subset_data(lead_days, season_used, flex_window)\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    BS_indices = np.random.choice(len(Actuals), len(Actuals)*bootstr) # generate all bootstrapped values\n",
    "    BS_indices = np.array_split(BS_indices, bootstr) # split into the number of subsets (samples)\n",
    "    BS_indices = np.array(BS_indices)\n",
    "    \n",
    "    Frcst_Data_BS = [{i_key: Frcst_Data[i_key][i] for i_key in Frcst_Data} for i in BS_indices]\n",
    "    Actuals_BS = [Actuals[i] for i in BS_indices]\n",
    "    Persistence_BS = [Persistence.iloc[i] for i in BS_indices]\n",
    "    \n",
    "    global BS_BS_Statistics, Actual_BS_Statistics\n",
    "    \n",
    "    Inputs = list(zip(Frcst_Data_BS, Actuals_BS, Persistence_BS, \n",
    "                      [lead_days]*bootstr, [season_used]*bootstr, [flex_window]*bootstr))\n",
    "    pool = multiprocessing.Pool() # object for multiprocessing\n",
    "    BS_BS_Statistics = list(pool.imap(brier_score, Inputs))\n",
    "    pool.close() \n",
    "    \n",
    "    Actual_BS_Statistics = brier_score([Frcst_Data, Actuals, Persistence, lead_days, season_used, flex_window])\n",
    "    \n",
    "    BS_Combo = BS_bootstrap_summary_statistics('Combo')\n",
    "    BSS_Combo, Sign_Combo = BSS_bootstrap_summary_statistics('Combo')\n",
    "    BS_Combo = xr.merge([BS_Combo, BSS_Combo, Sign_Combo])\n",
    "    BS_Clusters = BS_bootstrap_summary_statistics('Clusters')\n",
    "    BSS_Clusters, Sign_Clust = BSS_bootstrap_summary_statistics('Clusters')\n",
    "    BS_Clusters = xr.merge([BS_Clusters, BSS_Clusters, Sign_Clust])\n",
    "    BS_Dec_Ag = BS_bootstrap_summary_statistics('Dec_Aggr')\n",
    "    BS_Dec_Se = BS_bootstrap_summary_statistics('Dec_Seg')\n",
    "    \n",
    "    Sign_Combo = Sign_Combo.assign_coords({'Lead_days':lead_days, 'Flex_win': flex_window, 'Season': season_used})\n",
    "    Sign_Clust = Sign_Clust.assign_coords({'Lead_days':lead_days, 'Flex_win': flex_window, 'Season': season_used})\n",
    "        \n",
    "    del(BS_BS_Statistics, Actual_BS_Statistics)\n",
    "    \n",
    "    Inputs = list(zip(Frcst_Data_BS, Actuals_BS, [lead_days]*bootstr, [season_used]*bootstr, [flex_window]*bootstr))\n",
    "    pool = multiprocessing.Pool() # object for multiprocessing\n",
    "    BS_data = list(pool.imap(Freq_forecasts, Inputs))\n",
    "    pool.close() \n",
    "    \n",
    "    Actual_Freqs = Freq_forecasts([Frcst_Data, Actuals, lead_days, season_used, flex_window])\n",
    "    \n",
    "    BS_data = xr.concat(BS_data, dim='bootstrap')\n",
    "    BS_data_ordered = BS_data.argsort(axis=0)\n",
    "    BS_data_ordered_values = np.take_along_axis(BS_data.values, BS_data_ordered.values, axis=0)\n",
    "    \n",
    "    # get the quantiles from the actual bootstrapping statistics\n",
    "    Lower_Freq = BS_data[0]*0 + BS_data_ordered_values[l_m]\n",
    "    Upper_Freq = BS_data[0]*0 + BS_data_ordered_values[l_M]\n",
    "\n",
    "    # append the actual data to the full bootstraps so now the median value can be extracted\n",
    "    BS_data = xr.concat([BS_data, Actual_Freqs], dim='bootstrap')\n",
    "    BS_data_ordered = BS_data.argsort(axis=0)\n",
    "    BS_data_ordered_values = np.take_along_axis(BS_data.values, BS_data_ordered.values, axis=0)\n",
    "\n",
    "    Median_Freq = BS_data[0]*0 + BS_data_ordered_values[Md]\n",
    "\n",
    "    # combine all data to the final xarrays\n",
    "    dim_name = pd.Index(['BS_5', 'BS_Median', 'Actual', 'BS_95'], name='bootstrap')\n",
    "    FinFreq = xr.concat([Lower_Freq, Median_Freq, Actual_Freqs, Upper_Freq], dim=dim_name)    \n",
    "    \n",
    "    return {'Combo': BS_Combo, 'Clusters': BS_Clusters, \n",
    "            'Dec_Aggr': BS_Dec_Ag, 'Dec_Seg': BS_Dec_Se, 'Freq': FinFreq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasts_seasonal_statistics(input_data):\n",
    "    \n",
    "    lead_days = input_data[0]\n",
    "    flex_window = input_data[1]\n",
    "    \n",
    "    # brier statistics\n",
    "    All = statistics_bootstrapped(lead_days=lead_days, season_used='All', flex_window=flex_window)\n",
    "    Winter = statistics_bootstrapped(lead_days=lead_days, season_used='WinterHalf', flex_window=flex_window)\n",
    "    Summer = statistics_bootstrapped(lead_days=lead_days, season_used='SummerHalf', flex_window=flex_window)\n",
    "    \n",
    "    Brier_Clusters = [i['Clusters'] for i in [All, Winter, Summer]]\n",
    "    Brier_Clusters = xr.concat(Brier_Clusters, dim='Season')\n",
    "    Brier_Combo = [i['Combo'] for i in [All, Winter, Summer]]\n",
    "    Brier_Combo = xr.concat(Brier_Combo, dim='Season')\n",
    "    Decomp_Aggr = [i['Dec_Aggr'] for i in [All, Winter, Summer]]\n",
    "    Decomp_Aggr = xr.concat(Decomp_Aggr, dim='Season')\n",
    "    Decomp_Segm = [i['Dec_Seg'] for i in [All, Winter, Summer]]\n",
    "    Decomp_Segm = xr.concat(Decomp_Segm, dim='Season')\n",
    "    Freq_Occur = [i['Freq'] for i in [All, Winter, Summer]]\n",
    "    Freq_Occur = xr.concat(Freq_Occur, dim='Season')\n",
    "    \n",
    "    return {'Brier_Clusters': Brier_Clusters, 'Brier_Combo': Brier_Combo, \n",
    "            'Freq_Occur': Freq_Occur, 'Decomp_Aggr': Decomp_Aggr, 'Decomp_Segm': Decomp_Segm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasts_flexwind_statistics(lead_days):\n",
    "    \n",
    "    FlexAll = [forecasts_seasonal_statistics([lead_days, i]) for i in FlexWindows]\n",
    "    \n",
    "    Brier_Clusters = [i['Brier_Clusters'] for i in FlexAll]\n",
    "    Brier_Clusters = xr.concat(Brier_Clusters, dim='Flex_win')\n",
    "    Brier_Combo = [i['Brier_Combo'] for i in FlexAll]\n",
    "    Brier_Combo = xr.concat(Brier_Combo, dim='Flex_win')\n",
    "    Freq_Occur = [i['Freq_Occur'] for i in FlexAll]\n",
    "    Freq_Occur = xr.concat(Freq_Occur, dim='Flex_win')\n",
    "    Decomp_Aggr = [i['Decomp_Aggr'] for i in FlexAll]\n",
    "    Decomp_Aggr = xr.concat(Decomp_Aggr, dim='Flex_win')\n",
    "    Decomp_Segm = [i['Decomp_Segm'] for i in FlexAll]\n",
    "    Decomp_Segm = xr.concat(Decomp_Segm, dim='Flex_win')\n",
    "    \n",
    "    return {'Brier_Clusters': Brier_Clusters, 'Brier_Combo': Brier_Combo, \n",
    "            'Freq_Occur': Freq_Occur, 'Decomp_Aggr': Decomp_Aggr, 'Decomp_Segm': Decomp_Segm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForecastStatistics = []\n",
    "for i_lead in tqdm.tqdm(LeadDays[:]):\n",
    "    ForecastStatistics.append( forecasts_flexwind_statistics(i_lead) )\n",
    "\n",
    "Brier_Clusters = [i['Brier_Clusters'] for i in ForecastStatistics]\n",
    "Brier_Clusters = xr.concat(Brier_Clusters, dim='Lead_days')\n",
    "Brier_Combo = [i['Brier_Combo'] for i in ForecastStatistics]\n",
    "Brier_Combo = xr.concat(Brier_Combo, dim='Lead_days')\n",
    "Decomp_Aggr = [i['Decomp_Aggr'] for i in ForecastStatistics]\n",
    "Decomp_Aggr = xr.concat(Decomp_Aggr, dim='Lead_days')\n",
    "Decomp_Segm = [i['Decomp_Segm'] for i in ForecastStatistics]\n",
    "Decomp_Segm = xr.concat(Decomp_Segm, dim='Lead_days')\n",
    "Freq_Occur = [i['Freq_Occur'] for i in ForecastStatistics]\n",
    "Freq_Occur = xr.concat(Freq_Occur, dim='Lead_days')\n",
    "Freq_Occur = Freq_Occur.to_dataset('Var')\n",
    "\n",
    "# flag data that have significantly different frequencies (this is true if 5% and 95% have biases of same sign)\n",
    "Check = Freq_Occur['FreqBias'].sel(bootstrap='BS_5')*Freq_Occur['FreqBias'].sel(bootstrap='BS_95')\n",
    "Check = Check>0\n",
    "Check.name = 'SignDeviations'\n",
    "Freq_Occur = xr.merge([Freq_Occur, Check])\n",
    "\n",
    "del(ForecastStatistics, i_lead, Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brier_Combo.to_netcdf(output_dir+'BS_Combo_0UTC.nc')\n",
    "Brier_Clusters.to_netcdf(output_dir+'BS_Clusters_0UTC.nc')\n",
    "Decomp_Aggr.to_netcdf(output_dir+'Decomp_Aggr_0UTC.nc')\n",
    "Decomp_Segm.to_netcdf(output_dir+'Decomp_Segm_0UTC.nc')\n",
    "Freq_Occur.to_netcdf(output_dir+'FreqOccur_Forecasts_0UTC.nc')\n",
    "FreqsAll.to_netcdf(output_dir+'FreqsAll_ERA5_0UTC.nc')\n",
    "TransAll.to_netcdf(output_dir+'TransAll_ERA5_0UTC.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
