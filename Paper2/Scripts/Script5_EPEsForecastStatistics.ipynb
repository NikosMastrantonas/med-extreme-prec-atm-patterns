{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (C) Copyright 1996- ECMWF.\n",
    "#\n",
    "# This software is licensed under the terms of the Apache Licence Version 2.0\n",
    "# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "# In applying this licence, ECMWF does not waive the privileges and immunities\n",
    "# granted to it by virtue of its status as an intergovernmental organisation\n",
    "# nor does it submit to any jurisdiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOS\n",
    "This script takes too much time (more than 1 week if run on personal computer). It is recommended to run it in cluster, and preferably in a different session for each lead time, so multiple sessions can run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from pathlib import Path\n",
    "import multiprocessing # parallel processing\n",
    "import tqdm # timing\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Area_used = [48, -10, 27, 41]\n",
    "P_used = [90, 95, 99] # Thresholds for EPEs [90, 95, 99]\n",
    "offset_days = 15 # offset days used for getting the EPEs reference climatology of occurrence\n",
    "bootstraps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = ''\n",
    "output_dir = '/ProcessedData/ForecastsEPEs_Analysis/'\n",
    "\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True) # generate subfolder for storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActualClusters = pd.read_csv(input_dir+'ProcessedData/PatternAllocations_ERA5.csv', index_col=0)\n",
    "ActualClusters.index = pd.to_datetime(ActualClusters.index)\n",
    "n_clusters = len(ActualClusters.Label.unique())\n",
    "\n",
    "AllocatedClusters = pd.read_csv(input_dir+'ProcessedData/ForecastsClusterAllocations.csv').iloc[:, 1:]\n",
    "AllocatedClusters[['time', 'valid_time']] = AllocatedClusters[['time', 'valid_time']].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices for start and end of Summer Half (Summer Half between 16th April - 15th October, inclusive of both dates)\n",
    "Sorted_Dates = np.array(pd.date_range('20040101', '20041231').strftime('%m%d')) # a leap year for getting all dates\n",
    "StartSummerHalf = np.where(Sorted_Dates=='0416')[0]\n",
    "EndSummerHalf = np.where(Sorted_Dates=='1015')[0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_flagging(valid_dates, temp_subset):\n",
    "    \n",
    "    valid_dates = pd.to_datetime(valid_dates)\n",
    "    \n",
    "    if temp_subset == 'All':\n",
    "        temporal_flag = ['All']*len(valid_dates)\n",
    "    elif temp_subset == 'HalfYear':\n",
    "        temporal_flag_aux = pd.Series([i[-4:] for i in valid_dates.strftime('%Y%m%d')])\n",
    "        temporal_flag_aux = temporal_flag_aux.map({i: i_c for i_c, i in enumerate(Sorted_Dates)})\n",
    "        temporal_flag_aux = temporal_flag_aux.values\n",
    "        temporal_flag = np.repeat(['WinterHalf'], len(temporal_flag_aux))\n",
    "        temporal_flag[(temporal_flag_aux>=StartSummerHalf) & (temporal_flag_aux<=EndSummerHalf)] = 'SummerHalf'\n",
    "    elif temp_subset == 'Season':\n",
    "        temporal_flag = (valid_dates.month%12 + 3)//3\n",
    "        temporal_flag = temporal_flag.map({1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Autumn'})\n",
    "    elif temp_subset == 'Month':\n",
    "        temporal_flag = valid_dates.month.astype(str)\n",
    "    elif temp_subset == 'DayMonth':\n",
    "        temporal_flag = pd.Series([i[-4:] for i in valid_dates.strftime('%Y%m%d')])\n",
    "        temporal_flag = temporal_flag.values\n",
    "        \n",
    "    return temporal_flag  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_tag in ['All', 'HalfYear', 'Season', 'Month', 'DayMonth']:\n",
    "    ActualClusters[i_tag] = temp_flagging(ActualClusters.index, i_tag)\n",
    "    \n",
    "del(i_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ERA5 rainfall data\n",
    "Precipitation = xr.open_dataarray(input_dir+'Data/ERA5/D1_Total_Precipitation.grb', engine='cfgrib')\n",
    "Precipitation = Precipitation.reset_coords(drop=True)\n",
    "dates = pd.to_datetime(Precipitation.time.values) # get dates\n",
    "dates = pd.to_datetime(dates.strftime('%Y%m%d')) # convert to 00:00 hour of day\n",
    "Precipitation = Precipitation.assign_coords({'time': dates})\n",
    "Precipitation = Precipitation.sel(longitude=slice(Area_used[1], Area_used[3]), \n",
    "                                  latitude=slice(Area_used[0], Area_used[2]), \n",
    "                                  time=slice('1979', '2020')) # keep only full years 1979-2020\n",
    "\n",
    "precip_dates = Precipitation.time.values # get precipitation dates\n",
    "Lons_all = Precipitation.longitude.values # get longitudes, since due to memory limititations subsets are needed\n",
    "\n",
    "del(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting subset of precipitation data and generating boolean for exceedance of extremes\n",
    "def exceed_boolean(data):\n",
    "    \n",
    "    Quant = data.quantile(np.array(P_used)/100, interpolation='linear', dim='time', keep_attrs=True) # thresholds\n",
    "    Quant = Quant.rename({'quantile': 'percentile'}) # rename coordinate\n",
    "    Quant = Quant.assign_coords({'percentile': P_used}) # assign the dim values based on percentiles\n",
    "\n",
    "    # boolean xarray for identifying if an event is over the threshold\n",
    "    Exceed_xr = [data>Quant.sel(percentile=i_p) for i_p in P_used] # boolean of exceedance per percentile\n",
    "    Exceed_xr = xr.concat(Exceed_xr, dim=pd.Index(P_used, name='percentile')) # concatenate data for all percentiles\n",
    "    \n",
    "    return Exceed_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating \"statistical\" Brier Score based on conditional probabilities of EPEs at subsets\n",
    "def statistical_brier_score(cond_probs, weights, dim_used='subset'):\n",
    "    \n",
    "    weights = np.array(weights)\n",
    "    brier_score = cond_probs - cond_probs**2 # brier score for climatological probabilities\n",
    "    \n",
    "    if len(weights)>1:\n",
    "        brier_score = brier_score.rename({dim_used: 'subsetting'})\n",
    "        brier_score = [brier_score.isel(subsetting=i)*weights[i] for i in range(len(weights))]\n",
    "        brier_score = xr.concat(brier_score, dim='subsetting').sum(dim='subsetting')/weights.sum() \n",
    "    \n",
    "    return brier_score.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating conditional probabilities and relevant Brier Score for DayMonth temporal subsetting\n",
    "def DayMonth_EPEs_conditioning(subset_extremes):\n",
    "\n",
    "    nw_crd = ActualClusters.loc[subset_extremes.time.values, 'DayMonth'] # temporal flag to replace coordinate values\n",
    "    \n",
    "    exceed_flags = subset_extremes.assign_coords({'time': nw_crd.values}) # rename time based on temporal flag\n",
    "    \n",
    "    ConnProb = []\n",
    "    for i_dates_central in Sorted_Dates:\n",
    "        \n",
    "        central_loc = np.where(Sorted_Dates==i_dates_central)[0]\n",
    "        dates_check_all = np.linspace(central_loc-offset_days, central_loc+offset_days, 2*offset_days+1)\n",
    "\n",
    "        for loc, i_date_loc in enumerate(dates_check_all): \n",
    "            if i_date_loc >= len(Sorted_Dates):\n",
    "                dates_check_all[loc] = dates_check_all[loc] - len(Sorted_Dates)\n",
    "        \n",
    "        dates_check_all = np.take(Sorted_Dates, dates_check_all.astype(int)).flatten()\n",
    "        Kept_dates_locs = [np.where(nw_crd.values==i)[0].tolist() for i in dates_check_all]\n",
    "        Kept_dates_locs = np.array([j for i in Kept_dates_locs for j in i])\n",
    "        i_condprob = exceed_flags.isel(time=Kept_dates_locs).sum('time')/len(Kept_dates_locs)\n",
    "        i_condprob = i_condprob.assign_coords({'temporal': i_dates_central})\n",
    "        ConnProb.append(i_condprob)\n",
    "        \n",
    "    ConnProb = xr.concat(ConnProb, dim='temporal')\n",
    "    weights_temp = nw_crd.value_counts() # weights based on occurrence of temporal subsets\n",
    "    weights_temp = weights_temp.reindex(Sorted_Dates).fillna(0) # reorder to same order as the xarray \"ConnProb\"\n",
    "\n",
    "    BS = statistical_brier_score(ConnProb, weights_temp.values, dim_used='temporal') # statistical Brier Score\n",
    "    BS = BS.assign_coords({'Method': 'DayMonth_Temp'}) # assign new coord with the temporal subsetting info\n",
    "    \n",
    "    return (ConnProb.astype('float32'), BS.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating conditional probabilities and relevant Brier Score for specific temporal subsetting\n",
    "def temporal_conditioning_subset(subset_extremes, subset_type):\n",
    "\n",
    "    nw_crd = ActualClusters.loc[subset_extremes.time.values, subset_type] # temporal flag to replace coordinate values\n",
    "    \n",
    "    ConnProb = subset_extremes.assign_coords({'time': nw_crd.values}) # rename time based on temporal flag\n",
    "    \n",
    "    ConnProb = ConnProb.groupby('time').sum('time')/ConnProb.groupby('time').count() # get conditional prob\n",
    "    ConnProb = ConnProb.rename({'time': 'temporal'}) # rename coordinate\n",
    "    \n",
    "    weights_temp = nw_crd.value_counts() # weights based on occurrence of temporal subsets\n",
    "    weights_temp = weights_temp.reindex(ConnProb.temporal.values) # reorder to same order as the xarray \"ConnProb\"\n",
    "\n",
    "    BS = statistical_brier_score(ConnProb, weights_temp.values, dim_used='temporal') # statistical Brier Score\n",
    "    BS = BS.assign_coords({'Method': f'{subset_type}_Temp'}) # assign new coord with the temporal subset info\n",
    "    \n",
    "    return (ConnProb.astype('float32'), BS.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating conditional probabilities and relevant Brier Score for specific pattern-temporal subsetting\n",
    "def clusters_EPEs_conditioning(subset_extremes, subset_type):\n",
    "\n",
    "    nw_crd = subset_extremes.time.values # cluster ID\n",
    "    # generate new coordinates values based on the cluster ID and the temporal flag of interest for each instance\n",
    "    nw_crd = ActualClusters.loc[nw_crd, 'Label'].astype(str) + '-' + ActualClusters.loc[nw_crd, subset_type]\n",
    "    \n",
    "    DataUsed = subset_extremes.assign_coords({'time': nw_crd.values}) # new coordinate values\n",
    "    \n",
    "    DataUsed = DataUsed.groupby('time').sum('time')/DataUsed.groupby('time').count() # conditional prob.\n",
    "    DataUsed = DataUsed.rename({'time': 'cluster'})\n",
    "    \n",
    "    weights_cluster = nw_crd.value_counts().reindex(DataUsed.cluster.values) # weights based on occurrence\n",
    "    \n",
    "    temporal_splitting = ActualClusters[subset_type].unique() # get all available subsets of temporal flag\n",
    "    ConnProb = []\n",
    "    for i_temp in temporal_splitting:\n",
    "        Subset_used = [i for i in DataUsed.cluster.values if i_temp == i.split('-')[1]] # get all available clusters\n",
    "        Subset_used = DataUsed.sel(cluster=Subset_used) # subset only the available cluster at the temporal subset\n",
    "        Subset_used = Subset_used.assign_coords({'cluster': [int(i[0]) for i in Subset_used.cluster.values]}) # rename\n",
    "        ConnProb.append(Subset_used) # append to final list\n",
    "\n",
    "    ConnProb = xr.concat(ConnProb, dim=pd.Index(temporal_splitting, name='temporal'))\n",
    "    \n",
    "    BS = statistical_brier_score(DataUsed, weights_cluster.values, dim_used='cluster') # get Brier Score\n",
    "    BS = BS.assign_coords({'Method': f'{subset_type}_Patt'}) # assign new coord with the temporal subsetting info\n",
    "       \n",
    "    return (ConnProb.astype('float32'), BS.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connections_stats(input_data):\n",
    "    \n",
    "    subset_dates, longs_used = input_data\n",
    "    \n",
    "    Exceed_dataset = exceed_boolean(Precipitation.sel(time=subset_dates, longitude=longs_used))\n",
    "    \n",
    "    Conn_Clusters, BS_All = [], []\n",
    "    for i_temp in ['All', 'HalfYear']:\n",
    "        i_conn, i_BS = clusters_EPEs_conditioning(subset_extremes=Exceed_dataset, subset_type=i_temp)\n",
    "        Conn_Clusters.append(i_conn)\n",
    "        BS_All.append(i_BS)\n",
    "        \n",
    "    Conn_Clusters = xr.concat(Conn_Clusters, dim='temporal')\n",
    "    Conn_Temp, BS_Temp = DayMonth_EPEs_conditioning(subset_extremes=Exceed_dataset)\n",
    "    \n",
    "    Conn_Temp2, BS_Temp2 = temporal_conditioning_subset(Exceed_dataset, 'Season')\n",
    "    Conn_Temp = xr.concat([Conn_Temp, Conn_Temp2], dim='temporal')\n",
    "    \n",
    "    BS_All.append(BS_Temp)\n",
    "    BS_All.append(BS_Temp2)\n",
    "    \n",
    "    BS_All = xr.concat(BS_All, dim='Method')\n",
    "    BS_All.name = 'BS'\n",
    "    \n",
    "    BSS_All = 1 - BS_All/BS_All.sel(Method=['DayMonth_Temp', 'Season_Temp']).min('Method')\n",
    "    BSS_All.name = 'BSS'\n",
    "    \n",
    "    BS_All = xr.merge([BS_All, BSS_All])\n",
    "    \n",
    "    return {'Conn_Clusters': Conn_Clusters, 'Conn_Temp': Conn_Temp, 'BS_All': BS_All}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = connections_stats([Precipitation.time.values, Lons_all])\n",
    "Connections_Patterns = All_data['Conn_Clusters']\n",
    "Connections_Patterns.to_netcdf(output_dir+'Connections_Patterns.nc')\n",
    "Connections_Temporal = All_data['Conn_Temp']\n",
    "Connections_Temporal.to_netcdf(output_dir+'Connections_Temporal.nc')\n",
    "BS_All = All_data['BS_All']\n",
    "BS_All.to_netcdf(output_dir+'BS_ERA5_All.nc')\n",
    "\n",
    "del(All_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for generating a 2-d DF with the forecasted cluster allocation for each date (only for specific lead time)\n",
    "def forecast_subset(lead_time):\n",
    "    \n",
    "    Subset_Frcst = AllocatedClusters.query('step==@lead_time and number!=-1') # remove ens. mean data cause high bias\n",
    "    Subset_Frcst = Subset_Frcst.pivot_table(index='valid_time', columns='number', values='Cluster')\n",
    "    Subset_Frcst.index = pd.to_datetime(Subset_Frcst.index)\n",
    "        \n",
    "    return Subset_Frcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frcst_precip_init_date(init_date_used):\n",
    "    \n",
    "    ' Get the reforecast data for the selected initialization date '\n",
    "    ' There is no need to derive ens mean for precipitation data, cause the mean is very biased and wrong! '\n",
    "    \n",
    "    # get the data of the control member (cf)\n",
    "    file_name = input_dir+'Data/Precipitation/cf/Precipitation_cf_'+init_date_used+'.grb'\n",
    "    control_forecast = xr.open_dataarray(file_name, engine='cfgrib')\n",
    "    control_forecast = control_forecast.astype('float32') # float32 for memory efficiency\n",
    "    control_forecast = control_forecast.sel(longitude=slice(Area_used[1], Area_used[3]), \n",
    "                                            latitude=slice(Area_used[0], Area_used[2]))\n",
    "    control_forecast = control_forecast.assign_coords({'number': 0})\n",
    "    \n",
    "    # get the data of the ensemble members (pf)\n",
    "    file_name = input_dir+'Data/Precipitation/pf/Precipitation_pf_'+init_date_used+'.grb'\n",
    "    ensemble_forecast = xr.open_dataarray(file_name, engine='cfgrib')\n",
    "    ensemble_forecast = ensemble_forecast.astype('float32') # float32 for memory efficiency\n",
    "    ensemble_forecast = ensemble_forecast.sel(longitude=slice(Area_used[1], Area_used[3]), \n",
    "                                              latitude=slice(Area_used[0], Area_used[2]))\n",
    "    \n",
    "    final = xr.concat([control_forecast, ensemble_forecast], dim='number') # combine cf and pf data\n",
    "    \n",
    "    # Precipitation is a cumulative variable, so for daily values we need differences of next with day of interest\n",
    "    final = xr.concat([final.isel(step=0), final.diff('step')], dim='step')\n",
    "    final = final.assign_coords({'step': final.step.values-np.timedelta64(1, 'D')}) # step is the min possible lag\n",
    "    \n",
    "    # slicing the data due to memory limitations: i_lead is defined later on\n",
    "    final = final.sel(step=np.timedelta64(i_lead, 'D'))\n",
    "\n",
    "    return final.reset_coords(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frcst_precip_all(dates):\n",
    "    \n",
    "    pool = multiprocessing.Pool() # object for multiprocessing\n",
    "    Data_Pr = list(tqdm.tqdm(pool.imap(frcst_precip_init_date, dates), total=len(dates), position=0, leave=True))\n",
    "    pool.close()\n",
    "    \n",
    "    Data_Pr = xr.concat(Data_Pr, dim='time')\n",
    "    \n",
    "    return Data_Pr.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting subset of forecasted precipitation data and generating boolean for exceedance of extremes\n",
    "def exceed_boolean_frcst(data):\n",
    "    \n",
    "    Data = data.stack(all_data=['time', 'number'])    \n",
    "    \n",
    "    Quant = Data.quantile(np.array(P_used)/100, interpolation='linear', dim='all_data', keep_attrs=True) # thresholds\n",
    "    Quant = Quant.rename({'quantile': 'percentile'}) # rename coordinate\n",
    "    Quant = Quant.assign_coords({'percentile': P_used}) # assign the dim values based on percentiles\n",
    "\n",
    "    # boolean xarray for identifying if an event is over the threshold\n",
    "    Exceed_xr = [data>Quant.sel(percentile=i_p) for i_p in P_used] # boolean of exceedance per percentile\n",
    "    Exceed_xr = xr.concat(Exceed_xr, dim=pd.Index(P_used, name='percentile')) # concatenate data for all percentiles\n",
    "    \n",
    "    return Exceed_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating the cond. probs. for all forecasted dates\n",
    "def subset_cond_prob_clustering(data_sub, temp_subset, lons_used):\n",
    "    \n",
    "    temp_flag = temp_flagging(data_sub.index, temp_subset) # get temporal flags of the forecasted dates\n",
    "    \n",
    "    EPEsProb = [Connections_Patterns.sel(cluster=i_cluster, temporal=i_temp, longitude=lons_used).mean('cluster') \n",
    "                for i_cluster, i_temp in list(zip(data_sub.values, temp_flag))]\n",
    "\n",
    "    EPEsProb = xr.concat(EPEsProb, dim=pd.Index(data_sub.index, name='time')) # concat\n",
    "    EPEsProb = EPEsProb.transpose('percentile', ...) # transpose\n",
    "    \n",
    "    return EPEsProb.astype('float32')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting cond. prob. based on temporal subsetting only\n",
    "def subset_cond_prob_temporal(data_sub, temp_subset, lons_used):\n",
    "    \n",
    "    temp_flag = temp_flagging(data_sub.index, temp_subset)\n",
    "    \n",
    "    CondProb = [Connections_Temporal.sel(temporal=i_temp, longitude=lons_used) for i_temp in temp_flag]\n",
    "    CondProb = xr.concat(CondProb, dim=pd.Index(data_sub.index, name='time')) # concat\n",
    "    CondProb = CondProb.transpose('percentile', ...) # transpose\n",
    "    \n",
    "    return CondProb.reset_coords(drop=True).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating the brier score from actual data and not based on statistics as in the other function\n",
    "def BS_calculation(forecasts, observations):\n",
    "    \n",
    "    BS_value = (forecasts - observations)**2\n",
    "    BS_value = BS_value.sum('time')/len(BS_value.time)\n",
    "    \n",
    "    return BS_value#.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dates of Cycle 46r1: 11 June 2019 - 30 June 2020\n",
    "start_date = '20190611'\n",
    "end_date = '20200630'\n",
    "\n",
    "initialization_dates = pd.date_range(start_date, end_date)\n",
    "\n",
    "# keep Mondays (0) and Thursdays (3)\n",
    "initialization_dates = initialization_dates[(initialization_dates.weekday == 0) | (initialization_dates.weekday == 3)]\n",
    "initialization_dates = initialization_dates.strftime('%Y%m%d')\n",
    "\n",
    "del(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for performing Brier Score analysis\n",
    "def brier_score_analysis(input_data):\n",
    "    \n",
    "    lead_time, dates_subset, lons_used = input_data\n",
    "    \n",
    "    Subset_Frcst = forecast_subset(lead_time) # generate dataframe with forecasted cluster allocations\n",
    "    if dates_subset is not None:\n",
    "        Subset_Frcst = Subset_Frcst.loc[dates_subset]\n",
    "    else:\n",
    "        # drop the dates that correspond to the last 5 initiatilization dates, so that the dataset has a \n",
    "        # climatologically correct number of Winter/Spring/Summer/Autumn dates, since the EPEs are based on such data\n",
    "        DropDates = pd.to_datetime(initialization_dates[-5:])+np.timedelta64(lead_time, 'D')\n",
    "        DropDates = [DropDates-pd.DateOffset(years=i) for i in range(1,21)]\n",
    "        DropDates = [j for i in DropDates for j in i]\n",
    "        Subset_Frcst = Subset_Frcst[~Subset_Frcst.index.isin(DropDates)]\n",
    " \n",
    "    Subset_Exceed = exceed_boolean(Precipitation.sel(time=Subset_Frcst.index, longitude=lons_used))\n",
    "    \n",
    "    ActualClusters_Subset = ActualClusters.loc[Subset_Frcst.index, ['Label']]\n",
    "    \n",
    "    BS_Forecasts = [] # list for appending all Brier Score data (to be converted in DataArray)\n",
    "    \n",
    "    # calculate direct EPEs BS based on the forecasted precipitation fields\n",
    "    Frst_Extremes = exceed_boolean_frcst(Precip_Frcst.sel(time=Subset_Frcst.index, longitude=lons_used))\n",
    "    BS_Direct = BS_calculation(Frst_Extremes.mean('number'), Subset_Exceed)\n",
    "    BS_Forecasts.append(BS_Direct.assign_coords({'Method': 'EPEs_Direct'}))\n",
    "        \n",
    "    # calculate indirect EPEs BS for forecasted clusters given the CondProb of EPEs based on cluster and halfyear\n",
    "    for i_type in ['HalfYear']: # ['All', 'HalfYear', 'Season']: no need to perform other temporal subsets\n",
    "        Cond_Prob_Frcst = subset_cond_prob_clustering(Subset_Frcst, i_type, lons_used)\n",
    "        Cond_Prob_Frcst = BS_calculation(Cond_Prob_Frcst, Subset_Exceed)\n",
    "        BS_Forecasts.append(Cond_Prob_Frcst.assign_coords({'Method': f'{i_type}_Patt'}))\n",
    "        \n",
    "        Cond_Prob_Frcst_Perfect = subset_cond_prob_clustering(ActualClusters_Subset, i_type, lons_used)\n",
    "        Cond_Prob_Frcst_Perfect = BS_calculation(Cond_Prob_Frcst_Perfect, Subset_Exceed)\n",
    "        BS_Forecasts.append(Cond_Prob_Frcst_Perfect.assign_coords({'Method': f'{i_type}_Patt_Perfect'}))\n",
    "        del(Cond_Prob_Frcst)\n",
    "    \n",
    "    # calculate precipitation BS for temporal climatological connections (reference scores)\n",
    "    CondProb_Clim = subset_cond_prob_temporal(Subset_Frcst, 'DayMonth', lons_used)\n",
    "    CondProb_Clim = BS_calculation(CondProb_Clim, Subset_Exceed)\n",
    "    BS_Forecasts.append( CondProb_Clim.assign_coords({'Method': 'DayMonth_Temp'}) )\n",
    "    \n",
    "    CondProb_Clim = subset_cond_prob_temporal(Subset_Frcst, 'Season', lons_used)\n",
    "    CondProb_Clim = BS_calculation(CondProb_Clim, Subset_Exceed)\n",
    "    BS_Forecasts.append( CondProb_Clim.assign_coords({'Method': 'Season_Temp'}) )\n",
    "    \n",
    "    CondProb_Clim = Subset_Exceed.sum('time')/len(Subset_Exceed.time)\n",
    "    CondProb_Clim = statistical_brier_score(CondProb_Clim, [1], dim_used='subset')\n",
    "    BS_Forecasts.append( CondProb_Clim.assign_coords({'Method': 'All_Temp'}) )\n",
    "    del(CondProb_Clim)   \n",
    "    \n",
    "    BS_Forecasts = xr.concat(BS_Forecasts, dim='Method')\n",
    "    BS_Forecasts.name = 'BS'\n",
    "    \n",
    "    BS_Ref_min = BS_Forecasts.sel(Method=['DayMonth_Temp', 'Season_Temp', 'All_Temp']).min('Method')\n",
    "    BSS_Forecasts = 1 - BS_Forecasts/BS_Ref_min\n",
    "    BSS_Forecasts.name = 'BSS'\n",
    "    \n",
    "    BS_Forecasts = xr.merge([BS_Forecasts, BSS_Forecasts]).to_array().rename({'variable': 'Var'})\n",
    "    \n",
    "    return BS_Forecasts.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data due to memory limitations\n",
    "Lons_subsets = np.array_split(Lons_all, 2)\n",
    "\n",
    "# get the index values of the 5th, 95th and median number, when data are ordered\n",
    "l_m = int(bootstraps*5/100)\n",
    "l_M = int(bootstraps*95/100)-1\n",
    "Md = int(bootstraps/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for performing the full Brier Score analysis for a specific lead time (check all combinations of subsets)\n",
    "def long_subset_bs_statistics(lons_used, lead_time=0):\n",
    "    \n",
    "    BBS_dates = []\n",
    "    for i_ln, i_season in zip([247*3, 252*3, 252*3, 249*3], ['Winter', 'Spring', 'Summer', 'Autumn']):\n",
    "        AllDates = Days_used[temp_flagging(Days_used, 'Season')==i_season]\n",
    "        np.random.seed(10)\n",
    "        BBS_dates_i = np.random.choice(AllDates, i_ln*bootstraps) # generate all bootstrapped values\n",
    "        BBS_dates_i = np.array_split(BBS_dates_i, bootstraps) # split into the number of subsets (samples)\n",
    "        BBS_dates.append(BBS_dates_i)\n",
    "\n",
    "    BBS_dates = np.concatenate(BBS_dates, axis=1)\n",
    "    BBS_dates = list(BBS_dates)+[None] # add also the final bootstrap which is concerning the actual data\n",
    "    del(i_ln, i_season, AllDates, BBS_dates_i) \n",
    "\n",
    "    # generate bootstrapped statistics\n",
    "    pool = multiprocessing.Pool() # object for multiprocessing for bootstrapping\n",
    "    BBS_data = list(product([lead_time], BBS_dates, [lons_used]))\n",
    "    BBS_data = list(tqdm.tqdm(pool.imap(brier_score_analysis, BBS_data), total=len(BBS_data), position=0, leave=True))\n",
    "    pool.close()\n",
    "\n",
    "    BBS_data = xr.concat(BBS_data, dim='bootstrapping_frcst') # concatenate bootstrapping samples\n",
    "    \n",
    "    # get the 5th, and 95th value for BS without considering the \"Actual\" subset\n",
    "    BBS_data_ordered_values = np.sort(BBS_data.isel(bootstrapping_frcst=range(bootstraps)), axis=0) # don't use Actual\n",
    "    P5 = BBS_data[0]*0 + BBS_data_ordered_values[l_m]\n",
    "    P95 = BBS_data[0]*0 + BBS_data_ordered_values[l_M]\n",
    "    \n",
    "    # get the P50 after using \"Actual\" subset as well\n",
    "    BBS_data_ordered_values = np.sort(BBS_data, axis=0)    \n",
    "    P50 = BBS_data[0]*0 + BBS_data_ordered_values[Md]\n",
    "    \n",
    "    # combine all data to the final xarrays\n",
    "    dim_name = pd.Index(['P5', 'P50', 'Actual', 'P95'], name='bootstraps_frcst')\n",
    "    Final_BS = xr.concat([P5, P50, BBS_data.isel(bootstrapping_frcst=-1).reset_coords(drop=True), P95], dim=dim_name)\n",
    "    \n",
    "    # get percentage of methods outperforming Reference (Sign_Ref), and direct prediction of EPEs (Sign_Direct)\n",
    "    Sign_Ref = ( BBS_data.sel(Var='BSS') > 0 ).reset_coords(drop=True)\n",
    "    Sign_Dir = ( BBS_data.sel(Var='BS') < BBS_data.sel(Var='BS', Method='EPEs_Direct') ).reset_coords(drop=True)\n",
    "    Sign_Combo = Sign_Dir & Sign_Ref\n",
    "    Sign_Ref = Sign_Ref.sum('bootstrapping_frcst')\n",
    "    Sign_Ref.name = 'Sign_Ref'\n",
    "    Sign_Dir = Sign_Dir.sum('bootstrapping_frcst')\n",
    "    Sign_Dir.name = 'Sign_Direct'\n",
    "    Sign_Combo = Sign_Combo.sum('bootstrapping_frcst')\n",
    "    Sign_Combo.name = 'Sign_Combo'\n",
    "    Sign_Final = xr.merge([Sign_Ref, Sign_Dir, Sign_Combo])\n",
    "    Sign_Final = Sign_Final/(bootstraps+1)\n",
    "    Final_BS = Final_BS.to_dataset('Var')\n",
    "    Final_BS = xr.merge([Final_BS, Sign_Final])\n",
    "    \n",
    "    return Final_BS.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_bs_statistics(lead_time=0):\n",
    "    Final = []\n",
    "    for i_lon in Lons_subsets:\n",
    "        Final.append(long_subset_bs_statistics(i_lon, lead_time))\n",
    "\n",
    "    Final = xr.concat(Final, dim='longitude')\n",
    "    Final = Final.assign_coords({'leaddays':lead_time})\n",
    "    \n",
    "    return Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EconomicValue(input_data):\n",
    "    \n",
    "    frcst_data, obs_data, p_t = input_data\n",
    "    \n",
    "    HI = ((frcst_data>=p_t).where(obs_data==1)).sum('time')\n",
    "    FA = ((frcst_data>=p_t).where(obs_data==0)).sum('time')\n",
    "    MI = ((frcst_data<p_t).where(obs_data==1)).sum('time')\n",
    "    CR = ((frcst_data<p_t).where(obs_data==0)).sum('time')\n",
    "    \n",
    "    HitRate = HI/(HI+MI)\n",
    "    FalseAlarmRate = FA/(FA+CR)\n",
    "\n",
    "    EV = FalseAlarmRate*CostRatio*(1-Extr_Occur)-HitRate*Extr_Occur*(1-CostRatio)+Extr_Occur\n",
    "    EV = (EV_clim - EV)/(EV_clim-Extr_Occur*CostRatio)\n",
    "    EV = EV.assign_coords({'p_thr': p_t})\n",
    "      \n",
    "    return EV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EcVal_analysis(lead_time=0):\n",
    "    \n",
    "    \n",
    "    Subset_Frcst = forecast_subset(lead_time) # generate dataframe with forecasted pattern allocations\n",
    "    # drop all reforecasts of last 5 initiation dates, so that seasonal frequencies are correct\n",
    "    DropDates = pd.to_datetime(initialization_dates[-5:])+np.timedelta64(lead_time, 'D')\n",
    "    DropDates = [DropDates-pd.DateOffset(years=i) for i in range(1,21)]\n",
    "    DropDates = [j for i in DropDates for j in i]\n",
    "    Subset_Frcst = Subset_Frcst[~Subset_Frcst.index.isin(DropDates)]\n",
    "    \n",
    "    # generate boolean with extremes for actual data, direct precip. forecast and indirect based on patterns\n",
    "    Subset_Exceed = exceed_boolean(Precipitation.sel(time=Subset_Frcst.index))\n",
    "    Frst_Extremes = exceed_boolean_frcst(Precip_Frcst.sel(time=Subset_Frcst.index)).mean('number')\n",
    "    Cond_Prob_Frcst = subset_cond_prob_clustering(Subset_Frcst, 'HalfYear', Precipitation.longitude.values)\n",
    "    CondProb_DayMonth = subset_cond_prob_temporal(Subset_Frcst, 'DayMonth', Precipitation.longitude.values)\n",
    "    CondProb_Season = subset_cond_prob_temporal(Subset_Frcst, 'Season', Precipitation.longitude.values)\n",
    "    \n",
    "    # generate auxiliary data needed for calculating the Economic Value for different ratios of Cost/Gain measures\n",
    "    global Extr_Occur, CostRatio, EV_clim\n",
    "    Extr_Occur = Subset_Exceed.mean('time')\n",
    "    CostRatio = xr.DataArray(np.linspace(0, 1, 100), dims=['cost_ratio'], \n",
    "                             coords={'cost_ratio': np.linspace(0, 1, 100)})\n",
    "    CostRatio = Extr_Occur*0+CostRatio # generate the remaining dimensions of the cost ratio\n",
    "    Extr_Occur = CostRatio*0+Extr_Occur # generate the \"cost_ratio\" dimension of the extremes percentage occurrences\n",
    "    EV_clim = xr.concat([CostRatio, Extr_Occur], dim='clim').min('clim') # climatological gain for each coordinate\n",
    "    \n",
    "    # calculate Economic Value for direct, indirect and climatological forecasting and combine the data\n",
    "    pool = multiprocessing.Pool()\n",
    "    EV_dir = list(product([Frst_Extremes], [Subset_Exceed], np.arange(12)/11))\n",
    "    EV_dir = list(tqdm.tqdm(pool.imap(EconomicValue, EV_dir), total=len(EV_dir), position=0, leave=True))\n",
    "    EV_dir = xr.concat(EV_dir, dim='p_thr').max('p_thr')\n",
    "    pool.close()\n",
    "    \n",
    "    thresholds_used = list(np.linspace(0,np.ceil(Cond_Prob_Frcst.max().values*100)/100,100))+[1]\n",
    "    pool = multiprocessing.Pool()\n",
    "    EV_indir = list(product([Cond_Prob_Frcst], [Subset_Exceed], thresholds_used))\n",
    "    EV_indir = list(tqdm.tqdm(pool.imap(EconomicValue, EV_indir), total=len(EV_indir), position=0, leave=True))\n",
    "    EV_indir = xr.concat(EV_indir, dim='p_thr').max('p_thr')\n",
    "    pool.close()\n",
    "    \n",
    "    thresholds_used = list(np.linspace(0,np.ceil(CondProb_DayMonth.max().values*100)/100,100))+[1]\n",
    "    pool = multiprocessing.Pool()\n",
    "    EV_DM = list(product([CondProb_DayMonth], [Subset_Exceed], thresholds_used))\n",
    "    EV_DM = list(tqdm.tqdm(pool.imap(EconomicValue, EV_DM), total=len(EV_DM), position=0, leave=True))\n",
    "    EV_DM = xr.concat(EV_DM, dim='p_thr').max('p_thr')\n",
    "    pool.close()\n",
    "    \n",
    "    thresholds_used = list(np.linspace(0,np.ceil(CondProb_Season.max().values*100)/100,100))+[1]\n",
    "    pool = multiprocessing.Pool()\n",
    "    EV_Sea = list(product([CondProb_Season], [Subset_Exceed], thresholds_used))\n",
    "    EV_Sea = list(tqdm.tqdm(pool.imap(EconomicValue, EV_Sea), total=len(EV_Sea), position=0, leave=True))\n",
    "    EV_Sea = xr.concat(EV_Sea, dim='p_thr').max('p_thr')\n",
    "    pool.close()\n",
    "    \n",
    "    EV_final = xr.concat([EV_dir, EV_indir, EV_DM, EV_Sea], \n",
    "                         dim=pd.Index(['Direct', 'Indirect', 'Clim_DayMonth', 'Clim_Seasonal'], name='Method'))\n",
    "    EV_final = EV_final.assign_coords({'leaddays': lead_time})\n",
    "    del Extr_Occur, CostRatio, EV_clim\n",
    "    \n",
    "    return EV_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeadTimes = AllocatedClusters.step.unique()\n",
    "BS_EPEs_All = []\n",
    "EV_EPEs_All = []\n",
    "for i_lead in tqdm.tqdm(LeadTimes):\n",
    "    # read the forecasted precipitation data\n",
    "    Precip_Frcst = frcst_precip_all(initialization_dates) # get the precip forecasts for the lead time of interest\n",
    "    Days_used = Precip_Frcst.time.values+np.timedelta64(i_lead, 'D') # get the valid dates for the forecasts\n",
    "    Precip_Frcst = Precip_Frcst.assign_coords({'time': Days_used}) # change to the valid time\n",
    "    Days_used = pd.to_datetime(Days_used)\n",
    "    BS_subset = final_bs_statistics(lead_time=i_lead)\n",
    "    BS_EPEs_All.append(BS_subset)\n",
    "    EV_subset = EcVal_analysis(lead_time=i_lead)\n",
    "    EV_EPEs_All.append(EV_subset)\n",
    "    \n",
    "BS_EPEs_All = xr.concat(BS_EPEs_All, dim='leaddays')\n",
    "BS_EPEs_All.to_netcdf(output_dir+'BS_leaddays.nc')\n",
    "EV_EPEs_All = xr.concat(EV_EPEs_All, dim='leaddays')\n",
    "EV_EPEs_All.to_netcdf(output_dir+'EV_leaddays.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
