{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) Copyright 1996- ECMWF.\n",
    "This software is licensed under the terms of the Apache Licence Version 2.0\n",
    "which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.\n",
    "In applying this licence, ECMWF does not waive the privileges and immunities\n",
    "granted to it by virtue of its status as an intergovernmental organisation\n",
    "nor does it submit to any jurisdiction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Environment](#environment)\n",
    "    1. [Imports](#imports)\n",
    "    2. [User-defined inputs](#inputs)\n",
    "3. [Data Analysis](#analysis)\n",
    "    1. [Moisture/water-related variables](#moisture)\n",
    "        1. [Calculate Anomalies](#anomalies)\n",
    "        2. [EOF Analysis](#eof)\n",
    "    2. [Observational data - Overlap](#eobs)\n",
    "        1. [Preprocessing](#preprocessing-overlap)\n",
    "        2. [Temporal Overlap](#overlap)\n",
    "    3. [Observational Data - Connecting EPEs to Large-Scale Atmoshperic Flow Patterns](#extremes-to-patterns)\n",
    "        1. [Preprocessing](#preprocessing-connections) \n",
    "        2. [Auxiliary Functions](#auxiliary)\n",
    "        3. [Quantifying the Connections](#quantifying-connections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional data analysis for the work presented in the paper: <a name=\"introduction\"></a>\n",
    "### [Extreme precipitation events in the Mediterranean: Spatiotemporal characteristics and connection to large-scale atmospheric flow patterns](https://rmets.onlinelibrary.wiley.com/doi/10.1002/joc.6985)\n",
    "\n",
    "---\n",
    "Author: Nikolaos Mastrantonas\\\n",
    "Email: nikolaos.mastrantonas@ecmwf.int; nikolaos.mastrantonas@doktorand.tu-freiberg.de\n",
    "\n",
    "---\n",
    "The additional analysis is based on the reviewers's comments about:\n",
    "1. Why no moisture/water-related variables were used.\n",
    "2. What would the results regarding connection of extremes to large-scale patterns be, if observational data were used for precipitation instead of ERA5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment<a name=\"environment\"></a>\n",
    "Load the required packages and get the user-defined inputs.\n",
    "\n",
    "The analysis was done in a Linux machine with 8 CPUs and 32 GB RAM. The total duration was about 1 hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages (full package or specific functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing # parallel processing\n",
    "import tqdm # timing\n",
    "import sys\n",
    "from datetime import datetime # timing\n",
    "from pathlib import Path # creation of dictionaries\n",
    "import warnings # for suppressing RuntimeWarning\n",
    "\n",
    "# basic libraries for data analysis\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import xarray as xr\n",
    "\n",
    "# specialized libraries\n",
    "import metview as mv # the metview package is needed for calculating the equivalent potential temperature \n",
    "from eofs.xarray import Eof # EOF analysis\n",
    "from scipy.stats import binom # binomial distribution for significance testing of extremes and large-scale patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined inputs <a name=\"inputs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dictionary with the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_loc = '' # the main folder where the input data are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inputs related to EOF analysis of the Atmospheric Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_used = ['ThetaE850', 'WVF', 'SLP', 'T850', 'Z500', 'Q850'] # variables used for the EOF analysis\n",
    "\n",
    "Area_used =  [50, -11, 26, 41] # define extend of area of interest (The Med. domain) (One of the regions at Script2)\n",
    "\n",
    "Var_ex = 90 # define the minimum total variance that the subset of kept EOFs should explain (same as Script2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inputs related to comparison of ERA5 and EOBS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EOBS data were downloaded from https://surfobs.climate.copernicus.eu/dataaccess/access_eobs.php#datafiles at the 0.10 degrees resolution (file name: \"rr_ens_mean_0.1deg_reg_v21.0e\").\n",
    "\n",
    "The dataset was initally processed with cdo tooldbox for having the exact same grid cell coordinates and grid resolution with the used ERA5 precipitation data. The 0.25 degrees resolution product of the EOBS data was not selected, because the grid is shifted compared to ERA5. Thus, since remapping is needed, using finer resolution gives better results.\n",
    "\n",
    "The cdo preprocessing was done with a small txt file named \"mygrid.txt\" with the new grid information in the following lines:\\\n",
    "gridtype = lonlat\\\n",
    "xsize = 185\\\n",
    "ysize = 73\\\n",
    "xfirst = -8\\\n",
    "xinc = 0.25\\\n",
    "yfirst = 29\\\n",
    "yinc = 0.25\n",
    "\n",
    "The linux command within cdo for remaping the data is: **cdo remapcon,mygrid.txt rr_ens_mean_0.1deg_reg_v21.0e EOBS_Med.nc**. The above command is executable once we are at the folder of the input data, otherwise the relative location of the files should be included as well.\n",
    "\n",
    "**The *Remapcon* was selected, because for precipitation data the first-order convervative regridding method is recommended.**\n",
    "\n",
    "More infomation available at https://code.mpimet.mpg.de/boards/2/topics/296"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOBS_file_name = 'Data/EOBS_Med.nc' # the name of the file of the EOBS precipitation data\n",
    "ERA5_file_name = 'Data/D1_Total_Precipitation.grb' # the name of the grb file of the precipitation data\n",
    "\n",
    "P_used = [95, 97, 99] # define the percentile(s) of interest (same as Script2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the cluster daily allocation data for analysing the connections based on EOBS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combination_used = 'Med_SLP~Z500' # should be one of the available sets saved from Script2\n",
    "Clusters_used = 9 # should be one of the available sets saved from Script2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis <a name=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitializationTime = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moisture/Water-related Variables<a name=\"moisture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Anomalies<a name=\"anomalies\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalies(variable):\n",
    "    \n",
    "    # read actual daily values as xarray dataarray object\n",
    "    if variable in ['SLP', 'T850', 'Z500', 'Q850']: # directly read data as xarray\n",
    "        file_path = dir_loc + 'Data/D1_Mean_'+variable+'.grb'\n",
    "        Daily = xr.open_dataarray(file_path, engine='cfgrib') # read data\n",
    "    elif variable=='ThetaE850': # calcualte eq. pot. temperated with metview package and then convert to xarray\n",
    "        Q850 = mv.read(dir_loc+ 'Data/D1_Mean_Q850.grb') # specific humidity\n",
    "        T850 = mv.read(dir_loc+ 'Data/D1_Mean_T850.grb') # temperature\n",
    "        ThetaE850 = mv.eqpott_p(temperature=T850, humidity=Q850) # use mv package to calculate equiv. pot. temp.\n",
    "        Daily = ThetaE850.to_dataset() # convert from metview Fieldset to xarray Dataset\n",
    "        Daily = Daily.to_array()[0] # convet to DataArray\n",
    "    elif variable=='WVF': # read the east and north componenet of WVF and calculate the total magnitude per grid cell\n",
    "        WVFeast = xr.open_dataarray(dir_loc+ 'Data/D1_Mean_WVFeast.grb', engine='cfgrib')\n",
    "        WVFnorth = xr.open_dataarray(dir_loc+ 'Data/D1_Mean_WVFnorth.grb', engine='cfgrib')\n",
    "        Daily = np.sqrt(WVFeast**2 + WVFnorth**2) # total magnitude of the WVF\n",
    "        \n",
    "    # subset area of interest \n",
    "    Daily = Daily.sel(latitude=slice(Area_used[0], Area_used[2]), longitude=slice(Area_used[1], Area_used[3]))    \n",
    "    \n",
    "    actual_days = Daily.time.values # get actual timesteps\n",
    "    dates_grouped = pd.to_datetime(Daily.time.values).strftime('%m%d') # get Month-Day of each timestep\n",
    "    \n",
    "    # 5-day smoothed climatology. Rolling can be applied directly because the daily data refer to consequtive days. If\n",
    "    # days are not consecutive, firstly the xr.resample should be applied, so that missing days are generated with NaN\n",
    "    Smoothed = Daily.rolling(time=5, center=True, min_periods=1).mean() # 5-day smoothing\n",
    "    \n",
    "    Daily = Daily.assign_coords({'time': dates_grouped}) # change the time to Month-Day\n",
    "    Smoothed = Smoothed.assign_coords({'time': dates_grouped}) # change the time to Month-Day\n",
    "    \n",
    "    Climatology = Smoothed.groupby('time').mean() # climatology of the smoothed data\n",
    "    \n",
    "    Anomalies = Daily.groupby('time') - Climatology\n",
    "    Anomalies = Anomalies.assign_coords({'time': actual_days}) # change back to the original timestep information\n",
    "    \n",
    "    return Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [04:47<00:00, 47.92s/it] \n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "Anomalies = list(tqdm.tqdm(pool.imap(anomalies, variables_used), total=len(variables_used), position=0, leave=True))\n",
    "pool.close()\n",
    "\n",
    "Anomalies = {variables_used[i_c]: i_anom for i_c, i_anom in enumerate(Anomalies)}\n",
    "\n",
    "del(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF Analysis<a name=\"eof\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eof_analysis(variable):\n",
    "    \n",
    "    dataset_used = Anomalies[variable] # variable to be used for the analysis\n",
    "    \n",
    "    coslats = np.cos(np.deg2rad(dataset_used.latitude.values)).clip(0, 1) # coslat for weights on EOF\n",
    "    wgts = np.sqrt(coslats)[..., np.newaxis] # calculation of weights\n",
    "    solver = Eof(dataset_used, weights=wgts) # EOF analysis of the subset\n",
    "    \n",
    "    N_eofs = int(np.searchsorted(np.cumsum(solver.varianceFraction().values), Var_ex/100)) # number of EOFs needed\n",
    "    N_eofs += 1 # add 1 since python does not include the last index of a range\n",
    "    \n",
    "    EOFS = solver.eofs(neofs=N_eofs)\n",
    "    VARS = solver.varianceFraction(neigs=N_eofs).values*100\n",
    "    \n",
    "    return {'EOFS': EOFS, 'VARS': VARS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:21<00:00,  3.59s/it]\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "EOFS = list(tqdm.tqdm(pool.imap(eof_analysis, variables_used), total=len(variables_used), position=0, leave=True))\n",
    "pool.close()\n",
    "\n",
    "EOFS = {variables_used[i_c]: i_eof for i_c, i_eof in enumerate(EOFS)}\n",
    "del(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 EOFs needed for explaining at least 90% of the total variance for the ThetaE850 daily anomalies.\n",
      "49 EOFs needed for explaining at least 90% of the total variance for the WVF daily anomalies.\n",
      "6 EOFs needed for explaining at least 90% of the total variance for the SLP daily anomalies.\n",
      "12 EOFs needed for explaining at least 90% of the total variance for the T850 daily anomalies.\n",
      "7 EOFs needed for explaining at least 90% of the total variance for the Z500 daily anomalies.\n",
      "115 EOFs needed for explaining at least 90% of the total variance for the Q850 daily anomalies.\n"
     ]
    }
   ],
   "source": [
    "for i_var in variables_used:\n",
    "    print('{} EOFs needed for explaining at least {}% of the total variance for the {} daily anomalies.'.\\\n",
    "          format(len(EOFS[i_var]['VARS']), Var_ex, i_var))\n",
    "del(i_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of EOFs needed for the water/moisture-related variables (Q850, ThetaE850, WVF) is substantially higher compared to the other three variables (SLP, T850, Z500). This is because of the large spatial domain of the analysis. This result suggests that using the water/moisture-related variables at the K-means clustering for defining weather regimes would increase the level of complexity, without necessarily bringing significant improvements on the connection of extreme precipitation events to large-scale patterns. For smaller domains (e.g. country-wise, regional ones), the inclusion of such variables would be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observational Data - Overlap<a name=\"eobs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing<a name=\"preprocessing-overlap\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_used = sorted(list(np.array(P_used).flatten()))[::-1] # sort P_used & make list for consistency and avoiding errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read ERA5 and EOBS data and do some basic preprocessing for having both sets in same format\n",
    "ERA5 = xr.open_dataarray(dir_loc + ERA5_file_name, engine='cfgrib') # read data\n",
    "ERA5 = ERA5.drop(['valid_time', 'step', 'surface', 'number']) # drop not-used coordinates\n",
    "ERA5 = ERA5.assign_coords({'time': pd.to_datetime(ERA5.time.values).strftime('%Y%m%d')}) # change time to str\n",
    "\n",
    "EOBS = xr.open_dataarray(dir_loc + EOBS_file_name) # read data\n",
    "EOBS = EOBS.rename({'lat': 'latitude', 'lon': 'longitude'}) # rename for same name as ERA5\n",
    "EOBS = EOBS.reindex(latitude=EOBS.latitude[::-1]) # reverse order for same as ERA5\n",
    "EOBS = EOBS.assign_coords({'time': pd.to_datetime(EOBS.time.values).strftime('%Y%m%d')}) # change time to str\n",
    "EOBS = EOBS.sel(time=ERA5.time.values) # keep only the dates available in ERA5\n",
    "\n",
    "# calculate percentage of NaN per grid cell\n",
    "NANs_EOBS = np.isnan(EOBS).sum(dim='time')\n",
    "NANs_EOBS = NANs_EOBS/len(EOBS)*100\n",
    "NANs_EOBS = NANs_EOBS<5 # keep only locations that have less than 5% missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_extreme(dataset, quantile):\n",
    "    \n",
    "    ' Get the dates over user-defined percentile '\n",
    "    \n",
    "    with warnings.catch_warnings(): # if all are NaN then it gives Runtimewarning, which is now suppressed\n",
    "        warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "        Q_thres = dataset.quantile(quantile, interpolation='linear', dim='time', keep_attrs=True) # get the threshold\n",
    "    \n",
    "    dataset_df = dataset.values.flatten() # keep only the values as numpy array\n",
    "    dataset_df = pd.DataFrame(np.reshape(dataset_df, (len(dataset), -1)), index=dataset.time.values) # convert to DF\n",
    "\n",
    "    QuantExceed = dataset_df > Q_thres.values.flatten() # Boolean over /under-up to Q threshold\n",
    "    DaysExceed = QuantExceed.apply(lambda x: list(QuantExceed.index[np.where(x == 1)[0]]), axis=0) # exceedance days\n",
    "    \n",
    "    return DaysExceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_extend(actual_dates, days_offset):\n",
    "    \n",
    "    '''\n",
    "    Get a list with dates within a temporal window centered over \"actual_dates\" and extending \"days_offset\" before\n",
    "    and after the \"actual_dates\"\n",
    "    '''\n",
    "    \n",
    "    if type(actual_dates) == str: actual_dates = [actual_dates] # if only 1 value, then convert to list\n",
    "        \n",
    "    dates_dt = pd.to_datetime(actual_dates) # convert to datetime objects from string\n",
    "    all_dates = [pd.date_range(i_date - pd.DateOffset(days=days_offset), i_date + pd.DateOffset(days=days_offset) )\n",
    "                 for i_date in dates_dt]\n",
    "    all_dates = [j.strftime('%Y%m%d') for i in all_dates for j in i] # single list and convert to string\n",
    "    all_dates = set(all_dates)\n",
    "    \n",
    "    return all_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Overlap<a name=\"overlap\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(input_data):\n",
    "    \n",
    "    ' Get the temporal overlap of EPEs between the ERA5 and EOBS datasets for user-defined percentile'\n",
    "    \n",
    "    P_ERA5 = input_data[0] # percentile for ERA5 for defining the days of EPEs\n",
    "    P_EOBS = input_data[1] # percentile for EOBS for defining the days of EPEs\n",
    "    offset = input_data[2] # temporal window for allowing flexibility in overlap between the EPEs \n",
    "\n",
    "    ERA5_Q = dates_extreme(dataset=ERA5, quantile=P_ERA5/100) # get the days of EPEs from ERA5\n",
    "    EOBS_Q = dates_extreme(dataset=EOBS, quantile=P_EOBS/100) # get the days of EPEs from EOBS\n",
    "    \n",
    "    Common = ERA5[0] # generate xarray object for storing the overlap results\n",
    "    \n",
    "    # Calculate the overlap for each grid cell, with or without considering temporal flexibility at the EOBS results.\n",
    "    # Check if EOBS have at least 1 day (len(j)!=0) because EOBS has no data over the sea.\n",
    "    if offset == 0: # if no flexibility window, for time efficiency do not use the \"days_extend\" function\n",
    "        common_percent = [len(set(i) & set(j))/len(i)*100 if len(j)!=0 else np.nan \n",
    "                          for i, j in zip(ERA5_Q, EOBS_Q)]\n",
    "    else:\n",
    "        common_percent = [len(set(i) & days_extend(j, offset))/len(i)*100 if len(j)!=0 else np.nan \n",
    "                          for i, j in zip(ERA5_Q, EOBS_Q)]\n",
    "        \n",
    "    Common.values = np.reshape(common_percent, Common.shape) # assign the overlap values to the final dataset\n",
    "    Common = Common.assign_coords({'time': str(input_data)}) # assign the coordinate value based on the input data\n",
    "    Common = Common.rename({'time': 'Input_comb'}) # rename coordinate\n",
    "\n",
    "    return Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the list with the combinations checked for the temporal overlap between EOBS and ERA5. EOBS data are not provided at UTC, rather they are based on the \"day\" as used by the measuring authorities of the different countries/regions. Thus, there can be a +-1 day shift of the main hours of a precipitation event between ERA5 and EOBS. Findings about such temporal shift of the EOBS dataset are identified by previous researches, e.g. *Turco et al, 2013*: https://doi.org/10.5194/nhess-13-1457-2013. For this reason the temporal overlap is analysed considering a 1-day offset (3-days daily window centered at each day identified by the EOBS data). Moroever, the overlap is checked for each studied percentile of ERA5, and the same percentile of EOBS, as well as 2/100 lower percentile, to check the overlap when there is a flexibility in the intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combs = [[(i, i, 1), (i, i-2, 1)] for i in P_used] # input data for temporal overlap (P_ERA5, P_EOBS, offset)\n",
    "Combs = [j for i in Combs for j in i] # drop the internal lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [50:42<00:00, 507.11s/it]  \n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "Overlaps = list(tqdm.tqdm(pool.imap(calculate_overlap, Combs), total=len(Combs), position=0, leave=True))\n",
    "pool.close()\n",
    "Overlaps = xr.concat(Overlaps, dim='Input_comb') # concatenate to a single xarray dataarray\n",
    "\n",
    "del(pool)\n",
    "\n",
    "Overlaps = Overlaps.where(NANs_EOBS) # mask for keeping locations with less than 5% missing data\n",
    "Overlaps.to_netcdf(dir_loc+'DataForPlots/Overlap_ERA5_EOBS.nc') # save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observational Data - Connecting EPEs to Large-Scale Atmoshperic Flow Patterns <a name=\"extremes-to-patterns\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing<a name=\"preprocessing-connections\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = dir_loc + 'DataForPlots/Clusters_'+Combination_used+'.csv'\n",
    "Clustering = pd.read_csv(file_name, index_col=0)\n",
    "Clustering.index = pd.to_datetime(Clustering.index).strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate thresholds per location and percentile\n",
    "with warnings.catch_warnings(): # if all are NaN then it gives Runtimewarning, which is now suppressed\n",
    "    warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "    Quant = EOBS.quantile(np.array(P_used)/100, interpolation='linear', dim='time', keep_attrs=True) # thresholds\n",
    "\n",
    "Quant = Quant.rename({'quantile': 'percentile'}) # rename coordinate\n",
    "Quant = Quant.assign_coords({'percentile': P_used}) # assign the dim values based on lags\n",
    "\n",
    "# boolean xarray for identifying if an event is over the threshold\n",
    "Exceed_xr = [(EOBS>Quant.sel(percentile=i_p))*1 for i_p in P_used] \n",
    "Exceed_xr = xr.concat(Exceed_xr, dim=pd.Index(P_used, name='percentile')) # concatenate data for all percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions <a name=\"auxiliary\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative distribution of binomial for statistical significance testing\n",
    "def binom_test(occurrences, propabilities):\n",
    "    return binom.cdf(k=occurrences-1, n=occurrences.sum(), p=propabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix(data, lead=1):\n",
    "    \n",
    "    '''     \n",
    "    Function for calculating the transition matrix M of an item (list/numpy/pandas Series/pandas single column DF),\n",
    "    where M[i][j] is the probablity of transitioning from state i to state j.\n",
    "    Basic code taken from stackoverflow:\n",
    "    https://stackoverflow.com/questions/46657221/generating-markov-transition-matrix-in-python\n",
    "    \n",
    "    NOTE!: Data should not have NaN values, otherwise code crushes!\n",
    "    \n",
    "    :param data : input data: one dimensional vector with elements of same type (e.g. all str, or all float, etc)\n",
    "    :param lead : lead time for checking the transition (default=1)\n",
    "    :return     : transition matrix as pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    if type(data) == pd.core.frame.DataFrame:\n",
    "        data_used = list(data.values.flatten())\n",
    "    else:\n",
    "        data_used = data\n",
    "\n",
    "    unique_states = sorted(set(data_used)) # get the names of the unique states and sort them\n",
    "    \n",
    "    dict_sequencial = {val: i for i, val in enumerate(unique_states)} # sequencial numbering of states\n",
    "    \n",
    "    transitions_numbered = pd.Series(data_used).map(dict_sequencial) # map the data to sequencial order\n",
    "    transitions_numbered = transitions_numbered.values # get only the actual values of the Series\n",
    "    \n",
    "    n = len(unique_states) # number of unique states\n",
    "\n",
    "    M = [[0]*n for _ in range(n)] # transition matrix\n",
    "\n",
    "    for (i,j) in zip(transitions_numbered,transitions_numbered[lead:]): # the total times of the transition M[i][j]\n",
    "        M[i][j] += 1\n",
    "\n",
    "    # now convert to probabilities:\n",
    "    for row in M:\n",
    "        s = sum(row)\n",
    "        if s > 0:\n",
    "            row[:] = [f/s for f in row]\n",
    "    \n",
    "    M = pd.DataFrame(M, columns=unique_states, index=unique_states) # convert to DF and name columns/rows as per data\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_clusters(n_clusters):\n",
    "    \n",
    "    ' Calculate statistics of occurences and limits of climatological frequencies for each cluster '\n",
    "    \n",
    "    Data = Clustering['Clusters_'+str(n_clusters)] # cluster pd.Series with cluster label for each day\n",
    "    \n",
    "    # days per cluster, and statistics of total occurrences\n",
    "    Totals = Data.value_counts() # days per cluster (use all the daily data available at the clustering results)\n",
    "    Totals = pd.DataFrame(Totals.reindex(range(n_clusters))) # sort the data per cluster order\n",
    "    Totals.rename(columns={'Clusters_'+str(n_clusters): 'Occurrences'}, inplace=True) # rename column\n",
    "    \n",
    "    # persistence, climatological frequencies, and effective size due to persistence\n",
    "    transitions = transition_matrix(Data) # next-day transition probs matrix\n",
    "    Totals['Persistence'] = np.diag(transitions) # self-transition probability\n",
    "    total_days = len(Data) # total days used for clustering\n",
    "    Totals['Percent'] = Totals['Occurrences']/total_days # climatological frequencies\n",
    "    Totals['N_ef'] = total_days*(1-Totals['Persistence'])/(1+Totals['Persistence']) # effective length\n",
    "\n",
    "    # 95% CI of climatological frequencies: use normal approximation to Binomial distr. considering effective length \n",
    "    Totals['Perc_Upper'] = Totals['Percent']+1.96*np.sqrt(Totals['Percent']*(1-Totals['Percent'])/Totals['N_ef'])\n",
    "    Totals['Perc_Lower'] = Totals['Percent']-1.96*np.sqrt(Totals['Percent']*(1-Totals['Percent'])/Totals['N_ef'])\n",
    "\n",
    "    # Precipitation data do not include 1st Jan 1979, so use the Precipitation dates for accurate results\n",
    "    dates_all = EOBS.time.values\n",
    "    subset_totals = Data.loc[dates_all].value_counts() # days per cluster for the dates available in Precip. data\n",
    "    Totals['Subset_Occurrences'] = subset_totals.reindex(range(n_clusters)) # sort the data per cluster order\n",
    "    Totals['Occur_Max'] = np.ceil(Totals['Perc_Upper']*len(dates_all)) # ceiling to get the next integer\n",
    "    \n",
    "    return (Totals, Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying the connections <a name=\"quantifying-connections\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extremes_to_clusters(n_clusters):\n",
    "    \n",
    "    ' Calculate connection of extremes to patterns; % of events per cluster, condit. prob. and stat. sign. '\n",
    "    ' inputa data: number of clusters used'\n",
    "    \n",
    "    Totals, Data = statistics_clusters(n_clusters) # get statistics of clusters and daily attributions of labels\n",
    "    \n",
    "    ExceedCounts = Exceed_xr.copy(deep=True)\n",
    "    ExceedCounts = ExceedCounts.assign_coords({'time': Data.loc[Exceed_xr.time.values].values}) # change to cluster id\n",
    "    ExceedCounts = ExceedCounts.rename({'time': 'cluster'}) # rename the coordinate\n",
    "    ExceedCounts = ExceedCounts.groupby('cluster').sum() # find total extremes at each cell allocated per cluster\n",
    "    \n",
    "    RatioCluster = ExceedCounts.transpose(..., 'cluster')/Totals['Subset_Occurrences'].values*100 # conditional prob.\n",
    "    RatioClusterMax = ExceedCounts.transpose(..., 'cluster')/Totals['Occur_Max'].values*100 # cond. prob. of 95% freq.\n",
    "    Exceed_Perc = ExceedCounts/ExceedCounts.sum(dim=['cluster'])*100 # percent of extremes per cluster\n",
    "    \n",
    "    \"check statistical significance of occurrences based on binomial distribution for 95% Confidence Interval\"\n",
    "    # perform the analysis for the Upper tail and use the Upper 95% CI for the cluster probability\n",
    "    Binom_Cum_Upper = ExceedCounts.copy(deep=True) # new xr (SOS: deep=True otherwise the data are overwritten later)\n",
    "    Binom_Cum_Upper = Binom_Cum_Upper.astype(float) # convert to float from int\n",
    "    Binom_Cum_Upper = Binom_Cum_Upper.transpose('cluster', ...)\n",
    "    Counts_np = Binom_Cum_Upper.values.copy() # numpy of values for applying the function below\n",
    "    Binom_Cum_Upper_np = np.apply_along_axis(binom_test, propabilities=Totals['Perc_Upper'],  axis=0, arr=Counts_np)\n",
    "    Binom_Cum_Upper[:] = Binom_Cum_Upper_np # pass the results to the xr\n",
    "\n",
    "    # perform the analysis for the Lower tail and use the Lower 95% CI for the cluster propability\n",
    "    Binom_Cum_Lower = Binom_Cum_Upper.copy(deep=True)\n",
    "    Binom_Cum_Lower_np = np.apply_along_axis(binom_test, propabilities=Totals['Perc_Lower'],  axis=0, arr=Counts_np)\n",
    "    Binom_Cum_Lower[:] = Binom_Cum_Lower_np\n",
    "\n",
    "    Sign = (Binom_Cum_Upper > .975)*1 + (Binom_Cum_Lower < .025)*(-1) # assign boolean for statistical significance\n",
    "\n",
    "    # final object with counts, percentages, and statistical significance\n",
    "    All_data = [ExceedCounts, Exceed_Perc, RatioCluster, RatioClusterMax, Sign]\n",
    "    Coord_name = ['Counts', 'PercExtremes', 'CondProb', 'CondProbUpperLimit', 'Significance']\n",
    "    Coord_name = pd.Index(Coord_name, name='indicator')\n",
    "    Final = xr.concat(All_data, dim=Coord_name)\n",
    "    \n",
    "    return Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtremesClusters = extremes_to_clusters(n_clusters=Clusters_used)\n",
    "ExtremesClusters = ExtremesClusters.where(NANs_EOBS) # mask for keeping locations with less than 5% missing data\n",
    "ExtremesClusters.to_netcdf(dir_loc+'DataForPlots/ClusteringStats_Med_SLP~Z500_Clusters9_EOBS.nc') # save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Analysis completed in: 0:56:27.540382  HR:MN:SC.\n"
     ]
    }
   ],
   "source": [
    "print('Total Analysis completed in:', datetime.now() - InitializationTime, ' HR:MN:SC.')\n",
    "del(InitializationTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
