{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Environment](#environment)\n",
    "    1. [Imports](#imports)\n",
    "    2. [User-defined inputs](#inputs)\n",
    "3. [Data Download](#download)\n",
    "    1. [Data preprocessing](#preprocessing)\n",
    "    2. [Download Atmospheric Variables](#atm_vars)\n",
    "    3. [Download Precipitation](#precipitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data for the analysis in the paper: <a name=\"introduction\"></a>\n",
    "### [Extreme precipitation events in the Mediterranean: Spatiotemporal characteristics and connection to large-scale atmospheric flow patterns](https://rmets.onlinelibrary.wiley.com/doi/10.1002/joc.6985)\n",
    "\n",
    "---\n",
    "Author: Nikolaos Mastrantonas\\\n",
    "Email: nikolaos.mastrantonas@ecmwf.int; nikolaos.mastrantonas@doktorand.tu-freiberg.de\n",
    "\n",
    "---\n",
    "This works uses ERA5 data for quantifying the connections of localized extreme precipitation to large-scale patterns. The main variables that are used are:\n",
    "1. Mean Sea Level Pressure\n",
    "2. Temperature at 850hPa\n",
    "3. Geopotential height at 500hPa\n",
    "4. Total Precipitation\n",
    "\n",
    "Moreover, there are some additional variables tested that are related to moisture/water content. These variables are:\n",
    "1. Specfic humidity at 850hPa \n",
    "2. Water Vapour Flux (eastwards and northwards components)\n",
    "\n",
    "The downloading is done via the MARS data storage facility of ECMWF. Information about how to download data is available at [this](https://confluence.ecmwf.int/display/CKB/How+to+download+data+via+the+ECMWF+WebAPI) link.\n",
    "In case of no access to MARS, the data are also available from Copernicus CDS (https://cds.climate.copernicus.eu/#!/home). Downloading data from CDS requires non-trivial amendments in this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment<a name=\"environment\"></a>\n",
    "Load the required packages and get the user-defined inputs.\n",
    "The downloading was done in a Linux machine with 8 CPUs and 32 GB RAM. The total duration was about 8 hours. Downloading variables takes about 6 hours per variable if single CPU is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages (full package or specific functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing # parallel processing\n",
    "import tqdm # timing\n",
    "from datetime import datetime\n",
    "from itertools import groupby\n",
    "from pathlib import Path # creation of dictionaries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import metview as mv # metview package for downloading the data from MARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined inputs <a name=\"inputs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the main folder where the data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_loc = 'Data/'\n",
    "Path(dir_loc).mkdir(parents=True, exist_ok=True) # generate the subfolder for storing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inputs for the spatiotemporal coverage, and grid resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_generated_all = pd.date_range(start = '19790101', end = '20191231').strftime('%Y%m%d').to_list() # used dates\n",
    "\n",
    "area_precipt = [47, -8, 29, 38] # coordinates as in N/W/S/E of the subdomain of interest (Mediterranean domain)\n",
    "grid_precipt = [.25, .25] # grid resolution in degrees\n",
    "\n",
    "area_atm_var = [80, -90, 10, 50] # [70, -60, 10, 80] # extended area compared to Precipitation data\n",
    "grid_atm_var = [1, 1] # coarser resolution compared to Precipitation, as the interest is on large-scale patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables to be downloaded. The required information should be given in a list and include the following, in the exact order:\n",
    "1. **levelist**: level of interest, e.g. 0 for surface parameters, 500 for 500 hPa\n",
    "2. **levtype**: leveltype of interest, e.g. pressure levels ('pl'), surface ('sfc')\n",
    "3. **atm_var**: paramater of interest (e.g. the SLP is flagged as 151.128 at *MARS*)\n",
    "4. **file_name**: name of the file to save the data\n",
    "\n",
    "The above information is used on the function to retrieve data from *MARS* with the *metview* package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = [[0, 'sfc', '151.128', 'D1_Mean_SLP'],   # SLP data\n",
    "               [500, 'pl', '129.128', 'D1_Mean_Z500'], # Z500 data\n",
    "               [850, 'pl', '130.128', 'D1_Mean_T850'], # T850 data\n",
    "               [850, 'pl', 'q', 'D1_Mean_Q850'], # Q850 data  (not used for the main analysis)\n",
    "               [0, 'sfc', '71.162', 'D1_Mean_WVFeast'], # Water Vapour Flux east data (not used for main analysis)\n",
    "               [0, 'sfc', '72.162', 'D1_Mean_WVFnorth'], # Water Vapour Flux north data (not used for main analysis)\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download<a name=\"download\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing<a name=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitializationTime = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # dates are chunked per year-month for efficient download, since MARS uses this subsetting for storing the data\n",
    "dates_atm_vars = [list(v) for l, v in groupby(dates_generated_all[:], lambda x: x[:6])]\n",
    "\n",
    "# repeat the last value of each chunk to the next one, since daily precip data need info from previous day as well!\n",
    "dates_precipit = [dates_atm_vars[i][-1:] + dates_atm_vars[i+1] for i in range(len(dates_atm_vars)-1)] # from 2nd chunk\n",
    "dates_precipit.insert(0, dates_atm_vars[0]) # append the 1st chunk so all the dates are now complete\n",
    "\n",
    "# create a slighly larger extend so that the interpolation of precip data on the edges of the domain works better\n",
    "Area_precipt_ext = [coord+2 if i in [0, 3] else coord-2 for i, coord in enumerate(area_precipt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download mean daily data of atmospheric variables<a name=\"atm_vars\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For some reason, the multiprocessing that is used for speading up the process, works only if the atmospheric variables data are downloaded first, and then the precipitation data**. There is no understanding of how and why this issue occurs, but at least data are correct and there are no wrong outputs from the downloading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atm_subset(input_data):\n",
    "    \n",
    "    levelist, levtype, atm_var, dates_subset = input_data # inputs to be a list of 4 in specific order!\n",
    "    \n",
    "    '''\n",
    "    Function for downloading data of atmospheric variables from MARS and calculating daily mean values\n",
    "    \n",
    "    :param levelist: level of interest, e.g. 0 for surface parameters, 500 for 500 hPa\n",
    "    :param levtype: leveltype of interest, e.g. pressure levels ('pl'), surface ('sfc')\n",
    "    :param atm_var: paramater of interest (e.g. the SLP is flagged as 151.128 at MARS)\n",
    "    :param dates_subset: the subset of dates to be downloaded\n",
    "    '''\n",
    "    \n",
    "    # function for retrieving the data from MARS\n",
    "    fc_all = mv.retrieve(Class = 'ea', # class of data, e.g. ERA5 ('ea')\n",
    "                         stream = 'oper', # stream of interest, e.g. Ensemble ('enfo'), Deterministic ('oper') \n",
    "                         expver = 1, # experiment's version, e.g. Operational (1), Research (xxxx[A-Z/0-9])\n",
    "                         type = 'an', # type of data, e.g. Analysis ('an')\n",
    "                         param = atm_var, \n",
    "                         levtype = levtype,\n",
    "                         levelist = levelist,\n",
    "                         date = dates_subset,\n",
    "                         time = list(range(0,24)), # all hourly timesteps\n",
    "                         area = area_atm_var,\n",
    "                         grid = grid_atm_var, \n",
    "                         )\n",
    "\n",
    "    Daily_sub = mv.Fieldset() # mv for values for dates_subset\n",
    "    fields = mv.grib_get(fc_all, ['date']) # get the 'date' field from the fc_all object\n",
    "\n",
    "    for day_i in dates_subset: # loop through the whole list of unique dates_subset\n",
    "\n",
    "        used_indices = list(np.where(np.array(fields) == day_i)[0]) # indices that belong to the day of interest\n",
    "        used_indices = np.array(used_indices, dtype='float64') # convert to float64 for using it at mv object\n",
    "        daily_subset = fc_all[used_indices] # subset and keep only the fields of the day of interest\n",
    "        Daily_sub.append(mv.mean(daily_subset)) # calculate the daily mean and append it\n",
    "    \n",
    "    return Daily_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data per variable in a dictionary and name the keys, based on the variable name, e.g. for \"D1_Mean_SLP\", keep the \"SLP\" for the key.\n",
    "\n",
    "Note that for optimizing the downloading in MARS, it is preferable to loop through dates and download all variables, instead of looping through variables and downloading the dates ([find out more](https://confluence.ecmwf.int/display/WEBAPI/Retrieval+efficiency)). The latter is used in this script for making it simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 492/492 [1:18:00<00:00,  9.51s/it]\n",
      "100%|██████████| 492/492 [59:33<00:00,  7.26s/it]  \n",
      "100%|██████████| 492/492 [55:07<00:00,  6.72s/it]  \n",
      "100%|██████████| 492/492 [1:00:22<00:00,  7.36s/it]\n",
      "100%|██████████| 492/492 [1:16:12<00:00,  9.29s/it]  \n",
      "100%|██████████| 492/492 [1:12:04<00:00,  8.79s/it]\n"
     ]
    }
   ],
   "source": [
    "Times = len(dates_atm_vars)\n",
    "    \n",
    "AtmVar = {}\n",
    "for var in data_inputs:\n",
    "    \n",
    "    levelist, levtype, atm_var, file_name = var \n",
    "    Inputs = list(zip([levelist]*Times, [levtype]*Times, [atm_var]*Times, dates_atm_vars))\n",
    "    \n",
    "    pool_atmvar = multiprocessing.Pool() # object for multiprocessing\n",
    "    Daily = list(tqdm.tqdm(pool_atmvar.imap(atm_subset, Inputs), \n",
    "                           total=Times, position=0, leave=True)) # list of mv.Fieldsets\n",
    "    pool_atmvar.close()\n",
    "    del(pool_atmvar)\n",
    "    \n",
    "    for i in range(1, Times): # concatenate all Fieldsets to the first one\n",
    "        Daily[0].append(Daily[i])\n",
    "\n",
    "    Daily = Daily[0] # keep the full set of the atmospheric variable data\n",
    "    \n",
    "    mv.write(dir_loc + file_name + '.grb', Daily) # save data\n",
    "\n",
    "    AtmVar[var[-1].split('_')[-1]] = Daily\n",
    "    \n",
    "del(Times, var, levelist, levtype, atm_var, file_name, Inputs, Daily, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download total daily precipitation<a name=\"precipitation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precip_subset(dates_subset):\n",
    "    \n",
    "    ' Function for downloading precipitation data from MARS and calculating daily total values '\n",
    "    \n",
    "    fc_all = mv.retrieve(Class = 'ea', # class of data, e.g. ERA5 ('ea')\n",
    "                         stream = 'oper', # stream of interest, e.g. Ensemble ('enfo'), Deterministic ('oper') \n",
    "                         expver = 1, # experiment's version, e.g. Operational (1), Research (xxxx[A-Z/0-9])\n",
    "                         type = 'fc', # type of data, e.g. Forecast ('fc'), Analysis ('an')\n",
    "                         param = 'tp', # used paramater: Total Precipitation ('tp' = '228.128')\n",
    "                         levtype = 'sfc',\n",
    "                         levelist = 0,\n",
    "                         date = dates_subset, # use the subset of dates\n",
    "                         time = [6, 18], # time steps of interest (forecast fields only at 06:00 & 18:00)\n",
    "                         step = list(range(7,19)), # precipitation is calculated from short-range forecasted data\n",
    "                         grid = grid_precipt,\n",
    "                         area = Area_precipt_ext,\n",
    "                         interpolation = '\"--interpolation=grid-box-average\"' # first-order conservative remapping\n",
    "                         )\n",
    "\n",
    "    Daily_sub = mv.Fieldset() # create the mv object for storing the daily values for the dates_subset\n",
    "\n",
    "    for i_day in range(len(dates_subset) - 1): # loop through the whole list of unique dates_subset\n",
    "\n",
    "        # downloaded data are in the sequence: day_i 06:00 steps 7-18 (12 steps), 18:00 steps 7-18 (12 steps)\n",
    "        start_indice = 12 + 24*i_day # data for daily accumulation start at 18:00 step 7 of previous day\n",
    "        end_indice = 12 + 24*(i_day+1) # data end at 06:00 step 12 of current day (24 hourly steps in total)\n",
    "\n",
    "        sub = mv.sum(fc_all[start_indice : end_indice]) # total daily precipitation\n",
    "        sub = mv.grib_set(sub, ['date', int(dates_subset[i_day + 1])]) # replace the date field with the correct date\n",
    "\n",
    "        Daily_sub.append(sub) # append to Daily_sub metview object\n",
    "    \n",
    "    return Daily_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 492/492 [1:11:28<00:00,  8.72s/it]\n"
     ]
    }
   ],
   "source": [
    "pool_precip = multiprocessing.Pool() # object for multiprocessing for creating a list of mv.Fieldsets\n",
    "Precip = list(tqdm.tqdm(pool_precip.imap(precip_subset, dates_precipit), \n",
    "                        total=len(dates_precipit), position=0, leave=True))\n",
    "pool_precip.close()\n",
    "\n",
    "for i in range(1, len(Precip)): # concatenate all Fieldsets to the first one\n",
    "    Precip[0].append(Precip[i])\n",
    "\n",
    "Precip = Precip[0] # keep the full set of the precipitation data\n",
    "Precip = Precip*1000 # convert to mm\n",
    "Precip = mv.read(data=Precip, area=area_precipt) # crop to the actual area of interest\n",
    "\n",
    "mv.write(dir_loc + 'D1_Total_Precipitation.grb', Precip) # Save the daily total precipitation file\n",
    "\n",
    "del(pool_precip, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading completed in: 7:56:01.443161  HR:MN:SC.\n"
     ]
    }
   ],
   "source": [
    "print('Downloading completed in:', datetime.now() - InitializationTime, ' HR:MN:SC.')\n",
    "del(InitializationTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
