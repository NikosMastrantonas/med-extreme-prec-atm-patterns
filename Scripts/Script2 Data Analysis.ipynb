{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Environment](#environment)\n",
    "    1. [Imports](#imports)\n",
    "    2. [User-defined inputs](#inputs)\n",
    "3. [Data Analysis](#analysis)\n",
    "    1. [Spatiotemporal Analysis of Extreme Precipitation Events](#spatiotemporal)\n",
    "        1. [Preprocessing](#spatiotemporal-preprocessing)\n",
    "        2. [Seasonality](#seasonality)\n",
    "        3. [Temporal Dependencies](#temporal-dependencies)\n",
    "        4. [Spatiotemporal Overlap](#spatiotemporal-overlap)\n",
    "    2. [EOF and Clustering](#eof-clustering)\n",
    "        1. [Preprocessing](#eof-clustering-preprocessing)\n",
    "        2. [EOF Analysis](#eof-analysis)\n",
    "        3. [Clustering Analysis](#clustering-analysis)\n",
    "        4. [Frequencies of Clusters](#clustering-frequencies)\n",
    "    3. [Connecting Extremes to Large-Scale Patterns](#extremes-to-patterns)\n",
    "        1. [Auxiliary Functions](#auxiliary)\n",
    "        2. [Quantifying the Connections](#quantifying-connections)\n",
    "        3. [Summarizing Results](#summarizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis for the work presented in the paper: <a name=\"introduction\"></a>\n",
    "### [Extreme precipitation events in the Mediterranean: Spatiotemporal characteristics and connection to large-scale atmospheric flow patterns](https://rmets.onlinelibrary.wiley.com/doi/10.1002/joc.6985)\n",
    "\n",
    "---\n",
    "Author: Nikolaos Mastrantonas\\\n",
    "Email: nikolaos.mastrantonas@ecmwf.int; nikolaos.mastrantonas@doktorand.tu-freiberg.de\n",
    "\n",
    "---\n",
    "The data analysis is oranized in three sections:\n",
    "1. **Spatiotemporal Analysis of Extreme Precipitation Events (EPEs)**: Find the seasonality, temporal dependencies, and spatiotemporal overlap.\n",
    "2. **EOF and Clustering**: Perform EOF analysis and subsequent K-means clustering for a range of combinations of domains and atmospheric variables.\n",
    "3. **Connecting Extremes to Large-Scale Patterns**: Use the results of previous step and analyse how the different clusters relate to EPEs for each grid cell of the studied domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment<a name=\"environment\"></a>\n",
    "Load the required packages and get the user-defined inputs.\n",
    "\n",
    "The analysis was done in a Linux machine with 8 CPUs and 32 GB RAM. The total duration was about 1.5 hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages (full package or specific functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing # parallel processing\n",
    "import tqdm # timing\n",
    "from datetime import datetime # timing\n",
    "from pathlib import Path # creation of dictionaries\n",
    "import warnings # for suppressing RuntimeWarning\n",
    "\n",
    "# basic libraries for data analysis\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import xarray as xr\n",
    "\n",
    "from itertools import combinations, product\n",
    "\n",
    "# specialized libraries\n",
    "from eofs.xarray import Eof # EOF analysis\n",
    "from sklearn.cluster import KMeans # K-means clustering\n",
    "from scipy.stats import binom # binomial distribution for significance testing of extremes and large-scale patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined inputs <a name=\"inputs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dictionary for the input data, and specify & create the subfolder for storing all data for generating the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_loc = '' # the main folder where the input data are stored\n",
    "\n",
    "results_loc = dir_loc + 'DataForPlots/' # the subfolder for storing the results\n",
    "Path(results_loc).mkdir(parents=True, exist_ok=True) # generate the subfolder for storing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inputs related to Spatiotemporal Analysis of the Precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grb_file_name = 'Data/D1_Total_Precipitation.grb' # the name of the grb file of the precipitation data\n",
    "\n",
    "P_used = [95, 97, 99] # define the percentile(s) of interest\n",
    "\n",
    "lags_depend_sets = [1, 3, 7, 15] # check % of extremes that happen up to x days from a previous extreme on same cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inputs related to EOF and Clustering analysis of the Atmospheric Variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_used = ['SLP', 'T850', 'Z500'] # variables used for the clustering analysis\n",
    "\n",
    "# define the areas used for EOF and clustering: dict{area_name: [N, W, S, E]}. Prefered order from larger to smaller\n",
    "Area_used = {'EuroAtlantic': [80, -90, 20, 30], # EuroAtlantic as used by Cassou 2008\n",
    "             'MedExt': [55, -20, 20, 50], 'MedExtAtl': [51, -17, 26, 41], 'Med': [50, -11, 26, 41] }\n",
    "\n",
    "Var_ex = 90 # define the minimum total variance [0-100] that the subset of kept EOFs should explain\n",
    "Clusters_used = [4]+list(range(7, 13)) # clusters for Kmeans (4 for also analysing the common EuroAtlantic regimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis<a name=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitializationTime = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatiotemporal Analysis of Extreme Precipitation <a name=\"spatiotemporal\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Precipitation data and generate auxiliary elements <a name=\"spatiotemporal-preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precipitation_xr = xr.open_dataarray(dir_loc + grb_file_name, engine='cfgrib') # read data\n",
    "Precipitation_xr = Precipitation_xr.drop(['valid_time', 'step', 'surface', 'number']) # drop not-used coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_used = sorted(list(np.array(P_used).flatten())) # make P_used a sorted list for consistency and avoiding errors\n",
    "lags_depend_sets = sorted(list(np.array(lags_depend_sets).flatten())) # same as above but for temporal dependancies\n",
    "\n",
    "dates_all = pd.to_datetime(Precipitation_xr.time.values) # extract the dates of the xarray\n",
    "dates_all = pd.to_datetime(dates_all.strftime('%Y%m%d')) # convert time to refer to start of day (actual is at 18:00)\n",
    "Precipitation_xr = Precipitation_xr.assign_coords({'time': dates_all}) # change time to start of day\n",
    "    \n",
    "# calculate thresholds per location and percentile\n",
    "Quant = Precipitation_xr.quantile(np.array(P_used)/100, interpolation='linear', dim='time', keep_attrs=True) # thresh.\n",
    "Quant = Quant.rename({'quantile': 'percentile'}) # rename coordinate\n",
    "Quant = Quant.assign_coords({'percentile': P_used}) # assign the coord values based on percentiles\n",
    "    \n",
    "Quant.to_netcdf(results_loc+'ThresholdsEPEs.nc') # save data\n",
    "\n",
    "# boolean xarray for identifying if an event is over the threshold\n",
    "Exceed_xr = [(Precipitation_xr>Quant.sel(percentile=i_p))*1 for i_p in P_used] \n",
    "Exceed_xr = xr.concat(Exceed_xr, dim=pd.Index(P_used, name='percentile')) # concatenate data for all percentiles\n",
    "\n",
    "# create a 2d dataframe of the precipitation data, with index refer to dates, and columns to grid cells\n",
    "Prcp_df = Precipitation_xr.values.flatten()\n",
    "Prcp_df = pd.DataFrame(np.reshape(Prcp_df, (len(Precipitation_xr), -1)), index=dates_all.strftime('%Y%m%d'))\n",
    "\n",
    "# create DF with the coordinates of the grid points when used as a 2d dataframe\n",
    "LON, LAT = np.meshgrid(Precipitation_xr.longitude.values, Precipitation_xr.latitude.values)\n",
    "Coords = pd.DataFrame({'LAT': LAT.flatten(), 'LON': LON.flatten()})\n",
    "\n",
    "del(LON, LAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality of Extremes <a name=\"seasonality\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_grouped = [(int(month)%12 + 3)//3 for month in dates_all.month] # 1-4 refers to Winter-Autumn\n",
    "\n",
    "Exceed_xr = Exceed_xr.assign_coords({'time': dates_grouped}) # change the time values so they refer to the Season\n",
    "Seasonality = Exceed_xr.groupby('time').sum()/Exceed_xr.sum(dim='time')*100 # percentage of EPEs per Season\n",
    "Seasonality = Seasonality.rename({'time': 'season'}) # rename coordinate\n",
    "Seasonality.to_netcdf(results_loc+'Seasonality.nc') # save data\n",
    "\n",
    "Exceed_xr = Exceed_xr.assign_coords({'time': Precipitation_xr.time.values}) # rename to the actual time\n",
    "\n",
    "del(dates_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Dependencies of Extremes <a name=\"temporal-dependencies\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependence(x, required_lags=[1, 3, 4, 7, 15]):\n",
    "    \n",
    "    '''Return the % of events (\"x\": dates of events) occuring up to \"required_lags\" days after a preceding events'''\n",
    "    \n",
    "    # pre-process the provided lags so that the function works without errors\n",
    "    required_lags_used = np.array([required_lags]).flatten()\n",
    "    required_lags_used = required_lags_used[required_lags_used != 1] # remove lag 1 cause it causes problems\n",
    "    required_lags_used = list(required_lags_used)\n",
    "    \n",
    "    # sort array of dates and find the temporal distance in days\n",
    "    dates_datetime = pd.to_datetime(x).sort_values() # convert to datetime and sort\n",
    "    dates_dif = np.diff(dates_datetime)/np.timedelta64(1, 'D') # find difference in days (dif. days)\n",
    "    final_stats = pd.Series(dates_dif).value_counts().sort_index().cumsum() # cumulative sum of events per dif. days\n",
    "    final_stats.index = final_stats.index.astype(int) # convert index to integer (index refers to temporal lag)\n",
    "    \n",
    "    lag1_aux = pd.Series({1:0}) # in case there is no lag1 instances, the index is created and given 0 (0% of EPEs)\n",
    "    final_stats = final_stats.combine(lag1_aux, max, fill_value=0) # fill 1-day lag with 0 if not available\n",
    "    wanted_lags = pd.Series(np.nan, index=required_lags_used) # empty series with the lags of interest (lag 1 always)\n",
    "    final_stats = final_stats.combine(wanted_lags, max) # add the user-defined lags if not available\n",
    "    final_stats.fillna(method='ffill', inplace=True, axis=0) # fill nan (in case some user-defined lags were missing)\n",
    "    \n",
    "    final_stats = final_stats/(len(x)-1)*100 # calculate the percetange of events within each lag\n",
    "    final_stats = final_stats.loc[required_lags, ] # return only the lags of interest\n",
    "    \n",
    "    return final_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_dependence_xr(percentile):\n",
    "    \n",
    "    Quant_used = Quant.sel(percentile=percentile)\n",
    "    Extremes = Prcp_df > Quant_used.values.flatten() # Boolean over / under-upto the Precip threshold\n",
    "    DaysExceed = Extremes.apply(lambda x: list(Extremes.index[np.where(x == 1)[0]]), axis=0) # exceedance days\n",
    "    depend_perc = DaysExceed.apply(dependence, required_lags=lags_depend_sets).T # find temporal dependence\n",
    "    \n",
    "    Temp_depend = Precipitation_xr[:len(lags_depend_sets)].copy(deep=True) # generate final xr with grided results\n",
    "    Temp_depend = Temp_depend.rename({'time': 'temporal_lag'}) # rename coordinate\n",
    "    Temp_depend = Temp_depend.assign_coords({'temporal_lag': lags_depend_sets}) # assign the dim values based on lags\n",
    "    depend_perc = depend_perc.values # get the values in np array so that xr gets the values in the correct order\n",
    "    Temp_depend.values = np.reshape(depend_perc, Temp_depend.shape) # place the values to the final xarray item\n",
    "        \n",
    "    return Temp_depend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:13<00:00, 64.63s/it] \n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "TemporalDependencies = list(tqdm.tqdm(pool.imap(temporal_dependence_xr, P_used), \n",
    "                                      total=len(P_used), position=0, leave=True))\n",
    "pool.close()\n",
    "\n",
    "TemporalDependencies = xr.concat(TemporalDependencies, dim=pd.Index(P_used, name='percentile')) # concatenate data\n",
    "TemporalDependencies.to_netcdf(results_loc+'TemporalDependencies.nc') # save data\n",
    "\n",
    "del(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatiotemporal Dependencies of Extremes <a name=\"spatiotemporal-overlap\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatiotemporal_dep(percentile):\n",
    "    \n",
    "    Quant_used = Quant.sel(percentile=percentile)\n",
    "    Extremes = Prcp_df > Quant_used.values.flatten() # Boolean over / under-up to Precip threshold\n",
    "    total_extremes = Extremes.sum() # number of extremes per grid cell\n",
    "    \n",
    "    global spatiotemporaloverlap_singlecell# Extremes, total_extremes\n",
    "    ' Funtion for spatiotemporal analysis per gridcell. Set in global env. so it can be used in parallel processing '\n",
    "    def spatiotemporaloverlap_singlecell(i_cell):\n",
    "        \n",
    "        Data = Precipitation_xr[0].copy(deep=True) # keep a slice of xarray for having the same format \n",
    "        Data = Data.rename({'time': 'coordinates'}) # rename time to coordinates (lat, lon of target grid cell)\n",
    "        Data = Data.assign_coords({'coordinates': i_cell}) # assign the value of index of the analysed grid cell\n",
    "\n",
    "        extremes_i_cell = Extremes[Extremes.iloc[:,i_cell]==1] # keep only dates of extremes\n",
    "        overlap = extremes_i_cell.sum() # count extremes per each loc that happen same day as extremes at i_cell\n",
    "        overlap /= total_extremes # divide with total extremes per location, to find the percentage of co-occurrence\n",
    "        overlap *= 100 # multiply with 100 for giving the value as percentage   \n",
    "\n",
    "        Data.values = np.reshape(overlap.values, Data.shape) # place the values to the final xarray item\n",
    "\n",
    "        return Data\n",
    "    \n",
    "    pool = multiprocessing.Pool() # object for multiprocessing\n",
    "    Spatiotemporal_overlap = list(tqdm.tqdm(pool.imap(spatiotemporaloverlap_singlecell, range(Extremes.shape[1])), \n",
    "                                            total=Extremes.shape[1], position=0, leave=True))\n",
    "    Spatiotemporal_overlap = xr.concat(Spatiotemporal_overlap, dim='coordinates') # concatenate all data\n",
    "    pool.close()\n",
    "    \n",
    "    del(spatiotemporaloverlap_singlecell) # delete the function so it is not anymore in the global environment\n",
    "    \n",
    "    return Spatiotemporal_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13505/13505 [02:08<00:00, 104.80it/s]\n",
      "100%|██████████| 13505/13505 [02:03<00:00, 109.72it/s]\n",
      "100%|██████████| 13505/13505 [01:14<00:00, 181.75it/s]\n"
     ]
    }
   ],
   "source": [
    "SpatiotemporalOverlap = [spatiotemporal_dep(percentile=i_p) for i_p in P_used]\n",
    "SpatiotemporalOverlap = xr.concat(SpatiotemporalOverlap, dim=pd.Index(P_used, name='percentile')) # concatenate data\n",
    "\n",
    "# change the simple indexing value of the \"coordinates\" dim, with the actual coordinates of the target grid cell\n",
    "Coordinates = Coords.apply(lambda x:'Lat:{}, Lon:{}'.format(x[0], x[1]) , axis=1).values\n",
    "SpatiotemporalOverlap = SpatiotemporalOverlap.assign_coords({'coordinates': Coordinates})\n",
    "\n",
    "# save only the locations used for the plots\n",
    "locs_of_interest = [3435, 2417, 4385, 9072, 8118, 820, 622] # locs of interest for the plots\n",
    "SpatiotemporalOverlap.isel(coordinates=locs_of_interest).to_netcdf(results_loc+'SpatiotemporalOverlap.nc') # save data\n",
    "\n",
    "del(Coordinates, locs_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete variables not needed any more, for emptying memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(SpatiotemporalOverlap, spatiotemporal_dep, TemporalDependencies, temporal_dependence_xr, dependence, Seasonality,\n",
    "    Prcp_df, Precipitation_xr, lags_depend_sets, grb_file_name, Coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOF of Atmospheric variables and Clustering of daily patterns based on derived PCs <a name=\"eof-clustering\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data and perform preprocessing by calculating the daily anomalies <a name=\"eof-clustering-preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalies(variable):\n",
    "    \n",
    "    # read actual daily values\n",
    "    file_path = dir_loc + 'Data/D1_Mean_'+variable+'.grb'\n",
    "    Daily = xr.open_dataarray(file_path, engine='cfgrib') # read data\n",
    "    \n",
    "    actual_days = Daily.time.values # get actual timesteps\n",
    "    dates_grouped = pd.to_datetime(Daily.time.values).strftime('%m%d') # get Month-Day of each timestep\n",
    "    \n",
    "    # 5-day smoothed climatology. Rolling can be applied directly because the daily data refer to consequtive days. If\n",
    "    # days are not consecutive, firstly the xr.resample should be applied, so that missing days are generated with NaN\n",
    "    Smoothed = Daily.rolling(time=5, center=True, min_periods=1).mean() # 5-day smoothing\n",
    "    \n",
    "    Daily = Daily.assign_coords({'time': dates_grouped}) # change the time to Month-Day\n",
    "    Smoothed = Smoothed.assign_coords({'time': dates_grouped}) # change the time to Month-Day\n",
    "    \n",
    "    Climatology = Smoothed.groupby('time').mean() # climatology of the smoothed data\n",
    "    \n",
    "    Anomalies = Daily.groupby('time') - Climatology\n",
    "    Anomalies = Anomalies.assign_coords({'time': actual_days}) # change back to the original timestep information\n",
    "    \n",
    "    return Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:36<00:00, 12.13s/it]\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "Anomalies = list(tqdm.tqdm(pool.imap(anomalies, variables_used), \n",
    "                           total=len(variables_used), position=0, leave=True))\n",
    "pool.close()\n",
    "\n",
    "Anomalies = {variables_used[i_c]: i_anom for i_c, i_anom in enumerate(Anomalies)}\n",
    "\n",
    "del(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF Analysis <a name=\"eof-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eof_analysis(input_data):\n",
    "    \n",
    "    area_subset = input_data[0] # name of area_used (based on the keys of the \"Area_used\" dictionary)\n",
    "    area_subset = Area_used[area_subset] # list with the cordinates of the boundary box of the selected area\n",
    "    variable = input_data[1] # name of variable used (based on the keys of the \"Anomalies\" dictionary)\n",
    "    \n",
    "    dataset_used = Anomalies[variable].copy(deep=True) # dataset to be used for the analysis\n",
    "    \n",
    "    # subset area of interest \n",
    "    dataset_used = dataset_used.sel(latitude=slice(area_subset[0], area_subset[2]), \n",
    "                                    longitude=slice(area_subset[1], area_subset[3]))    \n",
    "    \n",
    "    \n",
    "    coslats = np.cos(np.deg2rad(dataset_used.latitude.values)).clip(0, 1) # coslat for weights on EOF\n",
    "    wgts = np.sqrt(coslats)[..., np.newaxis] # calculation of weights\n",
    "    solver = Eof(dataset_used, weights=wgts) # EOF analysis of the subset\n",
    "    \n",
    "    N_eofs = int(np.searchsorted(np.cumsum(solver.varianceFraction().values), Var_ex/100)) # n. of EOFs needed\n",
    "    N_eofs += 1 # add 1 since python does not include the last index of a range\n",
    "    \n",
    "    EOFS = solver.eofs(neofs=N_eofs)\n",
    "    PCS = pd.DataFrame(solver.pcs(npcs=N_eofs).values, index=dataset_used.time.values)\n",
    "    VARS = solver.varianceFraction(neigs=N_eofs).values*100\n",
    "    NOR = solver.northTest(neigs=N_eofs, vfscaled=True).values*100\n",
    "    \n",
    "    return {'EOFS': EOFS, 'PCS': PCS, 'VARS': VARS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noe that the EOF analysis as perfomed below with multiprocessing consumes all available memory of 32 Gb, so there might be a need to run the below section with less cores, or in a loop (single core)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [11:18<00:00, 56.57s/it]  \n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool()\n",
    "Combs_used = list(product(Area_used.keys(), variables_used)) # generate all combinations of area and variable\n",
    "EOF_analysis = list(tqdm.tqdm(pool.imap(eof_analysis, Combs_used), total=len(Combs_used), position=0, leave=True))\n",
    "pool.close()\n",
    "\n",
    "Combs_used = ['_'.join(i) for i in Combs_used]\n",
    "EOF_analysis = {Combs_used[i_c]: i_eof for i_c, i_eof in enumerate(EOF_analysis)}\n",
    "\n",
    "# save data used for plots (identified from the results of the last part of this script; Check Script4 Aux. Figure)\n",
    "for key in ['Med_SLP', 'Med_Z500']:\n",
    "    EOF_analysis[key]['EOFS'].to_netcdf(results_loc+'ComponentsEOF_{}.nc'.format(key)) # save EOF data for plotting\n",
    "    np.savetxt(results_loc+'VarianceEOF_{}.out'.format(key), EOF_analysis[key]['VARS']) # save Variance data \n",
    "    \n",
    "del(pool, Combs_used, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering of Daily Patterns <a name=\"clustering-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PC_norm(var_used):\n",
    "    \n",
    "    ' Normalize PCs based on standard deviation and weight them based on the % of explained variance'\n",
    "    \n",
    "    PCs = EOF_analysis[var_used]['PCS'] # extract PCs\n",
    "    Stand = PCs/PCs.std() # standardize PCs\n",
    "    \n",
    "    # normalize per sqrt of variance so K-means distance is weighted based on the importance of each PC to expl. var.\n",
    "    variance = EOF_analysis[var_used]['VARS']\n",
    "    \n",
    "    return Stand*np.sqrt(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combo_clusters(input_data):\n",
    "    \n",
    "    area_used = input_data[0] # area_subset used\n",
    "    var_used = input_data[1] # variable used\n",
    "    \n",
    "    var_used = list( np.array(var_used).flatten() ) # consistent format; always list\n",
    "    var_used = [area_used+'_'+i_var for i_var in var_used] # add info about area, so it is same name as EOF keys\n",
    "    \n",
    "    \"\"\" \n",
    "    Get the PCs of interest. If only 1 variable, then use the actual PCs, and if more than 1, then use normalized PCs.\n",
    "    In fact the results with actual data or normalized for only 1 variable are practically the same (~99% similarity),\n",
    "    but the actual ones are prefered for reducing additional computations that potentially lead to rounding errors.\n",
    "    \"\"\"\n",
    "    if len(var_used) == 1:\n",
    "        PCs = EOF_analysis[var_used[0]]['PCS']\n",
    "    else:\n",
    "        all_PCs = [PC_norm(i) for i in var_used] # make a list with the PCs of all variables of interest\n",
    "        PCs = pd.concat(all_PCs, axis=1) # concatenate all the PCs to the final DF\n",
    "    \n",
    "    Col_names = ['Clusters_'+str(i_c) for i_c in Clusters_used]\n",
    "    Labels_all = pd.DataFrame(np.nan, columns=Col_names, index=PCs.index)\n",
    "    \n",
    "    for i_c, clusters_used in enumerate(Clusters_used):\n",
    "        \n",
    "        KM_cluster = KMeans(n_clusters=clusters_used)\n",
    "        np.random.seed(10) # set always the same seed for reproducibility\n",
    "        KM_cluster.fit(PCs)\n",
    "        Labels_all.iloc[:, i_c] = KM_cluster.labels_\n",
    "        \n",
    "    return Labels_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_lists(r):\n",
    "    \n",
    "    ' Use the combinations function to generate all combs of used variables from only 1, up to all of them together '\n",
    "    \n",
    "    data = combinations(variables_used, r)\n",
    "    \n",
    "    return [list(i) for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:19<00:00,  2.84s/it]\n"
     ]
    }
   ],
   "source": [
    "All_combs = [comb_lists(i) for i in range(1, len(variables_used)+1)] # create list with all combinations of variables\n",
    "All_combs = [j for i in All_combs for j in i] # concat the sublists\n",
    "All_combs = list(product(Area_used.keys(), All_combs)) # final list with all combinations of variables and areas\n",
    "\n",
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "Clustering = list(tqdm.tqdm(pool.imap(combo_clusters, All_combs), total=len(All_combs), position=0, leave=True))\n",
    "pool.close()\n",
    "\n",
    "# change format so it can be used as dictionary key; format: <area_used>_<var1>~<var2>~<varN>, var2, ..N if avail.\n",
    "All_combs = [[i, '~'.join(j)]  for i, j in All_combs]\n",
    "All_combs = ['_'.join(i) for i in All_combs]\n",
    "\n",
    "Clustering = {All_combs[i_c]: i_clustering for i_c, i_clustering in enumerate(Clustering)}\n",
    "\n",
    "# save data used for plots (identified from the results of the last part of this script; Check Script4 Aux. Figure)\n",
    "Clustering['Med_SLP~Z500'].to_csv(results_loc+'Clusters_Med_SLP~Z500.csv') \n",
    "\n",
    "del(All_combs, pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the composites of **Med_SLP~Z500** combination for **9 clusters**, which is the preferred combination. (This is identified from the results of the last part of this script; Check Script4 Auxiliary Figure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediterranean 9 Regimes\n",
    "Labels_selection = Clustering['Med_SLP~Z500']['Clusters_9'] # get labels of cluster of interest\n",
    "Comp_SLP = Anomalies['SLP'].assign_coords({'time': Labels_selection.values}).groupby('time').mean() # composites SLP\n",
    "Comp_Z500 = Anomalies['Z500'].assign_coords({'time': Labels_selection.values}).groupby('time').mean() # compos. Z500\n",
    "Composites = xr.concat([Comp_SLP, Comp_Z500], dim=pd.Index(['SLP', 'Z500'], name='variable')) # concatenate variables\n",
    "Composites = Composites.rename({'time': 'cluster'}).drop(['surface', 'isobaricInhPa', 'step', 'number'])\n",
    "Composites = Composites.sel(latitude=slice(Area_used['Med'][0], Area_used['Med'][2]), # adjust domain for having only\n",
    "                            longitude=slice(Area_used['Med'][1], Area_used['Med'][3])) # .. area of actual data used \n",
    "\n",
    "Composites.to_netcdf(results_loc+'ClusteringComposites_Med_SLP~Z500_Clusters9.nc') # save data used for plotting\n",
    "\n",
    "del(Labels_selection, Comp_SLP, Comp_Z500, Composites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies of derived clusters per year and seasons <a name=\"clustering-frequencies\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_occurrence(x, aggregation='M', percentage=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Aggregate temporal dataframe and return the summation of occurrences or their percentages per column\n",
    "    As default the aggregation is done for monthly data and the summation of occurrenes is returned.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = x.copy()\n",
    "    data.index = pd.to_datetime(data.index.astype(str))\n",
    "    data.index = data.index.to_period(aggregation).astype(str)\n",
    "    \n",
    "    y = data.groupby([data.index, data.values]).size().to_frame('Occurrence').reset_index()\n",
    "    y.columns = ['Date', 'level_1', 'Occurrence']\n",
    "    y = y.pivot_table(values='Occurrence', index='Date', columns='level_1').fillna(0)\n",
    "    \n",
    "    if percentage:\n",
    "        y = y.apply(lambda x: (100*x)/x.sum(), axis=1)\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_occurrences(combination_used):\n",
    "    \n",
    "    seasons_names = {'Q1': 'Winter', 'Q2': 'Spring', 'Q3': 'Summer', 'Q4': 'Autumn'} # auxiliary for renaming\n",
    "\n",
    "    data_clusters = Clustering[combination_used] # read the data with the labels\n",
    "    Occurrences = pd.DataFrame()\n",
    "    for i_clusters in data_clusters: # loop through the columns (refering to number of clusters)\n",
    "        \n",
    "        data_used = data_clusters[i_clusters] # daily data used for the analysis\n",
    "\n",
    "        # create the dataset for the seasonal occurrences and the occurences for seasons and years\n",
    "        ssn_occur = aggregated_occurrence(data_used, aggregation='Q-Nov', percentage=True)\n",
    "        ssn_occur = ssn_occur.iloc[1:-1,:] # first and last seasons are not complete, so remove the data\n",
    "        ann_occur = aggregated_occurrence(data_used, aggregation='A', percentage=True)\n",
    "\n",
    "        # process the ssn_occur for deriving all subsets of year-season-cluster in a melted format\n",
    "        occurs_all = pd.melt(ssn_occur.reset_index(), id_vars='Date', value_vars=ssn_occur.columns)\n",
    "        occurs_all['Season'] = [i[-2:] for i in occurs_all.Date.astype(str)] # keep season info, without the year\n",
    "        occurs_all.replace({'Season': seasons_names}, inplace=True)\n",
    "        occurs_all['Date'] = occurs_all.Date.astype(str).apply(lambda x: x[:-2]).astype(int) # keep year info\n",
    "\n",
    "        # process the ann_occur data for having same format as the occurs_all\n",
    "        ann_occur = pd.melt(ann_occur.reset_index(), id_vars='Date', value_vars=ann_occur.columns)\n",
    "        ann_occur['Season'] = 'All'\n",
    "        ann_occur['Date'] = ann_occur['Date'].astype(str).astype(int)\n",
    "\n",
    "        # concatenate both dataframes for having the final dataframe\n",
    "        occurs_all = occurs_all.append(ann_occur)\n",
    "        occurs_all.index = pd.RangeIndex(len(occurs_all))\n",
    "        occurs_all.columns = ['Date', 'Cluster', 'Occurrence (%)', 'Season']\n",
    "        occurs_all['Clusters'] = i_clusters\n",
    "        \n",
    "        Occurrences = Occurrences.append(occurs_all)\n",
    "        \n",
    "    return Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:20<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool() # object for multiprocessing\n",
    "All_combs_names = list(Clustering.keys())\n",
    "SeasonalOccurrences = list(tqdm.tqdm(pool.imap(seasonal_occurrences, All_combs_names), \n",
    "                           total=len(All_combs_names), position=0, leave=True))\n",
    "pool.close()\n",
    "\n",
    "SeasonalOccurrences = {All_combs_names[i_c]: i_clustering for i_c, i_clustering in enumerate(SeasonalOccurrences)}\n",
    "\n",
    "# save data used for plots (identified from the results of the last part of this script; Check Script4 Aux. Figure)\n",
    "SeasonalOccurrences['Med_SLP~Z500'].to_csv(results_loc+'Frequencies_Med_SLP~Z500.csv')\n",
    "\n",
    "del(pool, All_combs_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete variables not needed any more, for emptying memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(SeasonalOccurrences, seasonal_occurrences, aggregated_occurrence, comb_lists, combo_clusters, \n",
    "    PC_norm, EOF_analysis, eof_analysis, anomalies, Anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection of Localized Extremes to Large-Scale Atmoshperic Flow Patterns <a name=\"extremes-to-patterns\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions <a name=\"auxiliary\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative distribution of binomial for statistical significance testing\n",
    "def binom_test(occurrences, propabilities):\n",
    "    return binom.cdf(k=occurrences-1, n=occurrences.sum(), p=propabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix(data, lead=1):\n",
    "    \n",
    "    '''     \n",
    "    Function for calculating the transition matrix M of an item (list/numpy/pandas Series/pandas single column DF),\n",
    "    where M[i][j] is the probablity of transitioning from state i to state j.\n",
    "    Basic code taken from stackoverflow:\n",
    "    https://stackoverflow.com/questions/46657221/generating-markov-transition-matrix-in-python\n",
    "    \n",
    "    NOTE!: Data should not have NaN values, otherwise code crushes!\n",
    "    \n",
    "    :param data : input data: one dimensional vector with elements of same type (e.g. all str, or all float, etc)\n",
    "    :param lead : lead time for checking the transition (default=1)\n",
    "    :return     : transition matrix as pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    if type(data) == pd.core.frame.DataFrame:\n",
    "        data_used = list(data.values.flatten())\n",
    "    else:\n",
    "        data_used = data\n",
    "\n",
    "    unique_states = sorted(set(data_used)) # get the names of the unique states and sort them\n",
    "    \n",
    "    dict_sequencial = {val: i for i, val in enumerate(unique_states)} # sequencial numbering of states\n",
    "    \n",
    "    transitions_numbered = pd.Series(data_used).map(dict_sequencial) # map the data to sequencial order\n",
    "    transitions_numbered = transitions_numbered.values # get only the actual values of the Series\n",
    "    \n",
    "    n = len(unique_states) # number of unique states\n",
    "\n",
    "    M = [[0]*n for _ in range(n)] # transition matrix\n",
    "\n",
    "    for (i,j) in zip(transitions_numbered,transitions_numbered[lead:]): # the total times of the transition M[i][j]\n",
    "        M[i][j] += 1\n",
    "\n",
    "    # now convert to probabilities:\n",
    "    for row in M:\n",
    "        s = sum(row)\n",
    "        if s > 0:\n",
    "            row[:] = [f/s for f in row]\n",
    "    \n",
    "    M = pd.DataFrame(M, columns=unique_states, index=unique_states) # convert to DF and name columns/rows as per data\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_clusters(input_data):\n",
    "    \n",
    "    ' Calculate statistics of occurences and limits of climatological frequencies for each cluster '\n",
    "    \n",
    "    type_used = input_data[0] # variables & area used for clustering (e.g. \"Med_T850\" or \"Full_SLP~Z500\")\n",
    "    clusters_used = input_data[1] # number of clusters used for K-means (e.g. \"Clusters_9\")\n",
    "\n",
    "    Data = Clustering[type_used][clusters_used] # cluster pd.Series with cluster label for each day\n",
    "    n_clusters = int(clusters_used.split('_')[1]) # number of clusters used, as integer\n",
    "\n",
    "    # days per cluster, and statistics of total occurrences\n",
    "    Totals = Data.value_counts() # days per cluster [use all the daily data available at the clustering results]\n",
    "    Totals = pd.DataFrame(Totals.reindex(range(n_clusters))) # sort the data per cluster order\n",
    "    Totals.rename(columns={clusters_used: 'Occurrences'}, inplace=True) # rename column\n",
    "    \n",
    "    # persistence, climatological frequencies, and effective size due to persistence\n",
    "    transitions = transition_matrix(Clustering[type_used][clusters_used]) # next-day transition probabilities matrix\n",
    "    Totals['Persistence'] = np.diag(transitions) # self-transition probability\n",
    "    total_days = len(Data) # total days used for clustering\n",
    "    Totals['Percent'] = Totals['Occurrences']/total_days # climatological frequencies\n",
    "    Totals['N_ef'] = total_days*(1-Totals['Persistence'])/(1+Totals['Persistence']) # effective length\n",
    "\n",
    "    # 95% CI of climatological frequencies: use normal approximation to Binomial distr. considering effective length \n",
    "    Totals['Perc_Upper'] = Totals['Percent']+1.96*np.sqrt(Totals['Percent']*(1-Totals['Percent'])/Totals['N_ef'])\n",
    "    Totals['Perc_Lower'] = Totals['Percent']-1.96*np.sqrt(Totals['Percent']*(1-Totals['Percent'])/Totals['N_ef'])\n",
    "\n",
    "    # Precipitation data do not include 1st Jan 1979, so use the Precipitation dates for accurate results\n",
    "    subset_totals = Data.loc[dates_all].value_counts() # days per cluster for the Precipitation data\n",
    "    Totals['Subset_Occurrences'] = subset_totals.reindex(range(n_clusters)) # sort the data per cluster order\n",
    "    Totals['Occur_Max'] = np.ceil(Totals['Perc_Upper']*len(dates_all)) # ceiling to get the next integer\n",
    "    \n",
    "    return (Totals, Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying the connections <a name=\"quantifying-connections\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extremes_to_clusters(input_data):\n",
    "    \n",
    "    ' Calculate connection of extremes to patterns; % of events per cluster, condit. prob. and stat. sign. '\n",
    "    ' inputa data: list of 2: 1) variables & area used for clustering, and 2) number of clusters used'\n",
    "    \n",
    "    Totals, Data = statistics_clusters(input_data) # get statistics of clusters and daily attributions of labels\n",
    "    \n",
    "    ExceedCounts = Exceed_xr.copy(deep=True)\n",
    "    ExceedCounts = ExceedCounts.assign_coords({'time': Data.loc[Exceed_xr.time.values].values}) # change to cluster id\n",
    "    ExceedCounts = ExceedCounts.rename({'time': 'cluster'}) # rename the coordinate\n",
    "    ExceedCounts = ExceedCounts.groupby('cluster').sum() # find total extremes at each cell allocated per cluster\n",
    "    \n",
    "    RatioCluster = ExceedCounts.transpose(..., 'cluster')/Totals['Subset_Occurrences'].values*100 # conditional prob.\n",
    "    RatioClusterMax = ExceedCounts.transpose(..., 'cluster')/Totals['Occur_Max'].values*100 # cond. prob. of 95% freq.\n",
    "    Exceed_Perc = ExceedCounts/ExceedCounts.sum(dim=['cluster'])*100 # percent of extremes per cluster\n",
    "    \n",
    "    \"check statistical significance of occurrences based on binomial distribution for 95% Confidence Interval\"\n",
    "    # perform the analysis for the Upper tail and use the Upper 95% CI for the cluster propability\n",
    "    Binom_Cum_Upper = ExceedCounts.copy(deep=True) # new xr (SOS: deep=True otherwise the data are overwritten later)\n",
    "    Binom_Cum_Upper = Binom_Cum_Upper.astype(float) # convert to float from int\n",
    "    Binom_Cum_Upper = Binom_Cum_Upper.transpose('cluster', ...)\n",
    "    Counts_np = Binom_Cum_Upper.values.copy() # numpy of values for applying the function below\n",
    "    Binom_Cum_Upper_np = np.apply_along_axis(binom_test, propabilities=Totals['Perc_Upper'],  axis=0, arr=Counts_np)\n",
    "    Binom_Cum_Upper[:] = Binom_Cum_Upper_np # pass the results to the xr\n",
    "\n",
    "    # perform the analysis for the Lower tail and use the Lower 95% CI for the cluster propability\n",
    "    Binom_Cum_Lower = Binom_Cum_Upper.copy(deep=True)\n",
    "    Binom_Cum_Lower_np = np.apply_along_axis(binom_test, propabilities=Totals['Perc_Lower'],  axis=0, arr=Counts_np)\n",
    "    Binom_Cum_Lower[:] = Binom_Cum_Lower_np\n",
    "\n",
    "    Sign = (Binom_Cum_Upper > .975)*1 + (Binom_Cum_Lower < .025)*(-1) # assign boolean for statistical significance\n",
    "\n",
    "    # final object with counts, percentages, and statistical significance\n",
    "    All_data = [ExceedCounts, Exceed_Perc, RatioCluster, RatioClusterMax, Sign]\n",
    "    Coord_name = ['Counts', 'PercExtremes', 'CondProb', 'CondProbUpperLimit', 'Significance']\n",
    "    Coord_name = pd.Index(Coord_name, name='indicator')\n",
    "    Final = xr.concat(All_data, dim=Coord_name)\n",
    "    \n",
    "    return Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis for EuroAtlantic_SLP completed.\n",
      "Analysis for EuroAtlantic_T850 completed.\n",
      "Analysis for EuroAtlantic_Z500 completed.\n",
      "Analysis for EuroAtlantic_SLP~T850 completed.\n",
      "Analysis for EuroAtlantic_SLP~Z500 completed.\n",
      "Analysis for EuroAtlantic_T850~Z500 completed.\n",
      "Analysis for EuroAtlantic_SLP~T850~Z500 completed.\n",
      "Analysis for MedExt_SLP completed.\n",
      "Analysis for MedExt_T850 completed.\n",
      "Analysis for MedExt_Z500 completed.\n",
      "Analysis for MedExt_SLP~T850 completed.\n",
      "Analysis for MedExt_SLP~Z500 completed.\n",
      "Analysis for MedExt_T850~Z500 completed.\n",
      "Analysis for MedExt_SLP~T850~Z500 completed.\n",
      "Analysis for MedExtAtl_SLP completed.\n",
      "Analysis for MedExtAtl_T850 completed.\n",
      "Analysis for MedExtAtl_Z500 completed.\n",
      "Analysis for MedExtAtl_SLP~T850 completed.\n",
      "Analysis for MedExtAtl_SLP~Z500 completed.\n",
      "Analysis for MedExtAtl_T850~Z500 completed.\n",
      "Analysis for MedExtAtl_SLP~T850~Z500 completed.\n",
      "Analysis for Med_SLP completed.\n",
      "Analysis for Med_T850 completed.\n",
      "Analysis for Med_Z500 completed.\n",
      "Analysis for Med_SLP~T850 completed.\n",
      "Analysis for Med_SLP~Z500 completed.\n",
      "Analysis for Med_T850~Z500 completed.\n",
      "Analysis for Med_SLP~T850~Z500 completed.\n",
      "\n",
      "Analysis completed in: 1:02:43.574643  HR:MN:SC.\n"
     ]
    }
   ],
   "source": [
    "StartTime = datetime.now()\n",
    "\n",
    "ExtremesClusters = {}\n",
    "for i_type in Clustering:\n",
    "    \n",
    "    clusters_used = ['Clusters_'+ str(i) for i in Clusters_used]\n",
    "    input_combo = list(zip([i_type]*len(clusters_used), clusters_used))\n",
    "    \n",
    "    # this section is memory demanding so it is not recommended to use multiprocessing\n",
    "    Extremes_Subset = [extremes_to_clusters(input_combo_i) for input_combo_i in input_combo] \n",
    "    ExtremesClusters[i_type] = {clusters_used[i]: j for i, j in enumerate(Extremes_Subset)}\n",
    "    print('Analysis for {} completed.'.format(i_type))\n",
    "    \n",
    "print('\\nAnalysis completed in:', datetime.now()-StartTime, ' HR:MN:SC.')\n",
    "\n",
    "# save data used for plots (identified from the results of the last part of this script; Check Script4 Aux. Figure)\n",
    "ExtremesClusters['Med_SLP~Z500']['Clusters_9'].to_netcdf(results_loc+'ClusteringStats_Med_SLP~Z500_Clusters9.nc')\n",
    "\n",
    "del(i_type, clusters_used, input_combo, Extremes_Subset, StartTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(Exceed_xr, binom_test, transition_matrix, statistics_clusters, extremes_to_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics for the connections between Extremes and Large-Scale Patterns <a name=\"summarizing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_data(slice_used):\n",
    "    \n",
    "    \"\"\" \n",
    "    Sort data based on multiple conditions: sorting from min to max with the following importance from most important \n",
    "    to least: significance (-1, 0, 1), cond prob, perc extremes, cond prob lower quantile\n",
    "    \"\"\"\n",
    "    test_sort = np.lexsort((slice_used[3], slice_used[2], slice_used[1], slice_used[0]))\n",
    "    \n",
    "    return test_sort[::-1] # reverse the sorting from max to min (descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_get_max_xr(xr_object, sorted_data, indicator):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reorder the xarray with the support of np, and then get the corresponding values assosicated to the 1st slice.\n",
    "    This is needed because the sorting is done initially on significance, and then on cond. prob, % of extremes, etc.\n",
    "    So the idea is to be able to get the values that correspond to the selected \"best\" slice.\n",
    "    \"\"\"\n",
    "    \n",
    "    Sorted = np.take_along_axis(xr_object.sel(indicator=indicator).values, sorted_data, 0)\n",
    "    Sorted_max = xr.DataArray(Sorted[0], dims=['latitude', 'longitude'],\n",
    "                              coords={'latitude': xr_object['latitude'].values, \n",
    "                                      'longitude': xr_object['longitude'].values})\n",
    "    return Sorted_max    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_values(subset, percentile):\n",
    "    \n",
    "    ' Get values of indicators that correspond to the cluster with highest association with extremes at each cell '\n",
    "    \n",
    "    Subset_perc = subset.sel(percentile=percentile) # get the data of interest\n",
    "\n",
    "    # sort data based on the combined sorting: most important is significance, then cond. prob., etc ...\n",
    "    Data_sort = Subset_perc.sel(indicator=['Significance', 'CondProb', 'PercExtremes', 'CondProbUpperLimit'])\n",
    "    \n",
    "    Data_sort = Data_sort.values\n",
    "\n",
    "    Sorted_data = Data_sort[0,:].copy()\n",
    "    Sorted_data[:] = np.nan\n",
    "    for i_y in range(Sorted_data.shape[1]):\n",
    "        for i_x in range(Sorted_data.shape[2]):\n",
    "            Sorted_data[:, i_y, i_x] = sort_data(slice_used=Data_sort[:, :, i_y, i_x])\n",
    "\n",
    "    Sorted_data = Sorted_data.astype(int) # convert the sorted indices to integer, as they refer to the order\n",
    "\n",
    "    MaxPerExt = reorder_get_max_xr(xr_object=Subset_perc, sorted_data=Sorted_data, indicator='PercExtremes')\n",
    "    MaxPerClu = reorder_get_max_xr(xr_object=Subset_perc, sorted_data=Sorted_data, indicator='CondProb')\n",
    "    MaxPerCluMax = reorder_get_max_xr(xr_object=Subset_perc, sorted_data=Sorted_data, indicator='CondProbUpperLimit')\n",
    "    \n",
    "    return (MaxPerExt, MaxPerClu, MaxPerCluMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_stats(input_data):\n",
    "    \n",
    "    type_used = input_data[0] # variables used for clustering (e.g. \"T850\" or \"SLP~Z500\")\n",
    "    clusters_used = input_data[1] # number of clusters used for K-means (e.g. \"Clusters_9\")\n",
    "\n",
    "    Subset = ExtremesClusters[type_used][clusters_used] # get the data of interest\n",
    "    \n",
    "    total_locs = len(Subset.longitude)*len(Subset.latitude) # total number of grid cells\n",
    "    \n",
    "    # general stats about % of locations that have significant connections to at least 1 cluster\n",
    "    MaxSign = Subset.sel(indicator='Significance').max(dim='cluster') # maximum significance per location\n",
    "    Sing_Perc = (MaxSign == 1).sum(dim=['latitude', 'longitude']) # number of sign. locations per percentile\n",
    "    Sing_Perc = Sing_Perc/total_locs*100 # percent of significant location\n",
    "    \n",
    "    Percentiles = ['P'+str(i_perc) for i_perc in P_used] # get the percentile names as Px, x:[0-100]\n",
    "    SignPerc = pd.DataFrame({'Percentile': Percentiles, 'PercentSign': Sing_Perc.values}) # dataframe with results\n",
    "    \n",
    "    # analyse more detailed statistics for all locations that have significant connections\n",
    "    # get the max values to the \"best\" cluster per cell\n",
    "    MaxStats = [max_values(subset=Subset, percentile=i_perc) for i_perc in P_used] \n",
    "    MaxPerExt = xr.concat([i[0] for i in MaxStats], dim=pd.Index(P_used, name='percentile')) # % of extremes to clust.\n",
    "    MaxPerClu = xr.concat([i[1] for i in MaxStats], dim=pd.Index(P_used, name='percentile')) # cond. prob. of cluster\n",
    "    MaxPerCluMax = xr.concat([i[2] for i in MaxStats], dim=pd.Index(P_used, name='percentile')) # cond prob for 95% CI\n",
    "    \n",
    "    Sign_all = Subset.sel(indicator='Significance').where(Subset.sel(indicator='Significance') == 1) # all sign. locs\n",
    "    \n",
    "    TotalPercSign = (Subset.sel(indicator='PercExtremes')*Sign_all).sum(dim='cluster') # total % EPEs sign. connected\n",
    "    with warnings.catch_warnings(): # if all are NaN then it gives Runtimewarning, which is now suppressed\n",
    "        warnings.simplefilter('ignore', category=RuntimeWarning)\n",
    "        MeanCond = (Subset.sel(indicator='CondProb')*Sign_all).mean(dim='cluster') # mean of conditional probabilities\n",
    "    \n",
    "    HighestOccurEx = MaxPerExt.where(MaxSign == 1).values # mask max % EPEs only for significant locations\n",
    "    HighestOccurClu = MaxPerClu.where(MaxSign == 1).values # mask max cond. prob. only for significant locations\n",
    "    HighestOccurCluMax = MaxPerCluMax.where(MaxSign == 1).values # as above for 95% CI\n",
    "    TotalPercSign = TotalPercSign.where(MaxSign == 1).values # total % of EPEs, significant for each grid cell\n",
    "    MeanCond = MeanCond.where(MaxSign == 1).values # mean cond. prob. of EPEs, significant for each grid cell\n",
    "    \n",
    "    Data_HighestOccur = pd.DataFrame() # dataframe for storing all values\n",
    "    datasets_used = [HighestOccurEx, HighestOccurClu, HighestOccurCluMax, TotalPercSign, MeanCond]\n",
    "    for i in range(HighestOccurEx.shape[0]): # loop through the percentiles\n",
    "        \n",
    "        data_aux = pd.DataFrame(columns=['MCP_PercExtr', 'MCP', 'MCP_UpperLim', 'TotalPercSign', 'MeanCondProbSign'])\n",
    "        for i_tp, i_c in zip(datasets_used, data_aux.columns):\n",
    "            data_aux_var = i_tp[i, :, :].flatten() # flatten the array\n",
    "            data_aux_var = data_aux_var[~np.isnan(data_aux_var)] # remove the non-significant locations\n",
    "            data_aux[i_c] = list(data_aux_var)\n",
    "         \n",
    "        data_aux['Percentile'] = Percentiles[i] # add the percentile info\n",
    "        \n",
    "        Data_HighestOccur = Data_HighestOccur.append(data_aux) # append to general dataframe\n",
    "    \n",
    "    # add auxiliary columns for subsetting\n",
    "    SignPerc['Clusters'] = Data_HighestOccur['Clusters'] = clusters_used\n",
    "    SignPerc['Type'] = Data_HighestOccur['Type'] = type_used\n",
    "    \n",
    "    # merge SignPerc and median of Data_HighestOccur\n",
    "    Data_Summary = Data_HighestOccur.groupby(['Percentile', 'Clusters', 'Type']).median().reset_index() # median\n",
    "    Data_Summary = pd.merge(Data_Summary, SignPerc)\n",
    "    \n",
    "    Data_Summary['Area'] = [i.split('_')[0] for i in Data_Summary['Type']]\n",
    "    Data_Summary['Variables'] = [i.split('_')[1] for i in Data_Summary['Type']]\n",
    "    \n",
    "    Data_HighestOccur['Area'] = [i.split('_')[0] for i in Data_HighestOccur['Type']]\n",
    "    Data_HighestOccur['Variables'] = [i.split('_')[1] for i in Data_HighestOccur['Type']]\n",
    "    \n",
    "    return {'DataSummary': Data_Summary, 'DataAll': Data_HighestOccur} # return summary data & data from all locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00,  9.00it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.39it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  7.92it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.21it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.07it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.79it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.39it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.88it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.35it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.78it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.00it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.65it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.64it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.19it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.69it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.07it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.56it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.19it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.54it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.83it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.83it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.21it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  9.20it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.80it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.62it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.90it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.53it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed in: 0:00:36.121428\n"
     ]
    }
   ],
   "source": [
    "StartTime = datetime.now()\n",
    "\n",
    "DataSummary = pd.DataFrame() # final dataframe with all the data\n",
    "\n",
    "for i_type in Clustering:\n",
    "    \n",
    "    clusters_used = ['Clusters_'+ str(i) for i in Clusters_used]\n",
    "    input_combo = list(zip([i_type]*len(clusters_used), clusters_used))\n",
    "    \n",
    "    pool = multiprocessing.Pool() # object for multiprocessing\n",
    "    Summary_Stats = list(tqdm.tqdm(pool.imap(summary_stats, input_combo), \n",
    "                                   total=len(input_combo), position=0, leave=True))\n",
    "    pool.close()\n",
    "    \n",
    "    # keep only the DataSummary, cause the file with all the data is too large (over 100Mb) for saving later on\n",
    "    DataSummary_aux = pd.concat([i_stats['DataSummary'] for i_stats in Summary_Stats]) # concat the data to 1 df\n",
    "    \n",
    "    DataSummary = DataSummary.append(DataSummary_aux)\n",
    "    \n",
    "print('Analysis completed in:', datetime.now()-StartTime)\n",
    "\n",
    "DataSummary.to_csv(results_loc+'DataSummary.csv') # save data used for plotting\n",
    "\n",
    "del(i_type, clusters_used, input_combo, pool, DataSummary_aux, StartTime, Summary_Stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Analysis completed in: 1:26:55.620317  HR:MN:SC.\n"
     ]
    }
   ],
   "source": [
    "print('Total Analysis completed in:', datetime.now() - InitializationTime, ' HR:MN:SC.')\n",
    "del(InitializationTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
